{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"contribute/","title":"Lemonade SDK Contribution Guide","text":"<p>The Lemonade SDK project welcomes contributions from everyone!</p> <p>See code organization for an overview of the repository.</p>"},{"location":"contribute/#collaborate-with-your-app","title":"Collaborate with Your App","text":"<p>Lemonade Server integrates quickly with most OpenAI-compatible LLM apps.</p> <p>You can: - Share an example of your app using Lemonade via Discord, GitHub Issue, or email. - Contribute a guide by adding a <code>.md</code> file to the server apps folder. Follow the style of the Open WebUI guide.</p> <p>Guides should: - Work in under 10 minutes. - Require no code changes to the app. - Use OpenAI API-compatible apps with configurable base URLs.</p>"},{"location":"contribute/#sdk-contributions","title":"SDK Contributions","text":"<p>To contribute code or examples, first open an Issue with:    - A descriptive title.    - Relevant labels (<code>enhancement</code>, <code>good first issue</code>, etc.).    - A proposal explaining what you're contributing.    - The use case it supports.</p> <p>One of the maintainers will get back to you ASAP with guidance.</p>"},{"location":"contribute/#issues","title":"Issues","text":"<p>Use Issues to report bugs or suggest features. </p> <p>A maintainer will apply one of these labels to indicate the status: - <code>on roadmap</code>: planned for development. - <code>good first issue</code>: open for contributors. - <code>needs detail</code>: needs clarification before proceeding. - <code>wontfix</code>: out of scope or unmaintainable.</p>"},{"location":"contribute/#pull-requests","title":"Pull Requests","text":"<p>Submit a PR to contribute code. Maintainers: - @danielholanda - @jeremyfowers - @ramkrishna - @vgodsoe</p> <p>Discuss major changes via an Issue first.</p>"},{"location":"contribute/#testing","title":"Testing","text":"<p>Tests are run automatically on each PR. These include: - Linting - Code formatting (<code>black</code>) - Unit tests - End-to-end tests</p> <p>To run tests locally, use the commands in <code>.github/workflows/</code>.</p>"},{"location":"contribute/#versioning","title":"Versioning","text":"<p>We follow Semantic Versioning.</p>"},{"location":"lemonade_api/","title":"\ud83c\udf4b Lemonade API: Model Compatibility and Recipes","text":"<p>Lemonade API (<code>lemonade.api</code>) provides a simple, high-level interface to load and run LLM models locally. This guide helps you understand what models work with which recipes, what to expect in terms of compatibility, and how to choose the right setup for your hardware.</p>"},{"location":"lemonade_api/#what-is-a-recipe","title":"\ud83e\udde0 What Is a Recipe?","text":"<p>A recipe defines how a model is run \u2014 including backend (e.g., PyTorch, ONNX Runtime), quantization strategy, and device support. The <code>from_pretrained()</code> function in <code>lemonade.api</code> uses the recipe to configure everything automatically. For the list of recipes, see Recipe Compatibility Table. The following is an example of using the Lemonade API <code>from_pretrained()</code> function:</p> <pre><code>from lemonade.api import from_pretrained\n\nmodel, tokenizer = from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", recipe=\"hf-cpu\")\n</code></pre> <p>Function Arguments: - checkpoint: The Hugging Face or OGA checkpoint that defines the LLM. - recipe: Defines the implementation and hardware used for the LLM. Default is \"hf-cpu\".</p>"},{"location":"lemonade_api/#system-information-functions","title":"\ud83d\udd0d System Information Functions","text":"<p>The Lemonade API also provides functions to programmatically access system and device information:</p> <pre><code>from lemonade.api import get_system_info, get_device_info\n\n# Get essential system information (OS, processor, memory, devices)\nsystem_info = get_system_info()\n\n# Get detailed system information including Python packages and extended details\nsystem_info_verbose = get_system_info(verbose=True)\n\n# Get only device information (CPU, GPU, NPU details)\ndevice_info = get_device_info()\n</code></pre> <p>These functions return the same information available through the <code>lemonade system-info</code> CLI command and the <code>/api/v1/system-info</code> server endpoint, including: - Hardware details (CPU, memory, BIOS) - Device availability (AMD iGPU, dGPU, NPU) - Inference engine compatibility per device - Driver versions and system configuration - Python package versions (verbose mode only)</p>"},{"location":"lemonade_api/#supported-model-formats","title":"\ud83d\udcdc Supported Model Formats","text":"<p>Lemonade API currently supports:</p> <ul> <li>Hugging Face hosted safetensors checkpoints</li> <li>AMD OGA (ONNXRuntime-GenAI) ONNX checkpoints</li> </ul> <p></p>"},{"location":"lemonade_api/#recipe-and-checkpoint-compatibility","title":"\ud83c\udf74 Recipe and Checkpoint Compatibility","text":"<p>The following table explains what checkpoints work with each recipe, the hardware and OS requirements, and additional notes:</p> Recipe Checkpoint Format Hardware Needed Operating System Notes <code>hf-cpu</code> safetensors (Hugging Face) Any x86 CPU Windows, Linux Compatible with x86 CPUs, offering broad accessibility. <code>hf-dgpu</code> safetensors (Hugging Face) Compatible Discrete GPU Windows, Linux Requires PyTorch and a compatible GPU.<sup>[1]</sup> <code>oga-cpu</code> safetensors (Hugging Face) Any x86 CPU Windows Converted from safetensors via `model_builder`. Accuracy loss due to RTN quantization. OGA ONNX Any x86 CPU Windows Use models from the CPU Collection. OGA ONNX AMD Ryzen AI PC Windows Use models from the GPU Collection. <code>oga-hybrid</code> Pre-quantized OGA ONNX AMD Ryzen AI 300 series PC Windows Use models from the Hybrid Collection. Optimized with AWQ to INT4. <code>oga-npu</code> Pre-quantized OGA ONNX AMD Ryzen AI 300 series PC Windows Use models from the NPU Collection. Optimized with AWQ to INT4. <p><sup>[1]</sup> Compatible GPUs are those that support PyTorch's <code>.to(\"cuda\")</code> function. Ensure you have the appropriate version of PyTorch installed (e.g., CUDA or ROCm) for your specific GPU. Note: Lemonade does not install PyTorch with CUDA or ROCm for you. For installation instructions, see PyTorch's Get Started Guide.</p>"},{"location":"lemonade_api/#converting-models-to-oga","title":"\ud83d\udd04 Converting Models to OGA","text":"<p>Lemonade API will do the conversion for you using OGA's <code>model_builder</code> if you pass a safetensors checkpoint.</p> <ul> <li>Takes \\~1\u20135 minutes per model.</li> <li>Uses RTN quantization (int4).</li> <li>For better quality, use pre-quantized models (see below).</li> </ul>"},{"location":"lemonade_api/#pre-converted-oga-models","title":"\ud83d\udce6 Pre-Converted OGA Models","text":"<p>You can skip the conversion step by using pre-quantized models from AMD\u2019s Hugging Face collection. These models are optimized using Activation Aware Quantization (AWQ), which provides higher-accuracy int4 quantization compared to RTN.</p> Recipe Collection <code>oga-hybrid</code> Hybrid Collection <code>oga-npu</code> NPU Collection <code>oga-cpu</code> CPU Collection"},{"location":"lemonade_api/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Lemonade API Examples</li> <li>lemonade.api source</li> <li>Model Support Matrix (ONNX Runtime GenAI)</li> </ul>"},{"location":"server/","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"server/#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"server/#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"server/concepts/","title":"Local LLM Server Concepts","text":"<p>This document gives background information about the main concepts for local LLM servers and \ud83c\udf4bLemonade Server.</p> <p>The intention is to answer these FAQs:</p> <ul> <li>What is a Local Server?</li> <li>What is a Local LLM Server?</li> <li>What is the OpenAI Standard?</li> <li>How does the OpenAI Standard work?</li> </ul>"},{"location":"server/concepts/#what-is-a-local-server","title":"What is a Local Server?","text":"<p>First, let\u2019s clarify what we mean by <code>server software</code>, as it\u2019s sometimes confused with <code>server hardware</code>, which is the actual physical systems running in data centers. - <code>Server software</code> refers to a process running on a computer that listens for and responds to requests initiated by <code>client software</code> (i.e., applications). - <code>Server software</code> often runs on <code>server hardware</code>, but there are many examples of <code>server software</code> running on the same <code>client hardware</code> (laptop, desktop computer, tablet, or smartphone) as the <code>application</code>.</p> <p>A <code>local server</code> is <code>server software</code> that runs on <code>client hardware</code>.</p>"},{"location":"server/concepts/#what-is-a-local-llm-server","title":"What is a Local LLM Server?","text":"<p>Local LLM servers are an extremely popular way of deploying LLMs directly to <code>client hardware</code>. A few famous examples of local LLM servers include Ollama, llama-cpp-server, and Docker Model Runner.</p> <p>The local server process loads the LLM into memory and exposes it to client software for handling requests. Compared to integrating the LLM directly into the client software using C++ or Python APIs, this setup provides the following benefits:</p> Benefit Description Simplified integration C++/Python APIs are typically framework- (e.g., llama.cpp, OGA, etc.) and/or device- (e.g., CPU, GPU, NPU, etc.) specific. Local LLM servers, on the other hand, facilitate conversing with the LLM at a high level that abstracts these details away (see What is the OpenAI Standard?). Sharing LLMs between applications A single local LLM can take up a significant portion of system RAM. The local LLM server can share this LLM between multiple applications, rather than requiring each application to load its own LLM into RAM. Separation of concerns Installing and managing LLMs, enabling features like tool use and streaming generation, and building in fault tolerance can be tricky to implement. A local LLM server abstracts away this complexity, letting application developers stay focused on their app. Cloud-to-client development A common practice for LLM developers is to first develop their application using cloud LLMs, then switch to local LLMs later in development. Local and cloud LLM servers behave similarly from the application's perspective, which makes this transition seamless."},{"location":"server/concepts/#what-is-the-openai-standard","title":"What is the OpenAI Standard?","text":"<p>All LLM servers (cloud or local) adhere to an application-program interface (API). This API lets the <code>application</code> make LLM requests to the <code>server software</code>.</p> <p>While there are several popular LLM server APIs available in the LLM ecosystem, the OpenAI API has emerged as the industry standard because it is (at the time of this writing) the only API that meets these three criteria: 1. Dozens of popular LLM <code>servers</code> support OpenAI API. 1. Dozens of popular LLM <code>applications</code> support OpenAI API. 1. OpenAI API is broadly supported in both <code>local</code> and <code>cloud</code>.</p> <p>Crucially, while OpenAI offers their own LLM API-as-a-cloud-service, their API standard is rigorously documented and available for other cloud and local servers to adopt.</p>"},{"location":"server/concepts/#how-does-the-openai-standard-work","title":"How does the OpenAI Standard Work?","text":"<p>In the OpenAI API standard, applications and servers communicate in the form of a multi-role conversation. There are three \"roles\" in this context: the \"system\", the \"assistant\", and the \"user\".</p> Role Description System Allows the application to provide instructions to the LLM, such as defining its persona, what tools are available to it, what tasks it is supposed to help with or not help with, etc. Assistant Messages sent from the LLM to the application. User Messages sent from the application to the LLM. Often these messages are written by the application's end-user. <p>OpenAI also provides convenient libraries in JavaScript, Python, .Net, Java, and Go to help application and server developers adhere to the standard.</p> <p>For example, the following Python code demonstrates how an application can request an LLM response from the Lemonade Server:</p> <pre><code># Client library provided by OpenAI to automate request\n# and response processing with the server\nfrom openai import OpenAI\n\n# The base_url points to an LLM server, which can either be\n# local (localhost address) or cloud-based (web address)\nbase_url = f\"http://localhost:8000/api/v1\"\n\n# The `client` instance here provides APIs to request\n# LLM invocations from the server\nclient = OpenAI(\n    base_url=base_url,\n    api_key=\"lemonade\",  # required, but unused in Lemonade\n)\n\n# The `messages` list provides the history of messages from\n# the system, assistant, and user roles\nmessages = [\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\", \"content\":\"Hi, how are you?\"},\n]\n\n# This is the API call that sends the `messages` history to\n# the server's specific LLM `model`\n# It returns a `completion`, which is OpenAI's way of referring\n# to the LLM's reponse to the messages\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.1-8B-Instruct-Hybrid\",\n    messages=messages,\n)\n\n# This code gets the LLM's response from the `completion`\n# and prints it to the screen\nreponse = completion.choices[0].message.content\nprint(response)\n</code></pre> <p>The Python above will work with Lemonade Server, along with a variety of other cloud and local LLM servers, just by changing the <code>base_url</code>, <code>api_key</code>, and <code>model</code> as needed. This example demonstrates that details like deployment location (local vs. cloud), hardware type (GPU vs. NPU), and backend implementation (OGA vs. llama.cpp), etc. are hidden behind a unified interface.</p>"},{"location":"server/lemonade-server-cli/","title":"<code>lemonade-server</code> CLI","text":"<p>The <code>lemonade-server</code> command-line interface (CLI) provides a set of utility commands for managing the server. When you install Lemonade Server using the GUI installer, <code>lemonade-server</code> is added to your PATH so that it can be invoked from any terminal.</p> <p>Note: if you installed from source or PyPI, you should call <code>lemonade-server-dev</code> in your activated Python environment, instead of using <code>lemonade-server</code>.</p> <p><code>lemonade-server</code> provides these utilities:</p> Option/Command Description <code>-v</code>, <code>--version</code> Print the <code>lemonade-sdk</code> package version used to install Lemonade Server. <code>serve</code> Start the server process in the current terminal. See command options below. <code>status</code> Check if server is running. If it is, print the port number. <code>stop</code> Stop any running Lemonade Server process. <code>pull MODEL_NAME</code> Install an LLM named <code>MODEL_NAME</code>. See the server models guide for more information. <code>run MODEL_NAME</code> Start the server (if not already running) and chat with the specified model. <code>list</code> List all models. <p>Example:</p> <pre><code>lemonade-server serve --port 8080 --log-level debug --truncate-inputs\n</code></pre>"},{"location":"server/lemonade-server-cli/#command-line-options-for-serve","title":"Command Line Options for <code>serve</code>","text":"<p>When using the <code>serve</code> command, you can configure the server with these additional options:</p> Option Description Default <code>--port [port]</code> Specify the port number to run the server on 8000 <code>--log-level [level]</code> Set the logging level info <p>The Lemonade Server integration guide provides more information about how these commands can be used to integrate Lemonade Server into an application.</p>"},{"location":"server/server_integration/","title":"Integrating with Lemonade Server","text":"<p>This guide provides instructions on how to integrate Lemonade Server into your application.</p> <p>There are two main ways in which Lemonade Sever might integrate into apps:</p> <ul> <li>User-Managed Server: User is responsible for installing and managing Lemonade Server.</li> <li>App-Managed Server: App is responsible for installing and managing Lemonade Server on behalf of the user.</li> </ul> <p>The first part of this guide contains instructions that are common for both integration approaches. The second part provides advanced instructions only needed for app-managed server integrations.</p>"},{"location":"server/server_integration/#general-instructions","title":"General Instructions","text":""},{"location":"server/server_integration/#identifying-existing-installation","title":"Identifying Existing Installation","text":"<p>To identify if Lemonade Server is installed on a system, you can use the <code>lemonade-server</code> CLI command, which is added to path when using our installer. This is a reliable method to:</p> <ul> <li>Verify if the server is installed.</li> <li>Check which version is currently available is running the command below.</li> </ul> <pre><code>lemonade-server --version\n</code></pre> <p>Note: The <code>lemonade-server</code> CLI command is added to PATH when using the Windows Installer (Lemonade_Server_Installer.exe). For Linux users or Python development environments, the command <code>lemonade-server-dev</code> is available when installing via pip.</p>"},{"location":"server/server_integration/#checking-server-status","title":"Checking Server Status","text":"<p>To identify whether or not the server is running anywhere on the system you may use the <code>status</code> command of <code>lemonade-server</code>.</p> <pre><code>lemonade-server status\n</code></pre> <p>This command will return either <code>Server is not running</code> or <code>Server is running on port &lt;PORT&gt;</code>.</p>"},{"location":"server/server_integration/#identifying-compatible-devices","title":"Identifying Compatible Devices","text":"<p>AMD Ryzen\u2122 AI <code>Hybrid</code> models are available on Windows 11 on all AMD Ryzen\u2122 AI 300 Series Processors. To programmatically identify supported devices, we recommend using a regular expression that checks if the CPU name converted to lowercase contains \"ryzen ai\" and a 3-digit number starting with 3 as shown below.</p> <pre><code>ryzen ai.*\\b3\\d{2}\\b\n</code></pre> <p>Explanation:</p> <ul> <li><code>ryzen ai</code>: Matches the literal phrase \"Ryzen AI\".</li> <li><code>.*</code>: Allows any characters (including spaces) to appear after \"Ryzen AI\".</li> <li><code>\\b3\\d{2}\\b</code>: Matches a three-digit number starting with 3, ensuring it's a standalone number.</li> </ul> <p>There are several ways to check the CPU name on a Windows computer. A reliable way of doing so is through cmd's <code>reg query</code> command as shown below.</p> <pre><code>reg query \"HKEY_LOCAL_MACHINE\\HARDWARE\\DESCRIPTION\\System\\CentralProcessor\\0\" /v ProcessorNameString\n</code></pre> <p>Once you capture the CPU name, make sure to convert it to lowercase before using the regular expression.</p>"},{"location":"server/server_integration/#downloading-server-installer","title":"Downloading Server Installer","text":"<p>The recommended way of directing users to the server installer is pointing users to our releases page at <code>https://github.com/lemonade-sdk/lemonade/releases</code>. Alternatively, you may also provide the direct path to the installer itself or download the installer programmatically as shown below:</p> <p>Latest version:</p> <pre><code>https://github.com/lemonade-sdk/lemonade/releases/latest/download/Lemonade_Server_Installer.exe\n</code></pre> <p>Specific version:</p> <pre><code>https://github.com/lemonade-sdk/lemonade/releases/download/v6.0.0/Lemonade_Server_Installer.exe\n</code></pre> <p>Please note that the Server Installer is only available on Windows. Apps that integrate with our server on a Linux machine must install Lemonade from source as described here.</p>"},{"location":"server/server_integration/#installing-additional-models","title":"Installing Additional Models","text":"<p>Lemonade Server installations always come with at least one LLM installed. If you want to install additional models on behalf of your users, the following tools are available:</p> <ul> <li>Discovering which LLMs are available:</li> <li>A human-readable list of supported models.</li> <li>A JSON file with the list of supported models is included in every Lemonade Server installation.</li> <li>Installing LLMs:</li> <li>The <code>pull</code> endpoint in the server.</li> <li><code>lemonade-server pull MODEL</code> on the command line interface.</li> </ul>"},{"location":"server/server_integration/#stand-alone-server-integration","title":"Stand-Alone Server Integration","text":"<p>Some apps might prefer to be responsible for installing and managing Lemonade Server on behalf of the user. This part of the guide includes steps for installing and running Lemonade Server so that your users don't have to install Lemonade Server separately.</p> <p>Definitions:</p> <ul> <li>Command line usage allows the server process to be launched programmatically, so that your application can manage starting and stopping the server process on your user's behalf.</li> <li>\"Silent installation\" refers to an automatic command for installing Lemonade Server without running any GUI or prompting the user for any questions. It does assume that the end-user fully accepts the license terms, so be sure that your own application makes this clear to the user.</li> </ul>"},{"location":"server/server_integration/#command-line-invocation","title":"Command Line Invocation","text":"<p>This command line invocation starts the Lemonade Server process so that your application can connect to it via REST API endpoints. To start the server, simply run the command below.</p> <pre><code>lemonade-server serve\n</code></pre> <p>By default, the server runs on port 8000. Optionally, you can specify a custom port using the --port argument:</p> <pre><code>lemonade-server serve --port 8123\n</code></pre> <p>You can also prevent the server from showing a system tray icon by using the <code>--no-tray</code> flag:</p> <pre><code>lemonade-server serve --no-tray\n</code></pre> <p>You can also run the server as a background process using a subprocess or any preferred method.</p> <p>To stop the server, you may use the <code>lemonade-server stop</code> command, or simply terminate the process you created by keeping track of its PID. Please do not run the <code>lemonade-server stop</code> command if your application has not started the server, as the server may be used by other applications.</p>"},{"location":"server/server_integration/#silent-installation","title":"Silent Installation","text":"<p>Silent installation runs <code>Lemonade_Server_Installer.exe</code> without a GUI and automatically accepts all prompts.</p> <p>In a <code>cmd.exe</code> terminal:</p> <p>Install with Ryzen AI hybrid support: </p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid\n</code></pre> <p>Install without Ryzen AI hybrid support:</p> <pre><code>Lemonade_Server_Installer.exe /S\n</code></pre> <p>The install directory can also be changed from the default by using <code>/D</code> as the last argument. </p> <p>For example: </p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid /D=C:\\a\\new\\path\n</code></pre> <p>Only <code>Qwen2.5-0.5B-Instruct-CPU</code> is installed by default in silent mode. If you wish to select additional models to download in silent mode, you may use the <code>/Models</code> argument.</p> <pre><code>Lemonade_Server_Installer.exe /S /Extras=hybrid /Models=\"Qwen2.5-0.5B-Instruct-CPU Llama-3.2-1B-Instruct-Hybrid\"\n</code></pre> <p>The available modes are documented here.</p> <p>Finally, if you don't want to create a desktop shortcut during installation, use the <code>/NoDesktopShortcut</code> parameter:</p> <pre><code>Lemonade_Server_Installer.exe /S /NoDesktopShortcut\n</code></pre>"},{"location":"server/server_models/","title":"\ud83c\udf4b Lemonade Server Models","text":"<p>This document provides the models we recommend for use with Lemonade Server. Click on any model to learn more details about it, such as the Lemonade Recipe used to load the model.</p>"},{"location":"server/server_models/#model-management-gui","title":"Model Management GUI","text":"<p>Lemonade Server offers a model management GUI to help you see which models are available, install new models, and delete models. You can access this GUI by starting Lemonade Server, opening http://localhost:8000 in your web browser, and clicking the Model Management tab.</p>"},{"location":"server/server_models/#naming-convention","title":"Naming Convention","text":"<p>The format of each Lemonade name is a combination of the name in the base checkpoint and the backend where the model will run. So, if the base checkpoint is <code>meta-llama/Llama-3.2-1B-Instruct</code>, and it has been optimized to run on Hybrid, the resulting name is <code>Llama-3.2-3B-Instruct-Hybrid</code>.</p>"},{"location":"server/server_models/#model-storage-and-management","title":"Model Storage and Management","text":"<p>Lemonade Server relies on Hugging Face Hub to manage downloading and storing models on your system. By default, Hugging Face Hub downloads models to <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub</code>.</p> <p>For example, the Lemonade Server <code>Llama-3.2-3B-Instruct-Hybrid</code> model will end up at <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub\\models--amd--Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid</code>. If you want to uninstall that model, simply delete that folder.</p> <p>You can change the directory for Hugging Face Hub by setting the <code>HF_HOME</code> or <code>HF_HUB_CACHE</code> environment variables.</p>"},{"location":"server/server_models/#installing-additional-models","title":"Installing Additional Models","text":"<p>Once you've installed Lemonade Server, you can install any model on this list using the <code>pull</code> command in the <code>lemonade-server</code> CLI.</p> <p>Example:</p> <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> <p>Note: <code>lemonade-server</code> is a utility that is added to your PATH when you install Lemonade Server with the GUI installer. If you are using Lemonade Server from a Python environment, use the <code>lemonade-server-dev pull</code> command instead.</p>"},{"location":"server/server_models/#supported-models","title":"Supported Models","text":""},{"location":"server/server_models/#hybrid","title":"Hybrid","text":"Llama-3.2-1B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-1B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Llama-3.2-3B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-3B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-3B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Phi-3-Mini-Instruct-Hybrid <pre><code>lemonade-server pull Phi-3-Mini-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Qwen-1.5-7B-Chat-Hybrid <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid DeepSeek-R1-Distill-Llama-8B-Hybrid <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-Hybrid\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-hybrid Recipeoga-hybrid Labelsreasoning DeepSeek-R1-Distill-Qwen-7B-Hybrid <pre><code>lemonade-server pull DeepSeek-R1-Distill-Qwen-7B-Hybrid\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Qwen-7B-awq-asym-uint4-g128-lmhead-onnx-hybrid Recipeoga-hybrid Labelsreasoning Mistral-7B-v0.3-Instruct-Hybrid <pre><code>lemonade-server pull Mistral-7B-v0.3-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Mistral-7B-Instruct-v0.3-awq-g128-int4-asym-fp16-onnx-hybrid Recipeoga-hybrid Llama-3.1-8B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.1-8B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.1-8B-Instruct-awq-asym-uint4-g128-lmhead-onnx-hybrid Recipeoga-hybrid Llama-xLAM-2-8b-fc-r-Hybrid <pre><code>lemonade-server pull Llama-xLAM-2-8b-fc-r-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-xLAM-2-8b-fc-r-awq-g128-int4-asym-bfp16-onnx-hybrid Recipeoga-hybrid"},{"location":"server/server_models/#cpu","title":"CPU","text":"Qwen2.5-0.5B-Instruct-CPU <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx Recipeoga-cpu Phi-3-Mini-Instruct-CPU <pre><code>lemonade-server pull Phi-3-Mini-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct_int4_float16_onnx_cpu Recipeoga-cpu Qwen-1.5-7B-Chat-CPU <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-CPU\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat_uint4_asym_g128_float16_onnx_cpu Recipeoga-cpu DeepSeek-R1-Distill-Llama-8B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning DeepSeek-R1-Distill-Qwen-7B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Qwen-7B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning"},{"location":"server/server_models/#gguf","title":"GGUF","text":"Qwen3-0.6B-GGUF <pre><code>lemonade-server pull Qwen3-0.6B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-0.6B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-1.7B-GGUF <pre><code>lemonade-server pull Qwen3-1.7B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-1.7B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-4B-GGUF <pre><code>lemonade-server pull Qwen3-4B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-4B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-8B-GGUF <pre><code>lemonade-server pull Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning DeepSeek-Qwen3-8B-GGUF <pre><code>lemonade-server pull DeepSeek-Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning Qwen3-14B-GGUF <pre><code>lemonade-server pull Qwen3-14B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-14B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Qwen3-30B-A3B-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Gemma-3-4b-it-GGUF <pre><code>lemonade-server pull Gemma-3-4b-it-GGUF\n</code></pre> KeyValue Checkpointggml-org/gemma-3-4b-it-GGUF GGUF VariantQ4_K_M Mmprojmmproj-model-f16.gguf Recipellamacpp Labelsvision Qwen2.5-VL-7B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-VL-7B-Instruct-GGUF\n</code></pre> KeyValue Checkpointggml-org/Qwen2.5-VL-7B-Instruct-GGUF GGUF VariantQ4_K_M Mmprojmmproj-Qwen2.5-VL-7B-Instruct-f16.gguf Recipellamacpp Labelsvision Llama-4-Scout-17B-16E-Instruct-GGUF <pre><code>lemonade-server pull Llama-4-Scout-17B-16E-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Llama-4-Scout-17B-16E-Instruct-GGUF GGUF VariantQ4_K_S Mmprojmmproj-F16.gguf Recipellamacpp Labelsvision nomic-embed-text-v1-GGUF <pre><code>lemonade-server pull nomic-embed-text-v1-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v1-GGUF GGUF VariantQ4_K_S Recipellamacpp Labelsembeddings nomic-embed-text-v2-moe-GGUF <pre><code>lemonade-server pull nomic-embed-text-v2-moe-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v2-moe-GGUF GGUF VariantQ8_0 Recipellamacpp Labelsembeddings bge-reranker-v2-m3-GGUF <pre><code>lemonade-server pull bge-reranker-v2-m3-GGUF\n</code></pre> KeyValue Checkpointpqnet/bge-reranker-v2-m3-Q8_0-GGUF Recipellamacpp Labelsreranking Devstral-Small-2507-GGUF <pre><code>lemonade-server pull Devstral-Small-2507-GGUF\n</code></pre> KeyValue Checkpointmistralai/Devstral-Small-2507_gguf GGUF VariantQ4_K_M Recipellamacpp Qwen2.5-Coder-32B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-Coder-32B-Instruct-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen2.5-Coder-32B-Instruct-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelsreasoning"},{"location":"server/server_spec/","title":"Lemonade Server Spec","text":"<p>The <code>lemonade</code> SDK provides a standards-compliant server process that provides a REST API to enable communication with other applications.</p> <p>Lemonade Server currently supports two backends:</p> Backend Model Format Description ONNX Runtime GenAI (OGA) <code>.ONNX</code> Lemonade's built-in server, recommended for standard use on AMD platforms. Llama.cpp (Experimental) <code>.GGUF</code> Uses llama.cpp's Vulkan-powered llama-server backend. More details here."},{"location":"server/server_spec/#oga-endpoints-overview","title":"OGA Endpoints Overview","text":"<p>Right now, the key endpoints of the OpenAI API are available.</p> <p>We are also actively investigating and developing additional endpoints that will improve the experience of local applications.</p>"},{"location":"server/server_spec/#openai-compatible-endpoints","title":"OpenAI-Compatible Endpoints","text":"<ul> <li>POST <code>/api/v1/chat/completions</code> - Chat Completions (messages -&gt; completion)</li> <li>POST <code>/api/v1/completions</code> - Text Completions (prompt -&gt; completion)</li> <li>POST <code>api/v1/responses</code> - Chat Completions (prompt|messages -&gt; event)</li> <li>GET <code>/api/v1/models</code> - List models available locally</li> </ul>"},{"location":"server/server_spec/#additional-endpoints","title":"Additional Endpoints","text":"<p>\ud83d\udea7 These additional endpoints are a preview that is under active development. The API specification is subject to change.</p> <p>These additional endpoints were inspired by the LM Studio REST API, Ollama API, and OpenAI API.</p> <p>They focus on enabling client applications by extending existing cloud-focused APIs (e.g., OpenAI) to also include the ability to load and unload models before completion requests are made. These extensions allow for a greater degree of UI/UX responsiveness in native applications by allowing applications to:</p> <ul> <li>Pre-load models at UI-loading-time, as opposed to completion-request time.</li> <li>Load models from the local system that were downloaded by other applications (i.e., a common system-wide models cache).</li> <li>Unload models to save memory space.</li> </ul> <p>The additional endpoints under development are:</p> <ul> <li>POST <code>/api/v1/pull</code> - Install a model</li> <li>POST <code>/api/v1/load</code> - Load a model</li> <li>POST <code>/api/v1/unload</code> - Unload a model</li> <li>POST <code>/api/v1/params</code> - Set generation parameters</li> <li>GET <code>/api/v1/health</code> - Check server health</li> <li>GET <code>/api/v1/stats</code> - Performance statistics from the last request</li> <li>GET <code>/api/v1/system-info</code> - System information and device enumeration</li> </ul> <p>\ud83d\udea7 We are in the process of developing this interface. Let us know what's important to you on Github or by email (lemonade at amd dot com).</p>"},{"location":"server/server_spec/#start-the-rest-api-server","title":"Start the REST API Server","text":"<p>NOTE: This server is intended for use on local systems only. Do not expose the server port to the open internet.</p>"},{"location":"server/server_spec/#windows-installer","title":"Windows Installer","text":"<p>See the Lemonade Server getting started instructions. </p>"},{"location":"server/server_spec/#python-environment","title":"Python Environment","text":"<p>If you have Lemonade installed in a Python environment, simply activate it and run the following command to start the server:</p> <pre><code>lemonade-server-dev serve\n</code></pre>"},{"location":"server/server_spec/#openai-compatible-endpoints_1","title":"OpenAI-Compatible Endpoints","text":""},{"location":"server/server_spec/#post-apiv1chatcompletions","title":"<code>POST /api/v1/chat/completions</code>","text":"<p>Chat Completions API. You provide a list of messages and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters","title":"Parameters","text":"Parameter Required Description Status <code>messages</code> Yes Array of messages in the conversation. Each message should have a <code>role</code> (\"user\" or \"assistant\") and <code>content</code> (the message text). <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. <code>temperature</code> No What sampling temperature to use. <code>tools</code> No A list of tools the model may call. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_completion_tokens</code>. This value is now deprecated by OpenAI in favor of <code>max_completion_tokens</code> <code>max_completion_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_tokens</code>. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#example-request","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest `\n  -Uri \"http://localhost:8000/api/v1/chat/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the population of Paris?\"\n      }\n    ],\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"messages\": [\n          {\"role\": \"user\", \"content\": \"What is the population of Paris?\"}\n        ],\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format","title":"Response format","text":"Non-streaming responsesStreaming responses <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    },\n    \"finish_reason\": \"stop\"\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a stream of server-sent events (however, Open AI recommends using their streaming libraries for parsing streaming responses):</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"delta\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris\"\n    }\n  }]\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1completions","title":"<code>POST /api/v1/completions</code>","text":"<p>Text Completions API. You provide a prompt and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_1","title":"Parameters","text":"Parameter Required Description Status <code>prompt</code> Yes The prompt to use for the completion. <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>echo</code> No Echo back the prompt in addition to the completion. Available on non-streaming mode. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. Only available when <code>stream=False</code>. <code>temperature</code> No What sampling temperature to use. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion, including input tokens. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#example-request_1","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"prompt\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"prompt\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_1","title":"Response format","text":"<p>The following format is used for both streaming and non-streaming responses:</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"text_completion\",\n  \"created\": 1742927481,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"choices\": [{\n    \"index\": 0,\n    \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\",\n    \"finish_reason\": \"stop\"\n  }],\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1responses","title":"<code>POST /api/v1/responses</code>","text":"<p>Responses API. You provide an input and receive a response. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_2","title":"Parameters","text":"Parameter Required Description Status <code>input</code> Yes A list of dictionaries or a string input for the model to respond to. <code>model</code> Yes The model to use for the response. <code>max_output_tokens</code> No The maximum number of output tokens to generate. <code>temperature</code> No What sampling temperature to use. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <p>Note: The value for <code>model</code> is either a Lemonade Server model name, or a checkpoint that has been pre-loaded using the load endpoint.</p>"},{"location":"server/server_spec/#streaming-events","title":"Streaming Events","text":"<p>The Responses API uses semantic events for streaming. Each event is typed with a predefined schema, so you can listen for events you care about. Our initial implementation only offers support to:</p> <ul> <li><code>response.created</code></li> <li><code>response.output_text.delta</code></li> <li><code>response.completed</code></li> </ul> <p>For a full list of event types, see the API reference for streaming.</p>"},{"location":"server/server_spec/#example-request_2","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/responses\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"input\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"input\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_2","title":"Response format","text":"Non-streaming responsesStreaming Responses <pre><code>{\n  \"id\": \"0\",\n  \"created_at\": 1746225832.0,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"object\": \"response\",\n  \"output\": [{\n    \"id\": \"0\",\n    \"content\": [{\n      \"annotations\": [],\n      \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    }]\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a series of events. Refer to OpenAI streaming guide for details.</p>"},{"location":"server/server_spec/#get-apiv1models","title":"<code>GET /api/v1/models</code>","text":"<p>Returns a list of key models available on the server in an OpenAI-compatible format. We also expanded each model object with the <code>checkpoint</code> and <code>recipe</code> fields, which may be used to load a model using the <code>load</code> endpoint.</p> <p>This list is curated based on what works best for Ryzen AI Hybrid. Only models available locally are shown.</p>"},{"location":"server/server_spec/#parameters_3","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_3","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/models\n</code></pre>"},{"location":"server/server_spec/#response-format_3","title":"Response format","text":"<pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"Qwen2.5-0.5B-Instruct-CPU\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"amd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx\",\n      \"recipe\": \"oga-cpu\"\n    },\n    {\n      \"id\": \"Llama-3.2-1B-Instruct-Hybrid\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid\",\n      \"recipe\": \"oga-hybrid\"\n    },\n  ]\n}\n</code></pre>"},{"location":"server/server_spec/#additional-endpoints_1","title":"Additional Endpoints","text":""},{"location":"server/server_spec/#get-apiv1pull","title":"<code>GET /api/v1/pull</code>","text":"<p>Register and install models for use with Lemonade Server.</p>"},{"location":"server/server_spec/#parameters_4","title":"Parameters","text":"<p>The Lemonade Server built-in model registry has a collection of model names that can be pulled and loaded. The <code>pull</code> endpoint can install any registered model, and it can also register-then-install any model available on Hugging Face.</p> <p>Install a Model that is Already Registered</p> Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to install. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p> <p>Register and Install a Model</p> <p>Registration will place an entry for that model in the <code>user_models.json</code> file, which is located in the user's Lemonade cache (default: <code>~/.cache/lemonade</code>). Then, the model will be installed. Once the model is registered and installed, it will show up in the <code>models</code> endpoint alongside the built-in models and can be loaded.</p> <p>The <code>recipe</code> field defines which software framework and device will be used to load and run the model. For more information on OGA and Hugging Face recipes, see the Lemonade API README. For information on GGUF recipes, see llamacpp.</p> <p>Note: the <code>model_name</code> for registering a new model must use the <code>user</code> namespace, to prevent collisions with built-in models. For example, <code>user.Phi-4-Mini-GGUF</code>.</p> Parameter Required Description <code>model_name</code> Yes Namespaced Lemonade Server model name to register and install. <code>checkpoint</code> Yes HuggingFace checkpoint to install. <code>recipe</code> Yes Lemonade API recipe to load the model with. <code>reasoning</code> No Whether the model is a reasoning model, like DeepSeek (default: false). <code>mmproj</code> No Multimodal Projector (mmproj) file to use for vision models. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"user.Phi-4-Mini-GGUF\",\n    \"checkpoint\": \"unsloth/Phi-4-mini-instruct-GGUF:Q4_K_M\",\n    \"recipe\": \"llamacpp\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: user.Phi-4-Mini-GGUF\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1delete","title":"<code>POST /api/v1/delete</code>","text":"<p>Delete a model by removing it from local storage. If the model is currently loaded, it will be unloaded first.</p>"},{"location":"server/server_spec/#parameters_5","title":"Parameters","text":"Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to delete. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/delete \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Deleted model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#get-apiv1load","title":"<code>GET /api/v1/load</code>","text":"<p>Explicitly load a registered model into memory. This is useful to ensure that the model is loaded before you make a request. Installs the model if necessary.</p>"},{"location":"server/server_spec/#parameters_6","title":"Parameters","text":"Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to load. <p>Example request:</p> <pre><code>curl http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Loaded model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1unload","title":"<code>POST /api/v1/unload</code>","text":"<p>Explicitly unload a model from memory. This is useful to free up memory while still leaving the server process running (which takes minimal resources but a few seconds to start).</p>"},{"location":"server/server_spec/#parameters_7","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_4","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/unload\n</code></pre>"},{"location":"server/server_spec/#response-format_4","title":"Response format","text":"<p><pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Model unloaded successfully\"\n}\n</code></pre> In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1params","title":"<code>POST /api/v1/params</code>","text":"<p>Set the generation parameters for text completion. These parameters will persist across requests until changed.</p>"},{"location":"server/server_spec/#parameters_8","title":"Parameters","text":"Parameter Required Description <code>temperature</code> No Controls randomness in the output. Higher values (e.g. 0.8) make the output more random, lower values (e.g. 0.2) make it more focused and deterministic. Defaults to 0.7. <code>top_p</code> No Controls diversity via nucleus sampling. Keeps the cumulative probability of tokens above this value. Defaults to 0.95. <code>top_k</code> No Controls diversity by limiting to the k most likely next tokens. Defaults to 50. <code>min_length</code> No The minimum length of the generated text in tokens. Defaults to 0. <code>max_length</code> No The maximum length of the generated text in tokens. Defaults to 2048. <code>do_sample</code> No Whether to use sampling (true) or greedy decoding (false). Defaults to true."},{"location":"server/server_spec/#example-request_5","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/params \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"max_length\": 1000\n  }'\n</code></pre>"},{"location":"server/server_spec/#response-format_5","title":"Response format","text":"<p><pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Generation parameters set successfully\",\n  \"params\": {\n    \"temperature\": 0.8,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"min_length\": 0,\n    \"max_length\": 1000,\n    \"do_sample\": true\n  }\n}\n</code></pre> In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#get-apiv1health","title":"<code>GET /api/v1/health</code>","text":"<p>Check the health of the server. This endpoint will also return the currently loaded model.</p>"},{"location":"server/server_spec/#parameters_9","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_6","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/health\n</code></pre>"},{"location":"server/server_spec/#response-format_6","title":"Response format","text":"<pre><code>{\n  \"status\": \"ok\",\n  \"checkpoint_loaded\": \"amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid\",\n  \"model_loaded\": \"Llama-3.2-1B-Instruct-Hybrid\",\n}\n</code></pre>"},{"location":"server/server_spec/#get-apiv1stats","title":"<code>GET /api/v1/stats</code>","text":"<p>Performance statistics from the last request.</p>"},{"location":"server/server_spec/#parameters_10","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_7","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/stats\n</code></pre>"},{"location":"server/server_spec/#response-format_7","title":"Response format","text":"<pre><code>{\n  \"time_to_first_token\": 2.14,\n  \"tokens_per_second\": 33.33,\n  \"input_tokens\": 128,\n  \"output_tokens\": 5,\n  \"decode_token_times\": [0.01, 0.02, 0.03, 0.04, 0.05]\n}\n</code></pre>"},{"location":"server/server_spec/#get-apiv1system-info","title":"<code>GET /api/v1/system-info</code>","text":"<p>System information endpoint that provides complete hardware details and device enumeration.</p>"},{"location":"server/server_spec/#parameters_11","title":"Parameters","text":"Parameter Required Description Status <code>verbose</code> No Include detailed system information. When <code>false</code> (default), returns essential information (OS, processor, memory, devices). When <code>true</code>, includes additional details like Python packages and extended system information."},{"location":"server/server_spec/#example-request_8","title":"Example request","text":"Basic system informationDetailed system information <pre><code>curl \"http://localhost:8000/api/v1/system-info\"\n</code></pre> <pre><code>curl \"http://localhost:8000/api/v1/system-info?verbose=true\"\n</code></pre>"},{"location":"server/server_spec/#response-format_8","title":"Response format","text":"Basic response (verbose=false) <pre><code>{\n  \"OS Version\": \"Windows-10-10.0.26100-SP0\",\n  \"Processor\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n  \"Physical Memory\": \"32.0 GB\",\n  \"devices\": {\n    \"cpu\": {\n      \"name\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n      \"cores\": 12,\n      \"threads\": 24,\n      \"available\": true\n    },\n    \"amd_igpu\": {\n      \"name\": \"AMD Radeon(TM) 890M Graphics\",\n      \"memory_mb\": 512,\n      \"driver_version\": 32.0.12010.10001,\n      \"available\": true\n    },\n    \"amd_dgpu\": [],\n    \"npu\": {\n      \"name\": \"AMD NPU\",\n      \"driver_version\": \"32.0.203.257\",\n      \"power_mode\": \"Default\",\n      \"available\": true\n    }\n  }\n}\n</code></pre>"},{"location":"server/server_spec/#debugging","title":"Debugging","text":"<p>To help debug the Lemonade server, you can use the <code>--log-level</code> parameter to control the verbosity of logging information. The server supports multiple logging levels that provide increasing amounts of detail about server operations.</p> <pre><code>lemonade-server serve --log-level [level]\n</code></pre> <p>Where <code>[level]</code> can be one of:</p> <ul> <li>critical: Only critical errors that prevent server operation.</li> <li>error: Error conditions that might allow continued operation.</li> <li>warning: Warning conditions that should be addressed.</li> <li>info: (Default) General informational messages about server operation.</li> <li>debug: Detailed diagnostic information for troubleshooting, including metrics such as input/output token counts, Time To First Token (TTFT), and Tokens Per Second (TPS).</li> <li>trace: Very detailed tracing information, including everything from debug level plus all input prompts.</li> </ul>"},{"location":"server/server_spec/#experimental-gguf-support","title":"Experimental GGUF Support","text":"<p>The OGA models (<code>*-CPU</code>, <code>*-Hybrid</code>) available in Lemonade Server use Lemonade's built-in server implementation. However, Lemonade SDK v7.0.1 introduced experimental support for llama.cpp's Vulkan <code>llama-server</code> as an alternative backend for CPU and GPU.</p> <p>The <code>llama-server</code> backend works with Lemonade's suggested <code>*-GGUF</code> models, as well as any .gguf model from Hugging Face. Windows and Ubuntu Linux are supported. Details: - Lemonade Server wraps <code>llama-server</code> with support for the <code>lemonade-server</code> CLI, client web app, and endpoints (e.g., <code>models</code>, <code>pull</code>, <code>load</code>, etc.).   - The <code>chat/completions</code>, <code>embeddings</code>, and <code>reranking</code> endpoints are supported.    - Non-chat <code>completions</code>, and <code>responses</code> are not supported at this time. - A single Lemonade Server process can seamlessly switch between OGA and GGUF models.   - Lemonade Server will attempt to load models onto GPU with Vulkan first, and if that doesn't work it will fall back to CPU.   - From the end-user's perspective, OGA vs. GGUF should be completely transparent: they wont be aware of whether the built-in server or <code>llama-server</code> is serving their model.</p>"},{"location":"server/server_spec/#installing-gguf-models","title":"Installing GGUF Models","text":"<p>To install an arbitrary GGUF from Hugging Face, open the Lemonade web app by navigating to http://localhost:8000 in your web browser and click the Model Management tab.</p>"},{"location":"server/server_spec/#platform-support-matrix","title":"Platform Support Matrix","text":"Platform Vulkan GPU x64 CPU Windows \u2705 \u2705 Ubuntu \u2705 \u2705 Other Linux \u26a0\ufe0f* \u26a0\ufe0f* <p>*Other Linux distributions may work but are not officially supported.</p>"},{"location":"server/apps/","title":"Lemonade Server Examples","text":"<p>Many applications today utilize OpenAI models like ChatGPT through APIs such as:</p> <p><code>POST https://api.openai.com/v1/chat/completions</code></p> <p>This API call triggers the ChatGPT model to generate responses for a chat. With Lemonade Server, we are replacing the OpenAI endpoint with a local LLM. The new API call becomes:</p> <p><code>POST http://localhost:8000/api/v1/chat/completions</code></p> <p>This allows the same application to leverage local LLMs instead of relying on OpenAI's cloud-based models. The guides in this folder show how to connect Lemonade Server to popular applications to enable local LLM execution. To run these examples, you'll need a Windows PC.</p>"},{"location":"server/apps/#video-tutorials","title":"\ud83c\udfa5 Video Tutorials","text":"Links to the video tutorials available are provided in the third column of the following table.   App Guide Video Open WebUI How to chat with Lemonade LLMs in Open WebUI Watch Demo Continue.dev How to use Lemonade LLMs as a coding assistant in Continue Watch Demo Microsoft AI Toolkit Experimenting with Lemonade LLMs in VS Code using Microsoft's AI Toolkit Watch Demo GAIA An application for running LLMs locally, includes a ChatBot, YouTube Agent, and more Watch Demo Microsoft AI Dev Gallery Microsoft's showcase application for exploring AI capabilities coming soon CodeGPT How to use Lemonade LLMs as a coding assistant in CodeGPT coming soon MindCraft How to use Lemonade LLMs as a Minecraft agent coming soon wut Terminal assistant that uses Lemonade LLMs to explain errors coming soon AnythingLLM Running agents locally with Lemonade and AnythingLLM coming soon lm-eval A unified framework to test generative language models on a large number of different evaluation tasks. coming soon PEEL Using Local LLMs in Windows PowerShell coming soon"},{"location":"server/apps/#looking-for-installation-help","title":"\ud83d\udce6 Looking for Installation Help?","text":"<p>To set up Lemonade Server, check out the Lemonade Server guide for installation instructions and the server spec to learn more about the functionality. For more information about \ud83c\udf4b Lemonade SDK, see the Lemonade SDK README.</p>"},{"location":"server/apps/#support","title":"\ud83d\udee0\ufe0f Support","text":"<p>If you encounter any issues or have questions, feel free to:</p> <ul> <li>File an issue on our GitHub Issues page.</li> <li>Email us at lemonade@amd.com.</li> </ul>"},{"location":"server/apps/#want-to-add-an-example","title":"\ud83d\udca1 Want to Add an Example?","text":"<p>If you've connected Lemonade to a new application, feel free to contribute a guide by following our contribution guide found here or let us know at lemonade@amd.com.</p>"},{"location":"server/apps/ai-dev-gallery/","title":"AI Dev Gallery with Lemonade Server","text":""},{"location":"server/apps/ai-dev-gallery/#overview","title":"Overview","text":"<p>AI Dev Gallery is Microsoft's showcase application that demonstrates various AI capabilities through built-in samples and applications. It provides an easy way to explore and experiment with different AI models and scenarios, including text generation, chat applications, and more.</p> <p>AI Dev Gallery has native integration with Lemonade Server, which means it can automatically detect and connect to your local Lemonade instance without manual URL configuration.</p>"},{"location":"server/apps/ai-dev-gallery/#expectations","title":"Expectations","text":"<p>AI Dev Gallery works well with most models available in Lemonade. The built-in samples are designed to work with various model types and sizes, making it a great tool for testing and exploring different AI capabilities locally.</p> <p>The application provides a user-friendly interface for experimenting with AI models through pre-built scenarios, making it accessible for both beginners and advanced users.</p>"},{"location":"server/apps/ai-dev-gallery/#setup","title":"Setup","text":""},{"location":"server/apps/ai-dev-gallery/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Important: Make sure your Lemonade Server is running before opening AI Dev Gallery.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#install-ai-dev-gallery","title":"Install AI Dev Gallery","text":"<ol> <li>Open the Microsoft Store on Windows.</li> <li>Search for \"AI Dev Gallery\" by Microsoft Corporation.</li> <li>Click \"Install\" to download and install the application.</li> </ol> <p>Alternatively, you can access AI Dev Gallery directly through aka.ms/ai-dev-gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#connect-to-lemonade","title":"Connect to Lemonade","text":"<p>AI Dev Gallery has native integration with Lemonade Server, so no manual configuration is required. The application will automatically detect your running Lemonade Server instance.</p> <p>Important: Ensure your Lemonade Server is running before launching AI Dev Gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#usage","title":"Usage","text":"<p>AI Dev Gallery provides various built-in applications and samples to explore AI capabilities:</p>"},{"location":"server/apps/ai-dev-gallery/#quick-start","title":"Quick Start","text":"<ol> <li>Launch AI Dev Gallery.</li> <li>Navigate to Samples \u2192 Text \u2192 Chat (or another text/code sample).</li> <li>Click on the model selector above the chat window.</li> <li>Select Lemonade from the available providers.</li> <li>Choose your preferred model from the list of available models.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#supported-scenarios","title":"Supported Scenarios","text":"<p>AI Dev Gallery supports various AI scenarios through its sample applications with Lemonade integration:</p> <p>Text Processing:</p> <ul> <li>Conversational AI: Chat and Semantic Kernel Chat for interactive conversations</li> <li>Content Generation: Generate text for various purposes and creative writing</li> <li>Language Tasks: Translation, grammar checking, and paraphrasing</li> <li>Text Analysis: Sentiment analysis and content moderation</li> <li>Information Retrieval: Semantic search and retrieval augmented generation</li> <li>Text Enhancement: Summarization and custom parameter configurations</li> </ul> <p>Code Assistance:</p> <ul> <li>Code Generation: Create code snippets and programs</li> <li>Code Analysis: Explain existing code and understand functionality</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#tips-for-best-experience","title":"Tips for Best Experience","text":"<ul> <li>Start your Lemonade Server before opening AI Dev Gallery</li> <li>Try different models to see how they perform across various scenarios</li> <li>Explore different sample categories to understand various AI capabilities</li> <li>Use the built-in samples as starting points for your own AI experiments</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#troubleshooting","title":"Troubleshooting","text":""},{"location":"server/apps/ai-dev-gallery/#ai-dev-gallery-doesnt-detect-lemonade","title":"AI Dev Gallery doesn't detect Lemonade","text":"<ul> <li>Ensure Lemonade Server is running and accessible at <code>http://localhost:8000</code></li> <li>Restart AI Dev Gallery after ensuring Lemonade Server is running</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#models-not-appearing-in-the-selector","title":"Models not appearing in the selector","text":"<ul> <li>Open <code>http://localhost:8000</code> in a browser and make sure to download the models you want to use through the \"Model Manager\" tab.</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Dev Gallery Website</li> <li>Lemonade Server Models</li> </ul>"},{"location":"server/apps/ai-toolkit/","title":"Microsoft AI Toolkit for VS Code","text":""},{"location":"server/apps/ai-toolkit/#overview","title":"Overview","text":"<p>The AI Toolkit for Visual Studio Code is a VS Code extension that simplifies generative AI app development by bringing together cutting-edge AI development tools and models from various catalogs. It supports running AI models locally or connecting to remote models via API keys.</p>"},{"location":"server/apps/ai-toolkit/#demo-video","title":"Demo Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p>"},{"location":"server/apps/ai-toolkit/#expectations","title":"Expectations","text":"<p>We have found that most LLMs work well with this application. </p> <p>However, the <code>Inference Parameters</code> option is not fully supported, as Lemonade Server currently does not accept those as inputs (see server_spec.md for details).</p>"},{"location":"server/apps/ai-toolkit/#setup","title":"Setup","text":""},{"location":"server/apps/ai-toolkit/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/ai-toolkit/#install-ai-toolkit-for-vs-code","title":"Install AI Toolkit for VS Code","text":"<ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search for \"AI Toolkit for Visual Studio Code\" in the Extensions Marketplace search bar.</li> <li>Select the AI Toolkit extension and click install.</li> </ol> <p>This will add an AI Toolkit icon to your VS Code Activity Bar.</p>"},{"location":"server/apps/ai-toolkit/#connect-lemonade-to-ai-toolkit","title":"Connect Lemonade to AI Toolkit","text":"<p>The AI Toolkit now supports \"Bring Your Own Model\" functionality, allowing you to connect to models served via the OpenAI API standard, which Lemonade uses.</p> <ol> <li>Open the AI Toolkit tab in your VS Code Activity Bar.</li> <li>In the right corner of the \"My Models\" section, click the \"+\" button to \"Add model for remote inference\".</li> <li>Select \"Add a custom model\".</li> <li>When prompted to \"Enter OpenAI chat completion endpoint URL\" enter:     <pre><code>http://localhost:8000/api/v1/chat/completions\n</code></pre></li> <li>When prompted to \"Enter the exact model name as in the API\" select a model (e.g., <code>Phi-3-Mini-Instruct-Hybrid</code>)<ul> <li>Note: You can get a list of all models available here.</li> </ul> </li> <li>Select the same name as the display model name.</li> <li>Skip the HTTP authentication step by pressing \"Enter\".</li> </ol>"},{"location":"server/apps/ai-toolkit/#usage","title":"Usage","text":"<p>Once you've set up the Lemonade model in AI Toolkit, you can:</p> <ol> <li>Use the AI Playground tool to directly interact with your added model.</li> <li>Use the Prompt Builder tool to craft effective prompts for your AI models.</li> <li>Use the Bulk Run tool to compute responses for custom datasets and easily visualize those responses on a table format.</li> <li>Use the Evaluation tool to quickly assess your model's coherence, fluency, relevance, and similarity, as well as to compute BLEU, F1, GLEU, and Meteor scores.</li> </ol>"},{"location":"server/apps/ai-toolkit/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Toolkit for VS Code Documentation</li> <li>AI Toolkit GitHub Repository</li> <li>Bring Your Own Models on AI Toolkit</li> </ul>"},{"location":"server/apps/anythingLLM/","title":"Running agents locally with Lemonade and AnythingLLM","text":""},{"location":"server/apps/anythingLLM/#overview","title":"Overview","text":"<p>AnythingLLM is a versatile local LLM platform that allows you to chat with your documents and code using a variety of models. It supports the OpenAI-compatible API interface, allowing easy integration with local servers like Lemonade.</p> <p>This guide will help you configure AnythingLLM to use Lemonade's OpenAI-compatible server, and utilize the powerful <code>@agent</code> capability to interact with documents, webpages, and more.</p>"},{"location":"server/apps/anythingLLM/#expectations","title":"Expectations","text":"<p>Lemonade integrates best with AnythingLLM when using models such as <code>Qwen-1.5-7B-Chat-Hybrid</code> and <code>Llama-3.2-1B-Instruct-Hybrid</code>, both of which support a context length of up to 3,000 tokens.</p> <p>Keep in mind that when using the <code>@agent</code> feature, multi-turn conversations can quickly consume available context. As a result, the number of back-and-forth turns in a single conversation may be limited due to the growing context size.</p>"},{"location":"server/apps/anythingLLM/#setup","title":"Setup","text":""},{"location":"server/apps/anythingLLM/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Install and set up AnythingLLM from their GitHub or website.</li> </ol>"},{"location":"server/apps/anythingLLM/#configure-anythingllm-to-use-lemonade","title":"Configure AnythingLLM to Use Lemonade","text":"<ol> <li>In the bottom of the left menu, click on the wrench icon to \"Open Settings\".</li> <li>Under the menu \"AI Providers\", click \"LLM\".</li> <li>     Select \"Generic OpenAI\" and enter the following info:      SettingValue Base URL<code>http://localhost:8000/api/v1</code> API Key<code>-</code> Chat Model Name<code>Qwen-1.5-7B-Chat-Hybrid</code> Token context window<code>3000</code> Max Tokens<code>3000</code> </li> <li>In the bottom left, click the back button to exit.</li> <li>In the left menu, click \"New Workspace\" and give it a name.</li> <li>Where you see your new workspace, click the gear icon to open the \"Workspace Settings\"</li> <li>In the top menu of the window that opens, click on \"Agent Configuration\"</li> <li>Under Chat Settings, select Generic OpenAI and click save.</li> <li>Under Workspace Agent LLM Provider, select \"Generic OpenAI\" and click save.</li> </ol>"},{"location":"server/apps/anythingLLM/#usage-with-agent","title":"Usage with @agent","text":""},{"location":"server/apps/anythingLLM/#overview_1","title":"Overview","text":"<p>Agents are capable of scraping websites, listing and summarizing documents, searching the web, creating charts, and even saving files to your desktop or their own memory.</p> <p>To start an agent session, simply go to any workspace and type <code>@agent &lt;your prompt&gt;</code>. To exit the session, just type <code>exit</code>.</p>"},{"location":"server/apps/anythingLLM/#agent-skills","title":"Agent Skills","text":"<p>You may turn on and off specific <code>Agent Skills</code> by going to your <code>Workspace Settings</code> \u2192 <code>Agent Configuration</code> \u2192 <code>Configure Agent Skills</code>.</p> <p>Available agent skills include:</p> <ul> <li>RAG &amp; long-term memory</li> <li>View and summarize documents</li> <li>Scrape Websites</li> <li>Generate &amp; save files to browser</li> <li>Generate Charts</li> <li>Web Search</li> <li>SQL Connector</li> </ul>"},{"location":"server/apps/anythingLLM/#examples","title":"Examples","text":"<p>Here are some examples on how you can interact with Anything LLM agents:</p> <ul> <li>Rag &amp; long-term memory<ul> <li><code>@agent My name is Dr Lemon. Remember this in our next conversation</code></li> <li>Then, on a follow up chat you can ask <code>@agent What is my name according to your memory?</code></li> </ul> </li> <li>Scrape Websites<ul> <li><code>@agent Scrape this website and tell me what are the two ways of installing lemonade https://github.com/lemonade-sdk/lemonade/blob/main/docs/server/README.md</code></li> </ul> </li> <li>Web Search (enable skill before trying)<ul> <li><code>@agent Search the web for the best place to buy shoes</code></li> </ul> </li> </ul> <p>You can find more details about agent usage here.</p>"},{"location":"server/apps/anythingLLM/#additional-resources","title":"Additional Resources","text":"<ul> <li>AnthingLLM Website</li> <li>AnythingLLM GitHub</li> <li>AnythingLLM Documentation</li> </ul>"},{"location":"server/apps/codeGPT/","title":"CodeGPT with VS Code","text":""},{"location":"server/apps/codeGPT/#overview","title":"Overview","text":"<p>CodeGPT Chat is an AI-powered chatbot designed to assist developers with coding tasks directly within their preferred integrated development environments (IDEs), for example, VS Code.</p>"},{"location":"server/apps/codeGPT/#expectations","title":"Expectations","text":"<p>We have found that the <code>Qwen-1.5-7B-Chat-Hybrid</code> model is the best Hybrid model available for coding. It is good at chatting with a few files at a time in your codebase to learn more about them. It can also make simple code editing suggestions pertaining to a few lines of code at a time.</p> <p>However, we do not recommend using this model for analyzing large codebases at once or making large or complex file edits.</p>"},{"location":"server/apps/codeGPT/#setup","title":"Setup","text":""},{"location":"server/apps/codeGPT/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/codeGPT/#install-codegpt-in-vs-code","title":"Install CodeGPT in VS Code","text":"<p>The following instructions are based off CodeGPT provided instructions found here.</p> <ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search \"CodeGPT: Chat &amp; AI Agents\" in the Extensions Marketplace search bar.</li> <li>Select the CodeGPT extension and click install.</li> </ol> <p>This will add a CodeGPT tab to your VS Code Activity Bar.</p>"},{"location":"server/apps/codeGPT/#add-lemonade-server-to-codegpt","title":"Add Lemonade Server to CodeGPT","text":"<p>Note: The following instructions are based on instructions from CodeGPT found here.</p> <ol> <li>Open the CodeGPT tab in your VS Code Activity Bar.</li> <li>Sign Up or Sign into your account.</li> <li>In the model dropdown menu and click \"View More\".</li> <li>Select the tab: \"LLMs Cloud model\"</li> <li>Under \"All Models\", set the following:    FieldValue Select Provider:<code>Custom</code> Select Model: <code>Qwen-1.5-7B-Chat-Hybrid</code> </li> <li>Click \"Change connection settings\" and enter the following information:      FieldValue API Key<code>-</code> Custom Link<code>http://localhost:8000/api/v1/api/v1</code> </li> </ol>"},{"location":"server/apps/codeGPT/#usage","title":"Usage","text":"<p>Note: see the CodeGPT user guide to learn about all of their features.</p> <p>To try out CodeGPT:</p> <ul> <li>Open the CodeGPT tab in your VS Code Activity Bar, and in the chat box, type a question about your code. Use the <code>#</code> symbol to specify a file.</li> <li>Example: \"What's the fastest way to install lemonade in #getting_started.md?\"</li> <li>Use /Fix to find and fix a minor bug.</li> <li>Use /Document to come up with docstrings and comments for a file.</li> <li>Use /UnitTest to make a  test file.</li> </ul>"},{"location":"server/apps/continue/","title":"Continue Coding Assistant","text":""},{"location":"server/apps/continue/#overview","title":"Overview","text":"<p>Continue is a coding assistant that lives inside of a VS Code extension. It supports chatting with your codebase, making edits, and a lot more.</p>"},{"location":"server/apps/continue/#demo-video","title":"Demo Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p>"},{"location":"server/apps/continue/#expectations","title":"Expectations","text":"<p>We have found that the <code>Qwen-1.5-7B-Chat-Hybrid</code> model is the best Hybrid model available for coding. It is good at chatting with a few files at a time in your codebase to learn more about them. It can also make simple code editing suggestions pertaining to a few lines of code at a time.</p> <p>However, we do not recommend using this model for analyzing large codebases at once or making large or complex file edits.</p>"},{"location":"server/apps/continue/#setup","title":"Setup","text":""},{"location":"server/apps/continue/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/continue/#install-continue","title":"Install Continue","text":"<p>Note: they provide their own instructions here</p> <ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search \"Continue - Codestral, Claude, and more\" in the Extensions Marketplace search bar.</li> <li>Select the Continue extension and click install.</li> </ol> <p>This will add a Continue tab to your VS Code Activity Bar.</p>"},{"location":"server/apps/continue/#add-lemonade-server-to-continue","title":"Add Lemonade Server to Continue","text":"<p>Note: The following instructions are based on instructions from Continue found here </p> <ol> <li>Open the Continue tab in your VS Code Activity Bar.</li> <li>Click the chat box. Some buttons will appear at the bottom of the box, including <code>Select model</code>.</li> <li>Click <code>Select model</code>, then <code>+ Add Chat model</code> to open the new model dialog box.</li> <li>Click the <code>config file</code> link at the very bottom of the dialog to open <code>config.yaml</code>.</li> <li>Replace the \"models\" key in the <code>config.yaml</code> with the following and save:</li> </ol> <pre><code>models:\n  - name: Lemonade\n    provider: openai\n    model: Qwen-1.5-7B-Chat-Hybrid \n    apiBase: http://localhost:8000/api/v1\n    apiKey: none\n</code></pre> <ol> <li>Close the dialog box.</li> <li>Click the chat box again. You should see <code>Lemonade</code> where you used to see <code>Select model</code>. Ready!</li> </ol>"},{"location":"server/apps/continue/#usage","title":"Usage","text":"<p>Note: see the Continue user guide to learn about all of their features.</p> <p>Here are some examples for trying out Continue. These examples assume you have cloned this repo and allowed Continue to index it.</p>"},{"location":"server/apps/continue/#chat-with-files","title":"Chat with Files","text":"<p>Open the Continue tab in your VS Code Activity Bar, and in the \"Ask anything\" box, type a question about your code. Use the <code>@</code> symbol to specify a file or tool.</p> <ul> <li>\"What's the fastest way to install Lemonade in <code>@getting_started.md?</code>\"</li> <li>\"According to <code>@README.md</code> what do I need to do to set up for <code>@api_oga_hybrid_streaming.py</code>?\"</li> </ul>"},{"location":"server/apps/continue/#editing-files","title":"Editing Files","text":"<p>Open a file, select some code, and push Ctrl+I to start a chat about editing that code.</p> <ol> <li>Open <code>//examples//lemonade//api_basic.py</code>.</li> <li>Select the <code>print(...</code> line at the bottom and press <code>ctrl+i</code>.</li> <li>Write \"Add a helpful comment\" in the chat box and press enter.</li> <li>Press \"accept\" if you would like to accept the change.</li> </ol>"},{"location":"server/apps/continue/#making-files","title":"Making Files","text":"<p>Start a new chat and prompt: </p> <p>write a script in the style of <code>@api_basic.py</code> that uses the microsoft/Phi-4-mini-instruct model on GPU</p> <p>Here's what we got:</p> <pre><code># Import necessary modules\nfrom lemonade.api import from_pretrained\n\n# Load the Phi-4-mini-instruct model with the hf-cpu recipe\nmodel, tokenizer = from_pretrained(\"microsoft/Phi-4-mini-instruct\", recipe=\"hf-cpu\")\n\n# Define your prompt\nprompt = \"This is a sample prompt for the Phi-4-mini-instruct model\"\n\n# Tokenize the prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate the response using the model\nresponse = model.generate(input_ids, max_new_tokens=100)  # Adjust the max_new_tokens as needed\n\n# Decode the generated response\ngenerated_text = tokenizer.decode(response[0])\n\n# Print the response\nprint(generated_text)\n</code></pre>"},{"location":"server/apps/lm-eval/","title":"Using LM-Evaluation-Harness with Lemonade","text":"<p>This guide demonstrates how to use Lemonade with LM-Evaluation-Harness (lm-eval) to evaluate language model performance across a variety of standardized benchmarks. Whether you're comparing different model implementations or validating model capabilities, lm-eval provides a comprehensive framework for model assessment. Refer to Lemonade Server to learn more about the server interface used by lm-eval for evaluations.</p>"},{"location":"server/apps/lm-eval/#what-is-lm-evaluation-harness","title":"What is LM-Evaluation-Harness?","text":"<p>LM-Evaluation-Harness (often called <code>lm-eval</code>) is an open-source framework for evaluating language models across a wide variety of tasks and benchmarks. Developed by EleutherAI, it has become a standard tool in the AI research community for consistent evaluation of language model capabilities.</p> <p>The framework supports evaluating models on more than 200 tasks and benchmarks, including popular ones such as:</p> <ul> <li>MMLU (Massive Multitask Language Understanding)</li> <li>GSM8K (Grade School Math)</li> <li>HumanEval (Code generation)</li> <li>TruthfulQA</li> <li>WinoGrande</li> <li>HellaSwag</li> <li>And many others...</li> </ul>"},{"location":"server/apps/lm-eval/#advantages-of-using-lm-eval-for-accuracy-measurement","title":"Advantages of Using lm-eval for Accuracy Measurement","text":"<ul> <li>Standardization: Provides a consistent methodology for comparing different models, ensuring fair comparisons across the industry.</li> <li>Community adoption: Used by major research labs, companies, and the open-source community including Hugging Face, Anthropic, and others.</li> <li>Comprehensive evaluation: Covers a wide range of capabilities from factual knowledge to reasoning.</li> <li>Open-source: Transparent methodology that's peer-reviewed by the AI research community.</li> <li>Regular updates: Continuously updated with new benchmarks and evaluation methods.</li> <li>Reproducibility: Enables reproducible research results across different models and implementations.</li> <li>Cross-implementation compatibility: Works with multiple model implementations (llama.cpp, OpenAI API, Hugging Face, etc.) enabling direct comparison of different implementations of the same model.</li> </ul>"},{"location":"server/apps/lm-eval/#running-lm-eval-with-lemonade","title":"Running lm-eval with Lemonade","text":"<p>Lemonade supports integration with lm-eval through its local LLM server. The basic workflow involves:</p> <ol> <li>Setting up the environment.</li> <li>Starting the Lemonade server.</li> <li>Loading a model via the API.</li> <li>Running lm-eval tests against the model through the lemonade server.</li> </ol>"},{"location":"server/apps/lm-eval/#step-1-environment-setup-and-installation","title":"Step 1: Environment Setup and Installation","text":"<p>Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p>"},{"location":"server/apps/lm-eval/#step-2-start-the-lemonade-server","title":"Step 2: Start the Lemonade Server","text":"<p>In a terminal with your environment activated, run the following command:</p> <pre><code>lemonade-server-dev serve\n</code></pre> <p>This starts a local LLM server on port 8000 by default.</p>"},{"location":"server/apps/lm-eval/#step-3-load-a-model","title":"Step 3: Load a Model","text":"<p>Use the following PowerShell command to load a model into the server:</p> <pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> <p>Where:</p> <ul> <li><code>checkpoint</code> can be changed to use other from Hugging Face (e.g., \"meta-llama/Llama-3.2-3B-Instruct\")</li> <li><code>recipe</code> can be changed to use different backends (e.g., \"oga-cpu\" for CPU inference on OnnxRuntime GenAI, \"oga-hybrid\" for AMD Ryzen\u2122 AI acceleration). For more information on Lemonade recipes, see the Lemonade API ReadMe.</li> </ul>"},{"location":"server/apps/lm-eval/#step-4-run-lm-eval-tests","title":"Step 4: Run lm-eval Tests","text":"<p>Now that the model is loaded, open a new PowerShell terminal, activate your environment, and run lm-eval tests using the following command:</p> <pre><code>lm_eval --model local-completions --tasks mmlu_abstract_algebra --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre> <p>Where: - Change <code>--tasks</code> as needed to run other tests (e.g., <code>--tasks gsm8k</code>, <code>--tasks wikitext</code>, etc.) For detailed tasks visit lm-eval - <code>checkpoint name</code> should match the model name loaded in step 2</p>"},{"location":"server/apps/lm-eval/#types-of-tests-in-lm-eval","title":"Types of Tests in lm-eval","text":"<p>The framework implements three primary evaluation methodologies that use different capabilities of language models:</p>"},{"location":"server/apps/lm-eval/#1-log-probability-based-tests-eg-mmlu","title":"1. Log Probability-Based Tests (e.g., MMLU)","text":"<p>These tests evaluate a model's ability to assign probabilities to different possible answers. The model predicts which answer is most likely based on conditional probabilities.</p> <p>Example: In MMLU (Massive Multitask Language Understanding), the model is given a multiple-choice question and must assign probabilities to each answer choice. The model's performance is measured by how often it assigns the highest probability to the correct answer.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run MMLU Tests</p> <pre><code>lm_eval --model local-completions --tasks mmlu_abstract_algebra --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#2-rolling-log-probability-tests-eg-wikitext","title":"2. Rolling Log Probability Tests (e.g., WikiText)","text":"<p>These tests evaluate a model's ability to predict text by measuring the perplexity on held-out data. The model assigns probabilities to each token in a sequence, and performance is measured by how well it predicts the actual next tokens.</p> <p>Example: In perplexity benchmarks like WikiText, the model is evaluated on how well it can predict each token in a document, using a rolling window approach for longer contexts.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests_1","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run Wikitext Tests</p> <pre><code>lm_eval --model local-completions --tasks wikitext --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#3-generation-based-tests-eg-gsm8k","title":"3. Generation-Based Tests (e.g., GSM8K)","text":"<p>These tests evaluate a model's ability to generate full responses to prompts. The model generates text that is then evaluated against reference answers or using specific metrics.</p> <p>Example: In GSM8K (Grade School Math), the model is given a math problem and must generate a step-by-step solution. Performance is measured by whether the final answer is correct.</p>"},{"location":"server/apps/lm-eval/#commands-to-log-probability-based-tests_2","title":"Commands to Log Probability-Based Tests","text":"<p>Step 1: Environment setup and installation - Please refer to the installation guide, using the PyPI or From Source methods, for environment setup.</p> <p>Step 2: Start the Lemonade Server.</p> <p>In a terminal with your environment activated, run the following command:</p> <p><pre><code>lemonade-server-dev serve\n</code></pre> Step 3: Load a Model</p> <p><pre><code>Invoke-RestMethod -Uri \"http://localhost:8000/api/v1/load\" -Method Post -Headers @{ \"Content-Type\" = \"application/json\" } -Body '{ \"checkpoint\": \"meta-llama/Llama-3.2-1B-Instruct\", \"recipe\": \"hf-cpu\" }'\n</code></pre> Step 4: Run GSM8k Tests</p> <pre><code>lm_eval --model local-completions --tasks gsm8k --model_args model=meta-llama/Llama-3.2-1B-Instruct,base_url=http://localhost:8000/api/v1/completions,num_concurrent=1,max_retries=0,tokenized_requests=False --limit 5\n</code></pre>"},{"location":"server/apps/lm-eval/#interpreting-results","title":"Interpreting Results","text":"<p>lm-eval provides detailed results for each benchmark, typically including:</p> <ul> <li>Accuracy: Percentage of correct answers.</li> <li>Exact Match: For generation tasks, whether the generated answer exactly matches the reference.</li> <li>F1 Score: For certain tasks, measuring the overlap between generated and reference answers.</li> <li>Perplexity: For language modeling tasks, measuring how well the model predicts text.</li> <li>Group breakdowns: For some benchmarks, performance across different categories or question types.</li> </ul> <p>Results are provided in a structured format at the end of evaluation, with both detailed and summary statistics.</p>"},{"location":"server/apps/lm-eval/#future-work","title":"Future Work","text":"<ul> <li>Integrate lm-eval as a Lemonade tool: Direct integration into the Lemonade CLI ecosystem.</li> </ul>"},{"location":"server/apps/lm-eval/#references","title":"References","text":"<ul> <li>LM-Evaluation-Harness GitHub Repository</li> <li>EleutherAI Documentation </li> </ul>"},{"location":"server/apps/mindcraft/","title":"Mindcraft","text":""},{"location":"server/apps/mindcraft/#overview","title":"Overview","text":"<p>Mindcraft is an open-source project that creates Minecraft bots powered by large language models (LLMs) to engage with the game and its players. This readme will demonstrate how to integrate Lemonade to use local LLMs with Mindcraft.</p>"},{"location":"server/apps/mindcraft/#expectations","title":"Expectations","text":"<p>We found the <code>Qwen-1.5-7B-Chat-Hybrid</code> model to be the most effective for this task, delivering fast responses with higher accuracy. However, as a smaller model running locally, with a limited context length, it may occasionally struggle with certain requests\u2014for instance, it might attempt to build a structure, but the result may not be correct. For more detailed information, please refer to the Data Insights section.</p>"},{"location":"server/apps/mindcraft/#setup","title":"Setup","text":""},{"location":"server/apps/mindcraft/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Obtain a copy of Minecraft from Microsoft. 1.20.4 of the JAVA Edition is required for this. You can obtain that version by following these instructions.</li> <li>Install Node.js (at least v14).</li> </ol>"},{"location":"server/apps/mindcraft/#environment-setup","title":"Environment Setup","text":"<p>Clone Mindcraft from GitHub:</p> <ul> <li><code>git checkout 07ea071ac3b0d4954d62b09d881c38a06bc2a589</code>: this will ensure the code base is equal to where this test was performed.</li> </ul> <p>In the clone of the <code>mindcraft</code> repository:</p> <ul> <li>Rename the file <code>keys.example.json</code> to <code>keys.json</code>. You do not need to edit the contents of this JSON file.</li> <li>Update <code>keys.json</code> by adding any text to the value of <code>OPENAI_API_KEY</code>.</li> </ul> <pre><code>{\n    \"OPENAI_API_KEY\": \"&lt;put any text here&gt;\",\n    \"OPENAI_ORG_ID\": \"\",\n    \"GEMINI_API_KEY\": \"\",\n    \"ANTHROPIC_API_KEY\": \"\",\n    \"REPLICATE_API_KEY\": \"\",\n    \"GROQCLOUD_API_KEY\": \"\",\n    \"HUGGINGFACE_API_KEY\": \"\",\n    \"QWEN_API_KEY\": \"\",\n    \"XAI_API_KEY\": \"\",\n    \"MISTRAL_API_KEY\": \"\",\n    \"DEEPSEEK_API_KEY\": \"\",\n    \"NOVITA_API_KEY\": \"\",\n    \"OPENROUTER_API_KEY\": \"\"\n}\n</code></pre> <ul> <li>In a terminal/command prompt, run <code>npm install</code> from the cloned Mindcraft directory.</li> <li>Replace the contents of the file <code>andy.json</code> with the following:</li> </ul> <pre><code>{\n    \"name\": \"Andy\",\n    \"model\": {\n        \"model\": \"Qwen-1.5-7B-Chat-Hybrid\",\n        \"url\": \"http://localhost:8000/api/v1\",\n        \"params\": {\n            \"temperature\": 0.5\n        }\n    },\n    \"modes\": {\n        \"self_preservation\": true,\n        \"unstuck\": true,\n        \"cowardice\": false,\n        \"self_defense\": true,\n        \"hunting\": true,\n        \"item_collecting\": true,\n        \"torch_placing\": true,\n        \"elbow_room\": true,\n        \"idle_staring\": true,\n        \"cheat\": true\n    }\n}\n</code></pre> <ul> <li>Find the line in <code>src/models/prompter.js</code> that says:</li> </ul> <pre><code>else if (profile.model.includes('gpt') || profile.model.includes('o1')|| profile.model.includes('o3'))\n</code></pre> <ul> <li>... and replace it with the following:</li> </ul> <pre><code>else if ([\n    'Qwen-1.5-7B-Chat-Hybrid',\n    'Llama-3.2-1B-Instruct-Hybrid'\n].includes(profile.model) || profile.model.includes('gpt') || profile.model.includes('o1') || profile.model.includes('o3'))\n</code></pre>"},{"location":"server/apps/mindcraft/#launching-everything","title":"Launching Everything","text":"<ul> <li>Start Lemonade Server by double-clicking the desktop icon \ud83c\udf4b.</li> <li>Start a Minecraft world and open it to LAN on localhost port <code>55916</code>.<ul> <li>This is done by pressing the ESC button to open the menu, then click \"Open to LAN\" and enter the Port Number: <code>55916</code>.</li> <li>Click \"Start LAN World\". For instructions on how Open to LAN, see the section \"Hosting a LAN Server\" for the JAVA Edition in this wiki article.</li> </ul> </li> <li>Run <code>node main.js</code> from the installed directory.</li> <li>In Minecraft, to give the agent commands, press <code>t</code> and enter the command. For example:<ul> <li>\"come here\"</li> <li>\"hunt pigs\" - you and the agent must be close to some pigs to do this.</li> </ul> </li> </ul>"},{"location":"server/apps/mindcraft/#model-configurations","title":"Model Configurations","text":"<p>Lemonade models tested:</p> <ul> <li>Llama-3.2-1B-Instruct-Hybrid</li> <li>Qwen-1.5-7B-Chat-Hybrid \ud83d\udc4d</li> </ul>"},{"location":"server/apps/mindcraft/#challenges-and-observations","title":"Challenges and Observations","text":"<ol> <li>The current MindCraft configuration has a tendency to send very large context that is resent. The context will include examples of behaviors that may not be necessary, generating over 2100 input tokens for simple commands. Further testing would be required to understand model behavior based on reduced context size.</li> <li>Frequent token limit breaches resulting in timeouts or incomplete responses due to the aforementioned context size problem. Once the token maximum content ceiling is raised, further testing would be prudent using the same <code>Qwen-1.5-7B-Chat-Hybrid</code> model as a baseline and using it to test other models such as DeepSeek and Llama variants. content</li> <li>High GPU resource consumption during model inference, impacting system performance.</li> </ol>"},{"location":"server/apps/mindcraft/#results","title":"Results","text":"<p>The <code>Qwen-1.5-7B-Chat-Hybrid</code> model showed the most potential, responding to commands and attempting construction tasks, though with limited accuracy.</p>"},{"location":"server/apps/mindcraft/#recommendations","title":"Recommendations","text":"<ol> <li>Optimize examples sent to Lemonade for conciseness to reduce token usage.</li> <li>Reduce input context to prevent model overload.</li> </ol>"},{"location":"server/apps/mindcraft/#data-insights","title":"Data Insights","text":"<p>The following are examples of requests made by the Mindcraft software to the Lemonade Server and how the tokens were interpreted. These examples are taken from an initial game stage, including the first request sent to the Lemonade Server and a subsequent user chat that says, \"come here.\" The purpose is to show how large the context is that is being sent. This could be optimized for more efficient performance and results.</p> <ul> <li>Initial Payload</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Clear\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\nExample 2:\\nUser input: miner_32: Hey! What are you up to?\\nYour output:\\nNothing much miner_32, what do you need?\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Initial Response</li> </ul> <pre><code>TRACE:    ::1:56880 - HTTP connection made\nTRACE:    ::1:56880 - ASGI [4] Started scope={'type': 'http', 'asgi': {'version': '3.0', 'spec_version': '2.3'}, 'http_version': '1.1', 'server': ('::1', 8000), 'client': ('::1', 56880), 'scheme': 'http', 'root_path': '', 'headers': '&lt;...&gt;', 'state': {}, 'method': 'POST', 'path': '/api/v1/chat/completions', 'raw_path': b'/api/v1/chat/completions', 'query_string': b''}\nTRACE:    ::1:56880 - ASGI [4] Receive {'type': 'http.request', 'body': '&lt;8378 bytes&gt;', 'more_body': False}\nDEBUG:    Input Tokens: 2036\nTRACE:    Input Message: &lt;|im_start|&gt;system\nYou are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\n\n\nSTATS\n- Position: x: 6.50, y: -60.00, z: 28.50\n- Gamemode: creative\n- Health: 20 / 20\n- Hunger: 20 / 20\n- Biome: plains\n- Weather: Clear\n- Block Below: grass_block\n- Block at Legs: air\n- Block at Head: air\n- First Solid Block Above Head: none\n- Time: Afternoon- Current Action: Idle\n- Nearby Human Players: Transpier\n- Nearby Bot Players: None.\nAgent Modes:\n- self_preservation(ON)\n- unstuck(ON)\n- cowardice(ON)\n- self_defense(ON)\n- hunting(ON)\n- item_collecting(ON)\n- torch_placing(ON)\n- elbow_room(ON)\n- idle_staring(ON)\n- cheat(ON)\n\n\n\nINVENTORY: Nothing\nWEARING: Nothing\n\n\n*COMMAND DOCS\nYou can use the following commands to perform actions and get information about the world. \n    Use the commands with the syntax: !commandName or !commandName(\"arg1\", 1.2, ...) if the command takes arguments.\n\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\n!stats: Get your bot's location, health, hunger, and time of day.\n!inventory: Get your bot's inventory.\n!nearbyBlocks: Get the blocks near the bot.\n!craftable: Get the craftable items with the bot's inventory.\n!entities: Get the nearby players and entities.\n!modes: Get all available modes and their docs and see which are on/off.\n!savedPlaces: List all saved locations.\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\nParams:\ntargetItem: (string) The item that we are trying to craft\nquantity: (number) The quantity of the item that we are trying to craft\n!help: Lists all available commands and their descriptions.\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\nParams:\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.     \n!stop: Force stop all actions and commands that are currently executing.\n!stfu: Stop all chatting and self prompting, but continue current action.\n!restart: Restart the agent process.\n!clearChat: Clear the chat history.\n!goToPlayer: Go to the given player.\nParams:\nplayer_name: (string) The name of the player to go to.\ncloseness: (number) How close to get to the player.\n!followPlayer: Endlessly follow the given player.\nParams:\nplayer_name: (string) name of the player to follow.\nfollow_dist: (number) The distance to follow from.\n!goToCoordinates: Go to the given x, y, z location.\nParams:\nx: (number) The x coordinate.\ny: (number) The y coordinate.\nz: (number) The z coordinate.\ncloseness: (number) How close to get to the location.\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\nParams:\ntype: (string) The block type to go to.\nsearch_range: (number) The range to search for the block.\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\nParams:\ntype: (string) The type of entity to go to.\nsearch_range: (number) The range to search for the entity.\n!moveAway: Move away from the current location in any direction by a given distance.\nParams:\ndistance: (number) The distance to move away.\n!rememberHere: Save the current location with a given name.\nParams:\nname: (string) The name to remember the location as.\n!goToRememberedPlace: Go to a saved location.\nParams:\nname: (string) The name of the location to go to.\n!givePlayer: Give the specified item to the given player.\nParams:\nplayer_name: (string) The name of the player to give the item to.\nitem_name: (string) The name of the item to give.\nnum: (number) The number of items to give.\n!consume: Eat/drink the given item.\nParams:\nitem_name: (string) The name of the item to consume.\n!equip: Equip the given item.\nParams:\nitem_name: (string) The name of the item to equip.\n!putInChest: Put the given item in the nearest chest.\nParams:\nitem_name: (string) The name of the item to put in the chest.\nnum: (number) The number of items to put in the chest.\n!takeFromChest: Take the given items from the nearest chest.\nParams:\nitem_name: (string) The name of the item to take.\nnum: (number) The number of items to take.\n!viewChest: View the items/counts of the nearest chest.\nParams:\n!discard: Discard the given item from the inventory.\nParams:\nitem_name: (string) The name of the item to discard.\nnum: (number) The number of items to discard.\n!collectBlocks: Collect the nearest blocks of a given type.\nParams:\ntype: (string) The block type to collect.\nnum: (number) The number of blocks to collect.\n!craftRecipe: Craft the given recipe a given number of times.\nParams:\nrecipe_name: (string) The name of the output item to craft.\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\n!smeltItem: Smelt the given item the given number of times.\nParams:\nitem_name: (string) The name of the input item to smelt.\nnum: (number) The number of times to smelt the item.\n!clearFurnace: Take all items out of the nearest furnace.\nParams:\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\nParams:\ntype: (string) The block type to place.\n!attack: Attack and kill the nearest entity of a given type.\nParams:\ntype: (string) The type of entity to attack.\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\nParams:\nplayer_name: (string) The name of the player to attack.\n!goToBed: Go to the nearest bed and sleep.\n!activate: Activate the nearest object of a given type.\nParams:\ntype: (string) The type of object to activate.\n!stay: Stay in the current location no matter what. Pauses all modes.\nParams:\ntype: (number) The number of seconds to stay. -1 for forever.\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\nParams:\nmode_name: (string) The name of the mode to enable.\non: (bool) Whether to enable or disable the mode.\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\nParams:\nselfPrompt: (string) The goal prompt.\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action.    \n!startConversation: Start a conversation with a player. Use for bots only.\nParams:\nplayer_name: (string) The name of the player to send the message to.\nmessage: (string) The message to send.\n!endConversation: End the conversation with the given player.\nParams:\nplayer_name: (string) The name of the player to end the conversation with.\n!digDown: Digs down a specified distance.\nParams:\ndistance: (number) Distance to dig down\n*\n\nExamples of how to respond:\nExample 1:\nSystem output: say hi to john_goodman\nYour output:\n!startConversation(\"john_goodman\", \"Hey John\"))\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\nYour output:\nHey John, not much. Just saying hi.\nUser input: john_goodman: (FROM OTHER BOT)Bye!\nYour output:\nBye! !endConversation('john_goodman')\n\nExample 2:\nUser input: miner_32: Hey! What are you up to?\nYour output:\nNothing much miner_32, what do you need?\n\n\nConversation Begin:&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Respond with hello world and your name&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}      \nINFO:     ::1:56880 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;406 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False} \nTRACE:    ::1:56880 - ASGI [4] Completed\nTRACE:    ::1:56880 - HTTP connection lost\n</code></pre> <ul> <li>Subsequent request:</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Thunderstorm\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nUser input: zZZn98: come here\\nYour output:\\nOn my way! !goToPlayer(\\\"zZZn98\\\", 3)\\nSystem output: Arrived at player.\\nYour output:\\nHere!\\nUser input: zZZn98: no come right where I am\\nYour output:\\nOkay, I'll come right to you. !goToPlayer(\\\"zZZn98\\\", 0)\\n\\nExample 2:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"},{\"role\":\"assistant\",\"content\":\"Hello world! My name is LLama.\"},{\"role\":\"user\",\"content\":\"Transpier: come here\"},{\"role\":\"assistant\",\"content\":\"On my way! !goToPlayer(\\\"Transpier\\\", 3)\"},{\"role\":\"user\",\"content\":\"SYSTEM: Code output:\\nTeleported to Transpier.\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Subsequent Response:</li> </ul> <pre><code>&lt;|im_start|&gt;user\nTranspier: come here&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nOn my way! !goToPlayer(\"Transpier\", 3)&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Code output:\nTeleported to Transpier.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}      \nINFO:     ::1:56890 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;381 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False} \nTRACE:    ::1:56890 - ASGI [6] Completed\nTRACE:    ::1:56890 - HTTP connection lost\n</code></pre>"},{"location":"server/apps/open-webui/","title":"Open WebUI","text":"<p>Open WebUI provides a highly polished chat interface in your browser for LLM interaction.</p> <p>This guide walks through how to connect Lemonade Server to Open WebUI and highlights some great features you can start using right away:</p> <ul> <li>Image Uploads to Vision-Language Models (VLMs): Upload images for analysis and interaction with your LLM-powered VLMs.</li> <li>Built-in Python Code Interpreter: Run and test Python code generated by your LLM directly within the interface.</li> <li>Live Preview for Web Development: Preview HTML, CSS, and JavaScript code generated by your LLM using the built-in preview server.</li> </ul>"},{"location":"server/apps/open-webui/#consider-using-gaia","title":"Consider Using GAIA","text":"<p>The team maintains a fork of Open WebUI called GAIA that provides automatic Lemonade integration and a simple Windows installer. You can try it out at https://github.com/amd/gaia. If you choose GAIA, you can skip the installation and configuration sections below and go straight to Using Open WebUI with Lemonade.</p>"},{"location":"server/apps/open-webui/#installing-open-webui","title":"Installing Open WebUI","text":"<ol> <li> <p>We recommend installing Open WebUI into a dedicated conda environment using the following commands. To download conda, see Miniforge.</p> <pre><code>conda create -n webui python=3.11\nconda activate webui\npip install open-webui\n</code></pre> <p>Note: Open WebUI also provides a variety of other installation options, such as Docker, on their GitHub.</p> </li> <li> <p>Run this command to launch the Open WebUI HTTP server:</p> <pre><code>open-webui serve\n</code></pre> </li> <li> <p>In a browser, navigate to http://localhost:8080/</p> </li> <li> <p>Open WebUI will ask you to create a local administrator account. You can fill any username, password, and email you like. Once you are signed in, you will see the chat interface:</p> <p></p> </li> </ol>"},{"location":"server/apps/open-webui/#configuring-open-webui","title":"Configuring Open WebUI","text":"<ol> <li> <p>Install and run Lemonade Server. Download here.</p> </li> <li> <p>Add Lemonade Server as a \"connection\" in Open WebUI using the following steps:</p> <ol> <li>Click the circular user profile button in the top-right of the UI, then click Settings:      Opening the settings menu. </li> <li>Click \"Connections\", then click the \"+\" button:      Navigating to the connection settings. </li> <li>Fill in the URL field with <code>http://localhost:8000/api/v1</code> (unless you're using a different port), API key (this is unused but required, suggest just putting a <code>-</code>), and then click \"Save\".      Filling in the connection details for Lemonade Server. </li> <li>Click \"Save\" in the settings menu, then exit the settings menu.</li> </ol> </li> <li> <p>Apply the suggested settings. These help Open WebUI to be more responsive with local LLMs.</p> <ol> <li>Click the user profile button again, and choose \"Admin Settings\".</li> <li>Click the \"Settings\" tab at the top, then \"Interface\" (which will be on the top or the left, depending on your window size), then disable the following:<ul> <li>Title Generation</li> <li>Follow Up Generation</li> <li>Tags Generation  Admin Settings </li> </ul> </li> <li>Click the \"Save\" button in the bottom right of the page, then return to http://localhost:8080.</li> </ol> </li> </ol>"},{"location":"server/apps/open-webui/#using-open-webui-with-lemonade","title":"Using Open WebUI with Lemonade","text":"<p>Now that everything is configured, you are ready to interact with an LLM!</p>"},{"location":"server/apps/open-webui/#chat","title":"Chat","text":"<ol> <li> <p>Click the dropdown menu in the top-left of the interface. This will display all of the Lemonade models you have installed. Select one to proceed.      Model Selection </p> </li> <li> <p>Enter a message to the LLM and click send (or hit enter). The LLM will take a few seconds to load into memory and then you will see the response stream in.</p> <p> Sending a message </p> <p> LLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#vision-language-models","title":"Vision Language Models","text":"<p>Vision Language Models (VLMs) can take images as part of their input.</p> <ol> <li> <p>Install a VLM in Lemonade by opening the Lemonade Model Manager:</p> <ol> <li>Open http://localhost:8000 in your browser.</li> <li>Select the Model Management tab.</li> <li> <p>Scroll down until you see a model with the blue <code>VISION</code> label and click the \"+\" button to install it.</p> <p> Installing a VLM </p> </li> </ol> </li> <li> <p>Return to Open WebUI in your browser and select your VLM in the models dropdown menu.</p> </li> <li> <p>Paste an image into the chat box and type a prompt or question about your image. You can also use the \"+\" button in the chat box to upload images.</p> <p> VLM prompt </p> <p> VLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#python-coding","title":"Python Coding","text":"<p>Open WebUI allows you to run Python code generated by an LLM directly within the interface.</p> <p>Note: only certain Python modules are enabled in Open WebUI. <code>matplotlib</code> is one of our favorites.</p> <ol> <li> <p>Ask the LLM to write some Python, then click the Run button at the top of the Python code block.</p> <p> Ask the LLM to write Python </p> </li> <li> <p>If all goes well, the result of running the Python code will appear below the code block.</p> <p> Python result </p> <p>Note: LLMs often produce incorrect code, so it might take a few chat iterations to fix any bugs. Copy-pasting the Python error message is usually enough to move things along.</p> </li> </ol>"},{"location":"server/apps/open-webui/#html-rendering","title":"HTML Rendering","text":"<p>Open WebUI has a built-in rendering engine for HTML, CSS, and JavaScript pages.</p> <p>Smaller LLMs can produce simple pages with tasteful styling and basic interactivity, while larger LLMs can accomplish tasks like 3D rendering in 3js.</p> <ol> <li> <p>Ask a small LLM to write a simple HTML+CSS page. The preview may pop up automatically, but if it doesn't you can click the Preview button above the HTML code block:</p> <p> HTML rendering </p> </li> <li> <p>Ask a large LLM to create a 3D shape using 3js.</p> <p> 3D rendering </p> </li> </ol>"},{"location":"server/apps/open-webui/#conclusion","title":"Conclusion","text":"<p>These are just a few of our favorite ways to try out LLMs in Open WebUI. There are a lot more features to explore, such as voice interaction and chatting with documents, so be sure to check out the Open WebUI documentation and YouTube content.</p>"},{"location":"server/apps/wut/","title":"<code>wut</code> Terminal Assistant","text":""},{"location":"server/apps/wut/#overview","title":"Overview","text":"<p>The <code>wut</code> terminal assistant uses LLMs to parse your terminal's scrollback, helping you troubleshoot your last command.</p>"},{"location":"server/apps/wut/#expectations","title":"Expectations","text":"<p>We found that <code>wut</code> works nicely with the <code>Llama-3.2-3B-Instruct-Hybrid</code> model.</p> <p>It is not especially convenient to use <code>wut</code> with Windows until the developers remove the requirement for <code>tmux</code>, however we do provide instructions for getting set up on Windows in this guide.</p> <p><code>wut</code> seems to send the entire terminal scrollback to the LLM, which can produce very long prompts that exceed the LLM's context length. We recommend restricting the terminal scrollback or using a fresh <code>tmux</code> session when trying this out.</p>"},{"location":"server/apps/wut/#setup","title":"Setup","text":""},{"location":"server/apps/wut/#prerequisites","title":"Prerequisites","text":""},{"location":"server/apps/wut/#install-lemonade-server","title":"Install Lemonade Server","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/wut/#installing-windows-subsystem-for-linux-wsl","title":"Installing Windows Subsystem for Linux (WSL)","text":"<p><code>wut</code> currently requires a <code>tmux</code> terminal in order to function. We found the simplest way to achieve this on Windows was through the Windows Subsystem for Linux (WSL).</p> <ol> <li>Install Windows Subsystem for Linux.</li> <li>Open the <code>WSL Settings</code> app, navigate to <code>Networking</code>, and make sure the <code>Networking mode</code> is <code>Mirrored</code>. This is required for WSL terminals to be able to see the Lemonade server running in Windows.</li> <li>If needed: shut down WSL to make sure the changes apply:</li> </ol> <pre><code>wsl --shutdown\n</code></pre>"},{"location":"server/apps/wut/#installing-wut","title":"Installing Wut","text":"<ul> <li>Start a WSL terminal.</li> <li>Install <code>pipx</code>, as recommended by the following <code>wut</code> instructions:</li> </ul> <pre><code>sudo apt update\nsudo apt install pipx\npipx ensurepath\n</code></pre> <ul> <li>Re-launch your terminal to make sure <code>pipx</code> is available, then install <code>wut</code>:</li> </ul> <pre><code>pipx install wut-cli\n</code></pre> <ul> <li>Add <code>wut</code>'s required environment variables to your <code>.bashrc</code> file:</li> </ul> <pre><code>export OPENAI_API_KEY=\"-\"\nexport OPENAI_MODEL=\"Llama-3.2-3B-Instruct-Hybrid\"\nexport OPENAI_BASE_URL=\"http://localhost:8000/api/v1\"\n</code></pre>"},{"location":"server/apps/wut/#usage","title":"Usage","text":""},{"location":"server/apps/wut/#start-a-terminal","title":"Start a terminal","text":"<ol> <li>Start a WSL terminal.</li> <li>Start a <code>tmux</code> session:</li> </ol> <pre><code>tmux\n</code></pre> <p>Then, try some of these example commands that <code>wut</code> can help explain.</p>"},{"location":"server/apps/wut/#help-with-lemonade-server","title":"Help with Lemonade Server","text":"<p>People often ask exactly what Lemonade Server's <code>models</code> endpoint does. Fortunately, <code>wut</code> is able to intuit the answer!</p> <pre><code>curl http://localhost:8000/api/v1/models\nwut\n</code></pre> <p>The terminal response of the <code>curl</code> command is this (only intelligible by machines):</p> <pre><code>curl http://localhost:8000/api/v1/models\n{\"object\":\"list\",\"data\":[{\"id\":\"Qwen2.5-0.5B-Instruct-CPU\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-1B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-3B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Phi-3-Mini-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Qwen-1.5-7B-Chat-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Llama-8B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Qwen-7B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"}]}\n</code></pre> <p>But <code>wut</code> does a nice job interpreting:</p> <pre><code>The output suggests that the API endpoint is returning a list of models, and the owned_by field indicates that all models are owned by \"lemonade\". Thecreated timestamp indicates when each model was created.\n\nThe output is a valid JSON response, and there is no error or warning message. The command was successful, and the output can be used for further processing or analysis. \n</code></pre>"},{"location":"server/apps/wut/#bad-git-command","title":"Bad Git Command","text":"<p>Run a command that doesn't exist, and then ask <code>wut</code> for help:</p> <pre><code>git pull-request\nwut\n</code></pre> <p>Results in:</p> <p>git: 'pull-request' is not a git command. See 'git --help'.</p> <p>And then <code>wut</code> provides some helpful feedback:</p> <p>Key takeaway: The command git pull-request is not a valid Git command. The correct command to create a pull request is git request-pull, but it's not a standard Git command. The output wut is the name of the activated Conda environment. To create a pull request, use git request-pull or git pull with the --pr option. </p>"}]}