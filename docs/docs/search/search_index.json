{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p>"},{"location":"#intro-video","title":"Intro Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"contribute/","title":"Lemonade SDK Contribution Guide","text":"<p>The Lemonade SDK project welcomes contributions from everyone!</p> <p>See code organization for an overview of the repository.</p>"},{"location":"contribute/#collaborate-with-your-app","title":"Collaborate with Your App","text":"<p>Lemonade Server integrates quickly with most OpenAI-compatible LLM apps.</p> <p>You can: - Share an example of your app using Lemonade via Discord, GitHub Issue, or email. - Contribute a guide by adding a <code>.md</code> file to the server apps folder. Follow the style of the Open WebUI guide.</p> <p>Guides should: - Work in under 10 minutes. - Require no code changes to the app. - Use OpenAI API-compatible apps with configurable base URLs.</p>"},{"location":"contribute/#backend-contributions","title":"Backend Contributions","text":"<p>To contribute code or examples, first open an Issue with:    - A descriptive title.    - Relevant labels (<code>enhancement</code>, <code>good first issue</code>, etc.).    - A proposal explaining what you're contributing.    - The use case it supports.</p> <p>One of the maintainers will get back to you ASAP with guidance.</p>"},{"location":"contribute/#uifrontend-contributions","title":"UI/Frontend Contributions","text":"<p>Current UI Development Approach:</p> <p>For now, UI and frontend development is being handled exclusively by core maintainers. Here's why: AI-assisted coding has made building UIs incredibly fast, but it's also made reviewing UI PRs quite challenging. UI changes often involve complex state management, visual consistency, accessibility considerations, and cross-platform considerations that require deep context about the entire application architecture.</p> <p>How You Can Still Influence the UI:</p> <p>We want your creativity and insights! Share UI/UX ideas, report bugs, or request features via Issue or Discord. Include mockups, screenshots, and reproduction steps where relevant.</p> <p>UI Scope: Management, Not Competition: Our UI exists to facilitate Lemonade management - not to compete with the apps built on top of Lemonade. While it's tempting to add agentic workflows, advanced chat features, or other sophisticated capabilities, that's not our goal. We focus on making model management, configuration, and monitoring delightful and effortless. Defining this line isn't always easy, but use this principle as your guide when considering new UI features.</p>"},{"location":"contribute/#issues","title":"Issues","text":"<p>Use Issues to report bugs or suggest features.</p> <p>A maintainer will apply one of these labels to indicate the status: - <code>on roadmap</code>: planned for development. - <code>good first issue</code>: open for contributors. - <code>needs detail</code>: needs clarification before proceeding. - <code>wontfix</code>: out of scope or unmaintainable.</p>"},{"location":"contribute/#pull-requests","title":"Pull Requests","text":"<p>Submit a PR to contribute code. Maintainers: - @danielholanda - @jeremyfowers - @ramkrishna2910 - @vgodsoe</p> <p>Discuss major changes via an Issue first.</p>"},{"location":"contribute/#code-formatting","title":"Code Formatting","text":"<p>We require that all Python files in this repo adhere to black formatting. This is enforced with a black check in CI workflows.</p>"},{"location":"contribute/#running-black-formatting","title":"Running Black formatting","text":"<p>The easiest way to ensure proper formatting is to enable the black formatter in VSCode with format-on-save:</p> <ol> <li> <p>Install the Python extension: Install the Python extension for VSCode if you haven't already.</p> </li> <li> <p>Set black as the default formatter:</p> </li> <li>Open VSCode settings (Ctrl/Cmd + ,)</li> <li>Search for \"Formatter\"</li> <li> <p>Set the Python default formatter to \"black\"</p> </li> <li> <p>Enable format-on-save:</p> </li> <li>In VSCode settings, search for \"format on save\"</li> <li>Check the \"Format On Save\" option</li> </ol> <p>This will automatically format your code according to black standards whenever you save a file.</p>"},{"location":"contribute/#alternative-setup","title":"Alternative Setup","text":"<p>You can also install black directly and run it manually: <pre><code># Install black (if not already installed)\npip install black\n\n# Run black formatter on your file\nblack your_file.py\n</code></pre></p>"},{"location":"contribute/#linting","title":"Linting","text":"<p>We use linting tools to maintain code quality and catch potential issues. The project uses standard Python linting tools that are automatically run in CI.</p>"},{"location":"contribute/#running-linters-locally","title":"Running Linters Locally","text":"<p>To run linting checks locally before submitting a PR:</p> <p><pre><code># Install linting dependencies (if not already installed)\npip install pylint\n\n# Run pylint from the root of the repo\npylint src/lemonade --rcfile .pylintrc --disable E0401\n</code></pre> This will show linting warnings and errors in your terminal.</p>"},{"location":"contribute/#testing","title":"Testing","text":"<p>Tests are run automatically on each PR. These include: - Linting - Code formatting (<code>black</code>) - Unit tests - End-to-end tests</p> <p>To run tests locally, use the commands in <code>.github/workflows/</code>.</p>"},{"location":"contribute/#versioning","title":"Versioning","text":"<p>We follow Semantic Versioning.</p>"},{"location":"dev-getting-started/","title":"Lemonade Development","text":"<p>This guide covers everything you need to build, test, and contribute to Lemonade from source. Whether you're fixing a bug, adding a feature, or just exploring the codebase, this document will help you get started.</p>"},{"location":"dev-getting-started/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Components</li> <li>Building from Source</li> <li>Prerequisites</li> <li>Build Steps</li> <li>Build Outputs</li> <li>Building the Electron Desktop App (Optional)</li> <li>Platform-Specific Notes</li> <li>Building Installers</li> <li>Windows Installer (WiX/MSI)</li> <li>Linux .deb Package (Debian/Ubuntu)</li> <li>Linux .rpm Package (Fedora, RHEL etc)</li> <li>Developer IDE &amp; IDE Build Steps</li> <li>Code Structure</li> <li>Architecture Overview</li> <li>Overview</li> <li>Client-Server Communication</li> <li>Dependencies</li> <li>Usage</li> <li>lemonade-router (Server Only)</li> <li>lemonade-server.exe (Console CLI Client)</li> <li>lemonade-tray.exe (GUI Tray Launcher)</li> <li>Logging and Console Output</li> <li>Testing</li> <li>Basic Functionality Tests</li> <li>Integration Tests</li> <li>Development</li> <li>Code Style</li> <li>Key Resources</li> <li>License</li> </ul>"},{"location":"dev-getting-started/#components","title":"Components","text":"<p>Lemonade consists of these main executables: - lemonade-router.exe - Core HTTP server executable that handles requests and LLM backend orchestration - lemonade-server.exe - Console CLI client for terminal users that manages server lifecycle, executes commands via HTTP API - lemonade-tray.exe (Windows only) - GUI tray launcher for desktop users, automatically starts <code>lemonade-server.exe serve</code> - lemonade-log-viewer.exe (Windows only) - Log file viewer with live tail support and installer-friendly file sharing</p>"},{"location":"dev-getting-started/#building-from-source","title":"Building from Source","text":""},{"location":"dev-getting-started/#prerequisites","title":"Prerequisites","text":"<p>All Platforms: - CMake 3.28 or higher - C++17 compatible compiler - Git (for fetching dependencies) - Internet connection (first build downloads dependencies)</p> <p>Windows: - Visual Studio 2019 or later - WiX 5.x (only required for building the installer)</p> <p>Linux:  - Ninja build system (optional, recommended)</p>"},{"location":"dev-getting-started/#build-steps","title":"Build Steps","text":"<p>A helper script is available that will set up the build environment on popular Linux distributions.  This will prompt to install dependencies via native package managers and create the build directory.</p> <p>Linux <pre><code>./setup.sh\n</code></pre></p> <p>Windows <pre><code>./setup.ps1\n</code></pre></p> <p>Build by running:</p> <p>Linux <pre><code>cmake --build --preset default\n</code></pre></p> <p>Windows <pre><code>cmake --build --preset windows\n</code></pre></p>"},{"location":"dev-getting-started/#build-outputs","title":"Build Outputs","text":"<ul> <li>Windows:</li> <li><code>build/Release/lemonade-router.exe</code> - HTTP server</li> <li><code>build/Release/lemonade-server.exe</code> - Console CLI client</li> <li><code>build/Release/lemonade-tray.exe</code> - GUI tray launcher</li> <li><code>build/Release/lemonade-log-viewer.exe</code> - Log file viewer</li> <li>Linux/macOS:</li> <li><code>build/lemonade-router</code> - HTTP server</li> <li><code>build/lemonade-server</code> - Console CLI client</li> <li>Resources: Automatically copied to <code>build/Release/resources/</code> on Windows, <code>build/resources/</code> on Linux/macOS (web UI files, model registry, backend version configuration)</li> </ul>"},{"location":"dev-getting-started/#building-the-electron-desktop-app-optional","title":"Building the Electron Desktop App (Optional)","text":"<p>The tray menu's \"Open app\" option and the <code>lemonade-server run</code> command can launch the Electron desktop app. To include it in your build:</p> <p>Build the Electron app using CMake (requires Node.js 20+):</p> <p>Linux <pre><code>cmake --build --preset default --target electron-app\n</code></pre></p> <p>Windows <pre><code>cmake --build --preset windows --target electron-app\n</code></pre></p> <p>This will: 1. Copy src/app to build/app-src (keeps source tree clean) 2. Run npm install in build/app-src 3. Build to build/app/linux-unpacked/ (Linux) or build/app/win-unpacked/ (Windows)</p> <p>The tray app searches for the Electron app in these locations: - Windows installed: <code>../app/Lemonade.exe</code> (relative to bin/ directory) - Windows development: <code>../app/win-unpacked/Lemonade.exe</code> (from build/Release/) - Linux installed: <code>/usr/local/share/lemonade-server/app/lemonade</code> - Linux development: <code>../app/linux-unpacked/lemonade</code> (from build/)</p> <p>If not found, the \"Open app\" menu option is hidden but everything else works.</p>"},{"location":"dev-getting-started/#platform-specific-notes","title":"Platform-Specific Notes","text":"<p>Windows: - The build uses static linking to minimize DLL dependencies - All dependencies are built from source (no external DLL requirements) - Security features enabled: Control Flow Guard, ASLR, DEP</p> <p>Linux: - Linux builds are headless-only (no tray application) by default - This avoids LGPL dependencies (GTK3, libappindicator3, libnotify) - Run server using: <code>lemonade-server serve</code> (headless mode is automatic) - Fully functional for server operations and model management - Uses permissively licensed dependencies only (MIT, Apache 2.0, BSD, curl license) - Clean .deb package with only runtime files (no development headers) - PID file system (<code>/tmp/lemonade-router.pid</code>) for reliable process management - Proper graceful shutdown - all child processes cleaned up correctly - File locations:   - Installed binaries: <code>/opt/bin</code>   - Downloaded backends (llama-server, ryzenai-server): <code>~/.cache/lemonade/bin/</code>   - Model downloads: <code>~/.cache/huggingface/</code> (follows HF conventions)</p> <p>macOS: - Uses native system frameworks (Cocoa, Foundation) - ARM Macs use Metal backend by default for llama.cpp - \u26a0\ufe0f Note: macOS build is currently a stub implementation and not fully functional</p>"},{"location":"dev-getting-started/#building-installers","title":"Building Installers","text":""},{"location":"dev-getting-started/#windows-installer-wixmsi","title":"Windows Installer (WiX/MSI)","text":"<p>Prerequisites: - WiX Toolset 5.0.2 installed from wix-cli-x64.msi - Completed C++ build (see above)</p> <p>Building:</p> <p>Using PowerShell script (recommended): <pre><code>cd src\\cpp\n.\\build_installer.ps1\n</code></pre></p> <p>Manual build using CMake: <pre><code>cd src\\cpp\\build\ncmake --build . --config Release --target wix_installer\n</code></pre></p> <p>Installer Output:</p> <p>Creates <code>lemonade-server-minimal.msi</code> which: - MSI-based installer (Windows Installer technology) - Per-user install (default): Installs to <code>%LOCALAPPDATA%\\lemonade_server\\</code>, adds to user PATH, no UAC required - All-users install (CLI only): Installs to <code>%PROGRAMFILES%\\Lemonade Server\\</code>, adds to system PATH, requires elevation - Creates Start Menu shortcuts (launches <code>lemonade-tray.exe</code>) - Optionally creates desktop shortcut and startup entry - Uses Windows Installer Restart Manager to gracefully close running processes - Includes all executables (router, server, tray, log-viewer) - Proper upgrade handling between versions - Includes uninstaller</p> <p>Available Installers: - <code>lemonade-server-minimal.msi</code> - Server only (~3 MB) - <code>lemonade.msi</code> - Full installer with Electron desktop app (~105 MB)</p> <p>Installation:</p> <p>For detailed installation instructions including silent install, custom directories, and all-users installation, see the Server Integration Guide.</p>"},{"location":"dev-getting-started/#linux-deb-package-debianubuntu","title":"Linux .deb Package (Debian/Ubuntu)","text":"<p>Prerequisites: - Completed C++ build (see above)</p> <p>Building:</p> <pre><code>cd build\ncpack\n</code></pre> <p>Package Output:</p> <p>Creates <code>lemonade-server-minimal_&lt;VERSION&gt;_amd64.deb</code> (e.g., <code>lemonade-server-minimal_9.0.3_amd64.deb</code>) which: - Installs to <code>/opt/bin/</code> (executables) - Installs resources to <code>/opt/share/lemonade-server/</code> - Creates desktop entry in <code>/opt/share/applications/</code> - Declares dependencies: libcurl4, libssl3, libz1 - Package size: ~2.2 MB (clean, runtime-only package) - Includes postinst script that creates writable <code>/opt/share/lemonade-server/llama/</code> directory</p> <p>Installation:</p> <pre><code># Replace &lt;VERSION&gt; with the actual version (e.g., 9.0.0)\nsudo apt install ./lemonade-server-minimal_&lt;VERSION&gt;_amd64.deb\n</code></pre> <p>Uninstallation:</p> <pre><code>sudo dpkg -r lemonade-server\n</code></pre> <p>Post-Installation:</p> <p>The executables will be available in PATH: <pre><code>lemonade-server --help\nlemonade-router --help\n\n# Start server in headless mode:\nlemonade-server serve --no-tray\n\n# Or just:\nlemonade-server serve\n</code></pre></p>"},{"location":"dev-getting-started/#linux-rpm-package-fedora-rhel-etc","title":"Linux .rpm Package (Fedora, RHEL etc)","text":"<p>Very similar to the Debian instructions above with minor changes</p> <p>Building:</p> <pre><code>cd build\ncpack -G RPM\n</code></pre> <p>Package Output:</p> <p>Creates <code>lemonade-server-minimal-&lt;VERSION&gt;.x86_64.rpm</code> (e.g., <code>lemonade-server-minimal-9.1.2.x86_64.rpm</code>) and resources are installed as per DEB version above</p> <p>Installation:</p> <pre><code># Replace &lt;VERSION&gt; with the actual version (e.g., 9.0.0)\nsudo dnf install ./lemonade-server-minimal-&lt;VERSION&gt;.x86_64.rpm\n</code></pre> <p>Uninstallation:</p> <pre><code>sudo dnf remove lemonade-server-minimal\n</code></pre> <p>Post-Installation:</p> <p>Same as .deb above</p> <p>macOS:</p>"},{"location":"dev-getting-started/#building-from-source-on-macos-for-m-series-arm64-family","title":"Building from Source on MacOS for M-Series / arm64 Family","text":""},{"location":"dev-getting-started/#macos-notary-tool-command","title":"Macos Notary Tool Command","text":"<p>For access with P <pre><code>xcrun notarytool store-credentials AC_PASSWORD --apple-id \"your-apple-id@example.com\" --team-id \"your-team-id\" --private-key \"/path/to/AuthKey_XXXXXX.p8\"\n</code></pre> or For access with API password <pre><code>xcrun notarytool store-credentials AC_PASSWORD --apple-id \"your-apple-id@example.com\" --team-id \"your-team-id\" --password \"\"\n</code></pre> Get your team id at: https://developer.apple.com/account</p>"},{"location":"dev-getting-started/#cmake-build-instructions","title":"Cmake build instructions","text":"<pre><code># Install Xcode command line tools\nxcode-select --install\n\n# Navigate to the C++ source directory\ncd src/cpp\n\n# Create and enter build directory\nmkdir build\ncd build\n\n# Configure with CMake\ncmake ..\n\n# Build with all cores\ncmake --build . --config Release -j\n</code></pre>"},{"location":"dev-getting-started/#cmake-targets","title":"CMake Targets","text":"<p>The build system provides several CMake targets for different build configurations:</p> <ul> <li><code>lemonade-router</code>: The main HTTP server executable that handles LLM inference requests</li> <li><code>package-macos</code>: Creates a signed macOS installer package (.pkg) using productbuild</li> <li><code>notarize_package</code>: Builds and submits the package to Apple for notarization and staples the ticket</li> <li><code>electron-app</code>: Builds the Electron-based GUI application</li> <li><code>prepare_electron_app</code>: Prepares the Electron app for inclusion in the installer</li> </ul>"},{"location":"dev-getting-started/#building-and-notarizing-for-distribution","title":"Building and Notarizing for Distribution","text":"<p>To build a notarized macOS installer for distribution:</p> <ol> <li>Prerequisites:</li> <li>Apple Developer Program membership</li> <li>Valid Developer ID Application and Installer certificates</li> <li>App-specific password for notarization</li> <li> <p>Xcode command line tools</p> </li> <li> <p>Set Environment Variables:    <pre><code>export DEVELOPER_ID_APPLICATION_IDENTITY=\"Developer ID Application: Your Name (TEAMID)\"\nexport DEVELOPER_ID_INSTALLER_IDENTITY=\"Developer ID Installer: Your Name (TEAMID)\"\nexport AC_PASSWORD=\"your-app-specific-password\"\n</code></pre></p> </li> <li> <p>Configure Notarization Keychain Profile:    <pre><code>xcrun notarytool store-credentials \"AC_PASSWORD\" \\\n  --apple-id \"your-apple-id@example.com\" \\\n  --team-id \"YOURTEAMID\" \\\n  --password \"your-app-specific-password\"\n</code></pre></p> </li> <li> <p>Build and Notarize:    <pre><code>cd src/cpp/build\ncmake --build . --config Release --target package-macos\ncmake --build . --config Release --target notarize_package\n</code></pre></p> </li> </ol> <p>The notarization process will: - Submit the package to Apple's notarization service - Wait for approval - Staple the notarization ticket to the package</p> <p>Note: The package is signed with hardened runtime entitlements during the build process for security.</p>"},{"location":"dev-getting-started/#developer-ide-ide-build-steps","title":"Developer IDE &amp; IDE Build Steps","text":""},{"location":"dev-getting-started/#visual-studio-code-setup-guide","title":"Visual Studio Code Setup Guide","text":"<ol> <li>Clone the repository into a blank folder locally on your computer.</li> <li>Open the folder in visual studio code.</li> <li>Install Dev Containers extension in Visual Studio Code by using   control + p to open the command bar at the top of the IDE or if on mac with Cmd + p.</li> <li>Type \"&gt; Extensions: Install Extensions\" which will open the Extensions side panel.</li> <li>in the extensions search type <code>Dev Containers</code> and install it.</li> <li>Once completed with the prior steps you may run command <code>&gt;Dev Containers: Open Workspace in Container</code> or <code>&gt;Dev Containers: Open Folder in Container</code> which you can do in the command bar in the IDE and it should reopen the visual studio code project.</li> <li>It will launch a docker and start building a new docker and then the project will open in visual studio code.</li> </ol>"},{"location":"dev-getting-started/#build-compile-options","title":"Build &amp; Compile Options","text":"<ol> <li>Assuming your VSCode IDE is open and the dev container is working.</li> <li>Go to the CMake plugin you may select the \"Folder\" that is where you currently want to build.</li> <li>Once done with that you may select which building toolkit you are using under Configure and then begin configure.</li> <li>Under Build, Test, Debug and/or Launch you may select whatever configuration you want to build, test, debug and/or launch.</li> </ol>"},{"location":"dev-getting-started/#debug-runtime-console-arguments","title":"Debug / Runtime / Console arguments","text":"<ol> <li>You may find arguments which are passed through to the application you are debugging in .vscode/settings.json which will look like the following: <pre><code>\"cmake.debugConfig\": {\n        \"args\": [\n            \"--llamacpp\", \"cpu\"\n        ]\n    }\n</code></pre></li> <li>If you want to debug lemonade-router you may pass --llamacpp cpu for cpu based tests.</li> <li>For lemonade-server you may pass serve as a argument as well.</li> </ol>"},{"location":"dev-getting-started/#the-hard-way-commands-only","title":"The hard way - commands only.","text":"<ol> <li> <p>Now if you want to do it the hard way below are the commands in which you can run in the command dropdown in which you can see if you use the following keyboard shortcuts. cmd + p / control + p <pre><code>&gt; Cmake: Select a Kit\n# Select a kit or Scan for kit. (Two options should be available gcc or clang)\n&gt; Cmake: Configure\n# Optional commands are:\n&gt; Cmake: Build Target\n# use this to select a cmake target to build\n&gt; Cmake: Set Launch/Debug target\n# use this to select/set your cmake target you want to build/debug\n\n# This next command lets you debug\n&gt; Cmake: Debug\n\n# This command lets you delete the cmake cache and reconfigure which is rarely needed.\n&gt; Cmake: Delete Cache and Reconfigure\n</code></pre></p> </li> <li> <p>Custom configurations for cmake are in the root directory under <code>.vscode/settings.json</code> in which you may set custom args for launching the debug in the json key <code>cmake.debugConfig</code></p> </li> </ol> <p>Note</p> <p>For running Lemonade as a containerized application (as an alternative to the MSI-based distribution), see <code>DOCKER_GUIDE.md</code>.</p>"},{"location":"dev-getting-started/#code-structure","title":"Code Structure","text":"<pre><code>src/cpp/\n\u251c\u2500\u2500 build_installer.ps1         # Installer build script\n\u251c\u2500\u2500 CopyElectronApp.cmake       # CMake module to copy Electron app to build output\n\u251c\u2500\u2500 CPackRPM.cmake              # RPM packaging configuration\n\u251c\u2500\u2500 DOCKER_GUIDE.md             # Docker containerization guide\n\u251c\u2500\u2500 Extra-Models-Dir-Spec.md    # Extra models directory specification\n\u251c\u2500\u2500 Multi-Model-Spec.md         # Multi-model loading specification\n\u251c\u2500\u2500 postinst                    # Debian package post-install script\n\u251c\u2500\u2500 postinst-full               # Debian package post-install script (full version)\n\u251c\u2500\u2500 resources/                  # Configuration and data files (self-contained)\n\u2502   \u251c\u2500\u2500 backend_versions.json   # llama.cpp/whisper version configuration\n\u2502   \u251c\u2500\u2500 server_models.json      # Model registry (available models)\n\u2502   \u2514\u2500\u2500 static/                 # Web UI assets\n\u2502       \u251c\u2500\u2500 index.html          # Server landing page (with template variables)\n\u2502       \u2514\u2500\u2500 favicon.ico         # Site icon\n\u2502\n\u251c\u2500\u2500 installer/                  # WiX MSI installer (Windows)\n\u2502   \u251c\u2500\u2500 Product.wxs.in          # WiX installer definition template\n\u2502   \u251c\u2500\u2500 installer_banner_wix.bmp  # Left-side banner (493\u00d7312)\n\u2502   \u2514\u2500\u2500 top_banner.bmp          # Top banner with lemon icon (493\u00d758)\n\u2502\n\u251c\u2500\u2500 server/                     # Server implementation\n\u2502   \u251c\u2500\u2500 main.cpp                # Entry point, CLI routing\n\u2502   \u251c\u2500\u2500 server.cpp              # HTTP server (cpp-httplib)\n\u2502   \u251c\u2500\u2500 router.cpp              # Routes requests to backends\n\u2502   \u251c\u2500\u2500 model_manager.cpp       # Model registry, downloads, caching\n\u2502   \u251c\u2500\u2500 cli_parser.cpp          # Command-line argument parsing (CLI11)\n\u2502   \u251c\u2500\u2500 recipe_options.cpp      # Recipe option handling\n\u2502   \u251c\u2500\u2500 wrapped_server.cpp      # Base class for backend wrappers\n\u2502   \u251c\u2500\u2500 streaming_proxy.cpp     # Server-Sent Events for streaming\n\u2502   \u251c\u2500\u2500 system_info.cpp         # NPU/GPU device detection\n\u2502   \u251c\u2500\u2500 lemonade.manifest.in    # Windows manifest template\n\u2502   \u251c\u2500\u2500 version.rc              # Windows version resource\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 backends/               # Model backend implementations\n\u2502   \u2502   \u251c\u2500\u2500 backend_utils.cpp     # Shared backend utilities\n\u2502   \u2502   \u251c\u2500\u2500 llamacpp_server.cpp   # Wraps llama.cpp for LLM inference (CPU/GPU)\n\u2502   \u2502   \u251c\u2500\u2500 fastflowlm_server.cpp # Wraps FastFlowLM for NPU inference\n\u2502   \u2502   \u251c\u2500\u2500 ryzenaiserver.cpp     # Wraps RyzenAI server for hybrid NPU\n\u2502   \u2502   \u251c\u2500\u2500 sd_server.cpp         # Wraps Stable Diffusion for image generation\n\u2502   \u2502   \u2514\u2500\u2500 whisper_server.cpp    # Wraps whisper.cpp for audio transcription (CPU/NPU)\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 utils/                  # Utility functions\n\u2502       \u251c\u2500\u2500 http_client.cpp     # HTTP client using libcurl\n\u2502       \u251c\u2500\u2500 json_utils.cpp      # JSON file I/O\n\u2502       \u251c\u2500\u2500 process_manager.cpp # Cross-platform process management\n\u2502       \u251c\u2500\u2500 path_utils.cpp      # Path manipulation\n\u2502       \u251c\u2500\u2500 wmi_helper.cpp      # Windows WMI for NPU detection\n\u2502       \u2514\u2500\u2500 wmi_helper.h        # WMI helper header\n\u2502\n\u251c\u2500\u2500 include/lemon/              # Public headers\n\u2502   \u251c\u2500\u2500 server.h                # HTTP server interface\n\u2502   \u251c\u2500\u2500 router.h                # Request routing\n\u2502   \u251c\u2500\u2500 model_manager.h         # Model management\n\u2502   \u251c\u2500\u2500 cli_parser.h            # CLI argument parsing\n\u2502   \u251c\u2500\u2500 recipe_options.h        # Recipe option definitions\n\u2502   \u251c\u2500\u2500 wrapped_server.h        # Backend wrapper base class\n\u2502   \u251c\u2500\u2500 streaming_proxy.h       # Streaming proxy\n\u2502   \u251c\u2500\u2500 system_info.h           # System information\n\u2502   \u251c\u2500\u2500 model_types.h           # Model type definitions\n\u2502   \u251c\u2500\u2500 audio_types.h           # Audio type definitions\n\u2502   \u251c\u2500\u2500 error_types.h           # Error type definitions\n\u2502   \u251c\u2500\u2500 server_capabilities.h   # Server capability definitions\n\u2502   \u251c\u2500\u2500 single_instance.h       # Single instance enforcement\n\u2502   \u251c\u2500\u2500 version.h.in            # Version header template\n\u2502   \u251c\u2500\u2500 backends/               # Backend headers\n\u2502   \u2502   \u251c\u2500\u2500 backend_utils.h       # Backend utilities\n\u2502   \u2502   \u251c\u2500\u2500 llamacpp_server.h     # LlamaCpp backend\n\u2502   \u2502   \u251c\u2500\u2500 fastflowlm_server.h   # FastFlowLM backend\n\u2502   \u2502   \u251c\u2500\u2500 ryzenaiserver.h       # RyzenAI backend\n\u2502   \u2502   \u251c\u2500\u2500 sd_server.h           # Stable Diffusion backend\n\u2502   \u2502   \u2514\u2500\u2500 whisper_server.h      # Whisper backend\n\u2502   \u2514\u2500\u2500 utils/                  # Utility headers\n\u2502       \u251c\u2500\u2500 http_client.h       # HTTP client\n\u2502       \u251c\u2500\u2500 json_utils.h        # JSON utilities\n\u2502       \u251c\u2500\u2500 process_manager.h   # Process management\n\u2502       |\u2500\u2500 path_utils.h        # Path utilities\n|       |\u2500\u2500 network_beacon.h    # Helps broadcast a beacon on port 8000 to network multicast\n\u2502\n\u2514\u2500\u2500 tray/                       # System tray application\n    \u251c\u2500\u2500 CMakeLists.txt          # Tray-specific build config\n    \u251c\u2500\u2500 main.cpp                # Tray entry point (lemonade-server)\n    \u251c\u2500\u2500 tray_launcher.cpp       # GUI launcher (lemonade-tray)\n    \u251c\u2500\u2500 log-viewer.cpp          # Log file viewer (lemonade-log-viewer)\n    \u251c\u2500\u2500 server_manager.cpp      # Manages lemonade-router process\n    \u251c\u2500\u2500 tray_app.cpp            # Main tray application logic\n    \u251c\u2500\u2500 lemonade-server.manifest.in  # Windows manifest template\n    \u251c\u2500\u2500 version.rc              # Windows version resource\n    \u2514\u2500\u2500 platform/               # Platform-specific implementations\n        \u251c\u2500\u2500 windows_tray.cpp    # Win32 system tray API\n        \u251c\u2500\u2500 macos_tray.mm       # Objective-C++ NSStatusBar\n        \u251c\u2500\u2500 linux_tray.cpp      # GTK/AppIndicator\n        \u2514\u2500\u2500 tray_factory.cpp    # Platform detection\n</code></pre>"},{"location":"dev-getting-started/#architecture-overview","title":"Architecture Overview","text":""},{"location":"dev-getting-started/#overview","title":"Overview","text":"<p>The Lemonade Server C++ implementation uses a client-server architecture:</p>"},{"location":"dev-getting-started/#lemonade-router-server-component","title":"lemonade-router (Server Component)","text":"<p>A pure HTTP server that: - Serves OpenAI-compatible REST API endpoints (supports both <code>/api/v0</code> and <code>/api/v1</code>) - Routes requests to appropriate LLM backends (llamacpp, fastflowlm, ryzenai) - Manages model loading/unloading and backend processes - Supports loading multiple models simultaneously with LRU eviction - Handles all inference requests - No command-based user interface - only accepts startup options</p> <p>Key Layers: - HTTP Layer: Uses cpp-httplib for HTTP server - Router: Determines which backend handles each request based on model recipe, manages multiple WrappedServer instances with LRU cache - Model Manager: Handles model discovery, downloads, and registry management - Backend Wrappers: Manages llama.cpp, FastFlowLM, RyzenAI, and whisper.cpp backends</p> <p>Multi-Model Support: - Router maintains multiple WrappedServer instances simultaneously - Separate LRU caches for LLM, embedding, reranking, and audio model types - NPU exclusivity: only one model can use NPU at a time - Configurable limits via <code>--max-loaded-models</code> (default: 1 1 1 1) - Automatic eviction of least-recently-used models when limits reached - Thread-safe model loading with serialization to prevent races - Protection against evicting models actively serving inference requests</p>"},{"location":"dev-getting-started/#lemonade-server-cli-client-component","title":"lemonade-server (CLI Client Component)","text":"<p>A console application for terminal users: - Provides command-based user interface (<code>list</code>, <code>pull</code>, <code>delete</code>, <code>run</code>, <code>status</code>, <code>stop</code>, <code>serve</code>) - Manages server lifecycle (start/stop persistent or ephemeral servers) - Communicates with <code>lemonade-router</code> via HTTP endpoints - Starts <code>lemonade-router</code> with appropriate options - Provides optional system tray interface via <code>serve</code> command</p> <p>Command Types: - serve: Starts a persistent server (with optional tray interface) - run: Starts persistent server, loads model, opens browser - Other commands: Use existing server or start ephemeral server, execute command via API, auto-cleanup</p>"},{"location":"dev-getting-started/#lemonade-tray-gui-launcher-windows-only","title":"lemonade-tray (GUI Launcher - Windows Only)","text":"<p>A minimal WIN32 GUI application for desktop users: - Simple launcher that starts <code>lemonade-server.exe serve</code> - Zero console output or CLI interface - Used by Start Menu, Desktop shortcuts, and autostart - Provides seamless GUI experience for non-technical users</p>"},{"location":"dev-getting-started/#client-server-communication","title":"Client-Server Communication","text":"<p>The <code>lemonade-server</code> client communicates with <code>lemonade-router</code> server via HTTP: - Model operations: <code>/api/v1/models</code>, <code>/api/v1/pull</code>, <code>/api/v1/delete</code> - Model control: <code>/api/v1/load</code>, <code>/api/v1/unload</code> - Server management: <code>/api/v1/health</code>, <code>/internal/shutdown</code> - Inference: <code>/api/v1/chat/completions</code>, <code>/api/v1/completions</code>, <code>/api/v1/audio/transcriptions</code></p> <p>The client automatically: - Detects if a server is already running - Starts ephemeral servers for one-off commands - Cleans up ephemeral servers after command completion - Manages persistent servers with proper lifecycle handling</p> <p>Single-Instance Protection: - Each component (<code>lemonade-router</code>, <code>lemonade-server serve</code>, <code>lemonade-tray</code>) enforces single-instance using system-wide mutexes - Only the <code>serve</code> command is blocked when a server is running - Commands like <code>status</code>, <code>list</code>, <code>pull</code>, <code>delete</code>, <code>stop</code> can run alongside an active server - Provides clear error messages with suggestions when blocked - Linux-specific: Uses PID file (<code>/tmp/lemonade-router.pid</code>) for efficient server discovery and port detection   - Avoids port scanning, finds exact server PID and port instantly   - Validated on read (checks if process is still alive)   - Automatically cleaned up on graceful shutdown</p> <p>Network Beacon based broadcasting: - Uses port 8000 to broadcast to the network that it exists - Clients can read the json broadcast message to add server to server picker. - Uses machine hostname as broadcast name. - The custom flag --no-broadcast is available in the command line to disable. - Auto protection, doesnt broadcast on non RFC1918 Networks.</p>"},{"location":"dev-getting-started/#dependencies","title":"Dependencies","text":"<p>All dependencies are automatically fetched by CMake via FetchContent:</p> <ul> <li>cpp-httplib (v0.26.0) - HTTP server with thread pool support [MIT License]</li> <li>nlohmann/json (v3.11.3) - JSON parsing and serialization [MIT License]</li> <li>CLI11 (v2.4.2) - Command-line argument parsing [BSD 3-Clause]</li> <li>libcurl (8.5.0) - HTTP client for model downloads [curl license]</li> <li>zstd (v1.5.7) - Compression library for HTTP [BSD License]</li> </ul> <p>Platform-specific SSL backends are used (Schannel on Windows, SecureTransport on macOS, OpenSSL on Linux).</p>"},{"location":"dev-getting-started/#usage","title":"Usage","text":""},{"location":"dev-getting-started/#lemonade-router-server-only","title":"lemonade-router (Server Only)","text":"<p>The <code>lemonade-router</code> executable is a pure HTTP server without any command-based interface:</p> <pre><code># Start server with default options\n./lemonade-router\n\n# Start server with custom options\n./lemonade-router --port 8080 --ctx-size 8192 --log-level debug\n\n# Available options:\n#   --port PORT              Port number (default: 8000)\n#   --host HOST              Bind address (default: localhost)\n#   --ctx-size SIZE          Context size (default: 4096)\n#   --log-level LEVEL        Log level: critical, error, warning, info, debug, trace\n#   --llamacpp BACKEND       LlamaCpp backend: vulkan, rocm, metal\n#   --max-loaded-models LLMS [EMBEDDINGS] [RERANKINGS] [AUDIO]\n#                            Maximum models to keep loaded (default: 1 1 1 1)\n#   --version, -v            Show version\n#   --help, -h               Show help\n</code></pre>"},{"location":"dev-getting-started/#lemonade-serverexe-console-cli-client","title":"lemonade-server.exe (Console CLI Client)","text":"<p>The <code>lemonade-server</code> executable is the command-line interface for terminal users: - Command-line interface for all model and server management - Starts persistent servers (with optional tray interface) - Manages ephemeral servers for one-off commands - Communicates with <code>lemonade-router</code> via HTTP endpoints</p> <pre><code># List available models\n./lemonade-server list\n\n# Pull a model\n./lemonade-server pull Llama-3.2-1B-Instruct-CPU\n\n# Delete a model\n./lemonade-server delete Llama-3.2-1B-Instruct-CPU\n\n# Check server status\n./lemonade-server status\n\n# Stop the server\n./lemonade-server stop\n\n# Run a model (starts persistent server with tray and opens browser)\n./lemonade-server run Llama-3.2-1B-Instruct-CPU\n\n# Start persistent server (with tray on Windows/macOS, headless on Linux)\n./lemonade-server serve\n\n# Start persistent server without tray (headless mode, explicit on all platforms)\n./lemonade-server serve --no-tray\n\n# Start server with custom options\n./lemonade-server serve --port 8080 --ctx-size 8192\n</code></pre> <p>Available Options: - <code>--port PORT</code> - Server port (default: 8000) - <code>--host HOST</code> - Server host (default: localhost) - <code>--ctx-size SIZE</code> - Context size (default: 4096) - <code>--log-level LEVEL</code> - Logging verbosity: info, debug (default: info) - <code>--log-file PATH</code> - Custom log file location - <code>--server-binary PATH</code> - Path to lemonade-router executable - <code>--no-tray</code> - Run without tray (headless mode) - <code>--max-loaded-models LLMS [EMBEDDINGS] [RERANKINGS] [AUDIO]</code> - Maximum number of models to keep loaded simultaneously (default: 1 1 1 1)</p> <p>Note: <code>lemonade-router</code> is always launched with <code>--log-level debug</code> for optimal troubleshooting. Use <code>--log-level debug</code> on <code>lemonade-server</code> commands to see client-side debug output.</p>"},{"location":"dev-getting-started/#lemonade-trayexe-gui-tray-launcher-windows-only","title":"lemonade-tray.exe (GUI Tray Launcher - Windows Only)","text":"<p>The <code>lemonade-tray</code> executable is a simple GUI launcher for desktop users: - Double-click from Start Menu or Desktop to start server - Automatically runs <code>lemonade-server.exe serve</code> in tray mode - Zero console windows or CLI interface - Perfect for non-technical users - Single-instance protection: shows friendly message if already running</p> <p>What it does: 1. Finds <code>lemonade-server.exe</code> in the same directory 2. Launches it with the <code>serve</code> command 3. Exits immediately (server continues running with tray icon)</p> <p>When to use: - Launching from Start Menu - Desktop shortcuts - Windows startup - Any GUI/point-and-click scenario</p> <p>System Tray Features (when running): - Left-click or right-click icon to show menu - Load/unload models via menu - Change server port and context size - Open web UI, documentation, and logs - \"Show Logs\" opens log viewer with historical and live logs - Background model monitoring - Click balloon notifications to open menu - Quit option</p> <p>UI Improvements: - Displays as \"Lemonade Local LLM Server\" in Task Manager - Shows large lemon icon in notification balloons - Single-instance protection prevents multiple tray apps</p>"},{"location":"dev-getting-started/#logging-and-console-output","title":"Logging and Console Output","text":"<p>When running <code>lemonade-server.exe serve</code>: - Console Output: Router logs are streamed to the terminal in real-time via a background tail thread - Log File: All logs are written to a persistent log file (default: <code>%TEMP%\\lemonade-server.log</code>) - Log Viewer: Click \"Show Logs\" in the tray to open <code>lemonade-log-viewer.exe</code>   - Displays last 100KB of historical logs   - Live tails new content as it's written   - Automatically closes when server stops   - Uses shared file access (won't block installer)</p> <p>Log Viewer Features: - Cross-platform tail implementation - Parent process monitoring for auto-cleanup - Installer-friendly (FILE_SHARE_DELETE on Windows) - Real-time updates with minimal latency (100ms polling)</p>"},{"location":"dev-getting-started/#testing","title":"Testing","text":""},{"location":"dev-getting-started/#basic-functionality-tests","title":"Basic Functionality Tests","text":"<p>Run the commands from the Usage section above to verify basic functionality.</p>"},{"location":"dev-getting-started/#integration-tests","title":"Integration Tests","text":"<p>The C++ implementation is tested using the existing Python test suite.</p> <p>Prerequisites: - Python 3.10+ - Test dependencies: <code>pip install -r test/requirements.txt</code></p> <p>Python integration tests (from <code>test/</code> directory, ordered least to most complex):</p> Test File Description <code>server_cli.py</code> CLI commands (version, list, pull, status, delete, serve, stop, run) <code>server_endpoints.py</code> HTTP endpoints (health, models, pull, load, unload, system-info, stats) <code>server_llm.py</code> LLM inference (chat completions, embeddings, reranking) <code>server_whisper.py</code> Audio transcription (whisper models) <code>server_sd.py</code> Image generation (Stable Diffusion, ~2-3 min per image on CPU) <p>Running tests: <pre><code># CLI tests (no inference backend needed)\npython test/server_cli.py\n\n# Endpoint tests (no inference backend needed)\npython test/server_endpoints.py\n\n# LLM tests (specify wrapped server and backend)\npython test/server_llm.py --wrapped-server llamacpp --backend vulkan\n\n# Audio transcription tests\npython test/server_whisper.py\n\n# Image generation tests (slow)\npython test/server_sd.py\n</code></pre></p> <p>The tests auto-discover the server binary from the build directory. Use <code>--server-binary</code> to override if needed.</p> <p>See the <code>.github/workflows/</code> directory for CI/CD test configurations.</p> <p>Note: The Python tests should now use <code>lemonade-server.exe</code> as the entry point since it provides the CLI interface.</p>"},{"location":"dev-getting-started/#development","title":"Development","text":""},{"location":"dev-getting-started/#code-style","title":"Code Style","text":"<ul> <li>C++17 standard</li> <li>Snake_case for functions and variables</li> <li>CamelCase for classes and types</li> <li>4-space indentation</li> <li>Header guards using <code>#pragma once</code></li> <li>All code in <code>lemon::</code> namespace</li> </ul>"},{"location":"dev-getting-started/#key-resources","title":"Key Resources","text":"<ul> <li>API Specification: <code>docs/server/server_spec.md</code></li> <li>Model Registry: <code>src/cpp/resources/server_models.json</code></li> <li>Web UI Files: <code>src/cpp/resources/static/</code></li> <li>Backend Versions: <code>src/cpp/resources/backend_versions.json</code></li> </ul>"},{"location":"dev-getting-started/#license","title":"License","text":"<p>This project is licensed under the Apache 2.0 License. All dependencies use permissive licenses (MIT, BSD, Apache 2.0, curl license).</p>"},{"location":"faq/","title":"\ud83c\udf4b Lemonade Frequently Asked Questions","text":""},{"location":"faq/#overview","title":"Overview","text":""},{"location":"faq/#1-what-is-lemonade-and-what-does-it-include","title":"1. What is Lemonade and what does it include?","text":"<p>Lemonade is an open-source local LLM solution that:       - Gets you started in minutes with one-click installers.       - Auto-configures optimized inference engines for your PC.       - Provides a convenient app to get set up and test out LLMs.       - Provides LLMs through the OpenAI API standard, enabling apps on your PC to access them.</p>"},{"location":"faq/#2-what-are-the-use-cases-for-different-audiences","title":"2. What are the use cases for different audiences?","text":"<ul> <li>LLM Enthusiasts: LLMs on your GPU or NPU with minimal setup, and connect to great apps listed here.</li> <li>Developers: Integrate LLMs into apps using standard APIs with no device-specific code. See the Server Integration Guide.</li> <li>Agent Developers: Use GAIA to quickly develop local-first agents.</li> </ul>"},{"location":"faq/#installation-compatibility","title":"Installation &amp; Compatibility","text":""},{"location":"faq/#1-how-do-i-install-lemonade-sdk-or-server","title":"1. How do I install Lemonade SDK or Server?","text":"<p>Visit https://lemonade-server.ai/install_options.html and click the options that apply to you.</p>"},{"location":"faq/#2-which-devices-are-supported","title":"2. Which devices are supported?","text":"<p>\ud83d\udc49 Supported Configurations</p> <p>For more information on AMD Ryzen AI NPU Support, see the section Hybrid/NPU.</p>"},{"location":"faq/#3-is-linux-supported-what-about-macos","title":"3. Is Linux supported? What about macOS?","text":"<p>Yes, Linux is supported!</p> <ul> <li>Linux: Visit https://lemonade-server.ai/ and check the \"Developer Setup\" section for installation instructions.</li> <li>macOS: Not supported right now, but it is on the roadmap.</li> </ul> <p>Visit the Supported Configurations section to see the support matrix for CPU, GPU, and NPU.</p>"},{"location":"faq/#4-how-do-i-uninstall-lemonade-server-windows","title":"4. How do I uninstall Lemonade Server? (Windows)","text":"<p>To uninstall Lemonade Server, use the Windows Add/Remove Programs menu.</p> <p>Optional: Remove cached files    - Open File Explorer and navigate to <code>%USERPROFILE%\\.cache</code>    - Delete the <code>lemonade</code> folder if it exists    - To remove downloaded models, delete the <code>huggingface</code> folder</p>"},{"location":"faq/#models","title":"Models","text":""},{"location":"faq/#1-where-are-models-stored-and-how-do-i-change-that","title":"1. Where are models stored and how do I change that?","text":"<p>Lemonade uses three model locations:</p> <p>Primary: Hugging Face Cache</p> <p>Models downloaded through Lemonade are stored using the Hugging Face Hub specification. By default, models are located at <code>~/.cache/huggingface/hub/</code>, where <code>~</code> is your home directory.</p> <p>For example, <code>Qwen/Qwen2.5-0.5B</code> is stored at <code>~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B</code>.</p> <p>You can change this location by setting the <code>HF_HOME</code> env var, which will store your models in <code>$HF_HOME/hub</code> (e.g., <code>$HF_HOME/hub/models--Qwen--Qwen2.5-0.5B</code>). Alternatively, you can set <code>HF_HUB_CACHE</code> and your models will be in <code>$HF_HUB_CACHE</code> (e.g., <code>$HF_HUB_CACHE/models--Qwen--Qwen2.5-0.5B</code>).</p> <p>You can use the official Hugging Face Hub utility (<code>pip install huggingface-hub</code>) to manage models outside of Lemonade, e.g., <code>hf cache ls</code> will print all models and their sizes.</p> <p>Secondary: Extra Models Directory (GGUF)</p> <p>Lemonade Server can discover GGUF models from a secondary directory using the <code>--extra-models-dir</code> option, enabling compatibility with llama.cpp and LM Studio model caches. Suggested paths:</p> <ul> <li>Windows:<ul> <li>LM Studio: <code>C:\\Users\\You\\.lmstudio\\models</code></li> <li>llamacpp: <code>%LOCALAPPDATA%\\llama.cpp</code> (e.g., <code>C:\\Users\\You\\AppData\\Local\\llama.cpp</code>)</li> </ul> </li> <li>Linux: <code>~/.cache/llama.cpp</code></li> </ul> <p>Example: <code>lemonade-server serve --extra-models-dir \"%LOCALAPPDATA%\\llama.cpp\"</code></p> <p>Any <code>.gguf</code> files found in this directory (including subdirectories) will automatically appear in Lemonade's model list in the <code>custom</code> category.</p> <p>FastFlowLM</p> <p>FastFlowLM (FLM) has its own model management system. When you first install FLM the install wizard asks for a model directory, which is then saved to the <code>FLM_MODEL_PATH</code> environment variable on your system PATH. Models are stored in that directory. If you change the variable's value, newly downloaded models will be stored on the new path, but your prior models will still be at the prior path.</p>"},{"location":"faq/#2-what-models-are-supported","title":"2. What models are supported?","text":"<p>Lemonade supports a wide range of LLMs including LLaMA, DeepSeek, Qwen, Gemma, Phi, gpt-oss, LFM, and many more. Most GGUF models can also be added to Lemonade Server by users using the Model Manager interface in the app or the <code>pull</code> command on the CLI.</p> <p>\ud83d\udc49 Supported Models List    \ud83d\udc49 pull command</p>"},{"location":"faq/#3-how-do-i-know-what-size-model-will-work-with-my-setup","title":"3. How do I know what size model will work with my setup?","text":"<p>Model compatibility depends on your system's RAM, VRAM, and NPU availability. The actual file size varies significantly between models due to different quantization techniques and architectures.</p> <p>To check if a model will work:    1. Visit the model's Hugging Face page (e.g., <code>amd/Qwen2.5-7B-Chat-awq-g128-int4-asym-fp16-onnx-hybrid</code>).    2. Check the \"Files and versions\" tab to see the actual download size.    3. Add ~2-4 GB overhead for KV cache, activations, and runtime memory.    4. Ensure your system has sufficient RAM/VRAM.</p>"},{"location":"faq/#4-im-looking-for-a-model-but-its-not-listed-in-the-model-manager","title":"4. I'm looking for a model, but it's not listed in the Model Manager.","text":"<p>If a model isn't listed, it may not be compatible with your PC due to device or RAM limitations, or we just haven't added it to the <code>server_models.json</code> file yet.</p> <p>You can:</p> <ul> <li>Add a custom model manually via the app's \"Add a Model\" interface or the CLI pull command.</li> <li>Use a pull request to add the model to the built-in <code>server_models.json</code> file.</li> <li>Request support by opening a GitHub issue.</li> </ul> <p>If you are sure that a model should be listed, but you aren't seeing it, you can set the <code>LEMONADE_DISABLE_MODEL_FILTERING</code> environment variable to show all models supported by Lemonade on any PC configuration. But please note, this can show models that definitely won't work on your system.</p> <p>Alternatively if you are attempting to use GTT on your dGPU then you can set the <code>LEMONADE_ENABLE_DGPU_GTT</code> environment variable to filter using the combined memory pool. Please note ROCM does not support splitting memory across multiple pools, vulkan is likely required for this usecase.</p>"},{"location":"faq/#5-is-there-a-script-or-tool-to-convert-models-to-ryzen-ai-npu-format","title":"5. Is there a script or tool to convert models to Ryzen AI NPU format?","text":"<p>Yes, there's a guide on preparing your models for Ryzen AI NPU:</p> <p>\ud83d\udc49 Model Preparation Guide</p>"},{"location":"faq/#6-whats-the-difference-between-gguf-and-onnx-models","title":"6. What's the difference between GGUF and ONNX models?","text":"<ul> <li>GGUF: Used with llama.cpp backend, supports CPU, and GPU via Vulkan or ROCm.</li> <li>ONNX: Used with OnnxRuntime GenAI, supports NPU and NPU+iGPU Hybrid execution.</li> </ul>"},{"location":"faq/#inference-behavior-performance","title":"Inference Behavior &amp; Performance","text":""},{"location":"faq/#1-can-lemonade-print-out-stats-like-tokens-per-second","title":"1. Can Lemonade print out stats like tokens per second?","text":"<p>Yes! Lemonade Server exposes a <code>/stats</code> endpoint that returns performance metrics from the most recent completion request:</p> <pre><code>curl http://localhost:8000/api/v1/stats\n</code></pre> <p>Or, you can launch <code>lemonade-server</code> with the option <code>--log-level debug</code> and that will also print out stats.</p>"},{"location":"faq/#2-how-does-lemonades-performance-compare-to-llamacpp","title":"2. How does Lemonade's performance compare to llama.cpp?","text":"<p>Lemonade supports llama.cpp as a backend, so performance is similar when using the same model and quantization.</p>"},{"location":"faq/#3-how-can-rocm-performance-be-improved-for-my-use-case","title":"3. How can ROCm performance be improved for my use case?","text":"<p>File a detailed issue on TheRock repo for support: https://github.com/ROCm/TheRock</p>"},{"location":"faq/#4-how-should-dedicated-gpu-ram-be-allocated-on-strix-halo","title":"4. How should dedicated GPU RAM be allocated on Strix Halo","text":"<p>Strix Halo PCs can have up to 128 GB of unified RAM and Windows allows the user to allocate a portion of this to dedicated GPU RAM.</p> <p>We suggest setting dedicated GPU RAM to <code>64/64 (auto)</code>.</p> <p>Note: On Windows, the GPU can access both unified RAM and dedicated GPU RAM, but the CPU is blocked from accessing dedicated GPU RAM. For this reason, allocating too much dedicated GPU RAM can interfere with model loading, which requires the CPU to access a substantial amount unified RAM.</p>"},{"location":"faq/#hybrid-and-npu-questions","title":"Hybrid and NPU Questions","text":""},{"location":"faq/#1-does-llm-inference-with-the-npu-only-work-on-windows","title":"1. Does LLM inference with the NPU only work on Windows?","text":"<p>Yes, today, NPU and hybrid inference is currently supported only on Windows.</p> <p>To request NPU support on Linux, file an issue with either:      - Ryzen AI SW: https://github.com/amd/ryzenai-sw      - FastFlowLM: https://github.com/FastFlowLM/FastFlowLM</p>"},{"location":"faq/#2-i-loaded-a-hybrid-model-but-the-npu-is-barely-active-is-that-expected","title":"2. I loaded a hybrid model, but the NPU is barely active. Is that expected?","text":"<p>Yes. In hybrid mode:</p> <ul> <li>The NPU handles prompt processing.</li> <li>The GPU handles token generation.</li> <li>If your prompt is short, the NPU finishes quickly. Try a longer prompt to see more NPU activity.</li> </ul>"},{"location":"faq/#3-does-lemonade-work-on-older-amd-processors-or-non-ryzen-ai-systems","title":"3. Does Lemonade work on older AMD processors or non-Ryzen AI systems?","text":"<p>Yes! Lemonade supports multiple execution modes:</p> <ul> <li>AMD Ryzen 7000/8000/200 series: GPU acceleration via llama.cpp + Vulkan backend</li> <li>Systems with Radeon GPUs: Yes</li> <li>Any x86 CPU: Yes</li> <li>Intel/NVIDIA systems: CPU inference, with GPU support via the llama.cpp + Vulkan backend</li> </ul> <p>While you won't get NPU acceleration on non-Ryzen AI 300 systems, you can still benefit from GPU acceleration and the OpenAI-compatible API.</p>"},{"location":"faq/#4-is-the-npu-on-the-amd-ryzen-ai-70008000200-series-going-to-be-supported-for-llm-inference","title":"4. Is the NPU on the AMD Ryzen AI 7000/8000/200 series going to be supported for LLM inference?","text":"<p>No inference engine providers have plans to support NPUs prior to Ryzen AI 300-series, but you can still request this by filing an issue on their respective GitHubs:       - Ryzen AI SW: https://github.com/amd/ryzenai-sw       - FastFlowLM: https://github.com/FastFlowLM/FastFlowLM</p>"},{"location":"faq/#5-how-do-i-know-what-model-architectures-are-supported-by-the-npu","title":"5. How do I know what model architectures are supported by the NPU?","text":"<p>AMD publishes pre-quantized and optimized models in their Hugging Face collections:</p> <ul> <li>Ryzen AI NPU Models</li> <li>Ryzen AI Hybrid Models</li> </ul> <p>To find the architecture of a specific model, click on any model in these collections and look for the \"Base model\" field, which will show you the underlying architecture (e.g., Llama, Qwen, Phi).</p>"},{"location":"faq/#6-how-can-i-get-better-performance-from-the-npu","title":"6. How can I get better performance from the NPU?","text":"<p>Make sure that you've put the NPU in \"Turbo\" mode to get the best results. This is done by opening a terminal window and running the following commands:</p> <pre><code>cd C:\\Windows\\System32\\AMD\n.\\xrt-smi configure --pmode turbo\n</code></pre>"},{"location":"faq/#remote-access","title":"Remote Access","text":""},{"location":"faq/#1-how-do-i-run-lemonade-server-on-one-pc-and-access-it-from-another","title":"1. How do I run Lemonade Server on one PC and access it from another?","text":"<p>Lemonade supports running the server on one machine while using the app from another machine on the same network.</p> <p>Quick setup:    1. On the server machine, start with network access enabled:       <pre><code>lemonade-server serve --host 0.0.0.0 --port 8000\n</code></pre>    2. On the client machine, launch the app and configure the endpoint through the UI:       <pre><code>lemonade-app\n</code></pre></p> <p>For detailed instructions and security considerations, see Remote Server Connection.</p>"},{"location":"faq/#customization","title":"Customization","text":""},{"location":"faq/#1-how-do-i-use-my-own-llamacpp-or-whispercpp-binaries","title":"1. How do I use my own llama.cpp or whisper.cpp binaries?","text":"<p>Lemonade Server allows you to use custom <code>llama-server</code> or <code>whisper-server</code> binaries instead of the bundled ones by setting environment variables to the full path of your binary.</p> <p>\ud83d\udc49 Custom Backend Binaries</p>"},{"location":"faq/#support-roadmap","title":"Support &amp; Roadmap","text":""},{"location":"faq/#1-what-if-i-encounter-installation-or-runtime-errors","title":"1. What if I encounter installation or runtime errors?","text":"<p>Check the Lemonade Server logs via the App (all supported OSes) or tray icon (Windows only). Common issues include model compatibility or outdated versions.</p> <p>\ud83d\udc49 Open an Issue on GitHub</p>"},{"location":"faq/#2-lemonade-is-missing-a-feature-i-really-want-what-should-i-do","title":"2. Lemonade is missing a feature I really want. What should I do?","text":"<p>Open a feature request on GitHub. We're actively shaping the roadmap based on user feedback.</p>"},{"location":"faq/#3-do-you-plan-to-share-a-roadmap","title":"3. Do you plan to share a roadmap?","text":"<p>Yes! Check out the project README:</p> <p>\ud83d\udc49 Lemonade Roadmap</p>"},{"location":"self_hosted_runners/","title":"\ud83c\udf29\ufe0f Self Hosted Runners \ud83c\udf29\ufe0f Documentation","text":"<p>This page documents how to set up and maintain self-hosted runners for lemonade-sdk.</p> <p>Topics:  - What are Self-Hosted Runners?  - NPU Runner Setup  - Maintenance and Troubleshooting     - Check your runner's status     - Actions are failing unexpectedly     - Take a laptop offline - Creating Workflows     - Capabilities and Limitations</p>"},{"location":"self_hosted_runners/#what-are-self-hosted-runners","title":"What are Self-Hosted Runners?","text":"<p>A \"runner\" is a computer that has installed GitHub's runner software, which runs a service that makes the laptop available to run GitHub Actions. In turn, Actions are defined by Workflows, which specify when the Action should run (manual trigger, CI, CD, etc.) and what the Action does (run tests, build packages, run an experiment, etc.).</p> <p>You can read about all this here: GitHub: About self-hosted runners.</p>"},{"location":"self_hosted_runners/#npu-runner-setup","title":"NPU Runner Setup","text":"<p>This guide will help you set up a Ryzen AI laptop as a GitHub self-hosted runner. This will make the laptop available for on-demand and CI jobs that require NPU resources.</p>"},{"location":"self_hosted_runners/#new-machine-setup","title":"New Machine Setup","text":"<ul> <li>Install the following software:<ul> <li>The latest RyzenAI driver ONLY (do not install RyzenAI Software), which is available here</li> <li>VS Code</li> <li>git</li> </ul> </li> <li>If your laptop has an Nvidia GPU, you must disable it in device manager</li> <li>Open a PowerShell script in admin mode, and run <code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned</code></li> <li>Go into Windows settings:</li> <li>Go to system, power &amp; battery, screen sleep &amp; hibernate timeouts, and make it so the laptop never sleeps while plugged in. If you don't do this it can fall asleep during jobs.</li> <li>Search \"Change the date and time\", and then click \"sync\" under \"additional settings.\"</li> </ul>"},{"location":"self_hosted_runners/#runner-configuration","title":"Runner Configuration","text":"<p>These steps will place your machine in the <code>stx-test</code> pool, which is where we put machines while we are setting them up. In the next section we will finalize setup and then move the runner into the production pool.</p> <ol> <li> <p>IMPORTANT: before doing step 2, read this:</p> <ul> <li>Use a powershell administrator mode terminal</li> <li>Enable permissions by running <code>Set-ExecutionPolicy RemoteSigned</code></li> <li>When running <code>./config.cmd</code> in step 2, make the following choices:<ul> <li>Name of the runner group = <code>stx</code></li> <li>For the runner name, call it <code>NAME-stx-NUMBER</code>, where NAME is your alias and NUMBER would tell you this is the Nth STX machine you've added.</li> <li>Apply the label <code>stx-test</code> as well as a label with your name to indicate that you are maintaining the runner.</li> <li>Accept the default for the work folder</li> <li>You want the runner to function as a service (respond Y)</li> <li>User account to use for the service = <code>NT AUTHORITY\\SYSTEM</code> (not the default of <code>NT AUTHORITY\\NETWORK SERVICE</code>)</li> </ul> </li> </ul> </li> <li> <p>Follow the instructions here for Windows|Ubuntu, minding what we said in step 1: https://github.com/organizations/lemonade-sdk/settings/actions/runners/new</p> </li> <li>You should see your runner show up in the <code>stx</code> runner group in the lemonade-sdk org</li> </ol>"},{"location":"self_hosted_runners/#runner-setup","title":"Runner Setup","text":"<p>These steps will use GitHub Actions to run automated setup and validation for your new runner while it is still in the <code>stx-test</code> group.</p> <ol> <li>Go to the lemonade ryzenai test action and click \"run workflow\".<ul> <li>Select <code>stx-test</code> as the runner group</li> <li>Click <code>Run workflow</code></li> </ul> </li> <li>The workflow should appear at the top of the queue. Click into it.<ul> <li>Expand the <code>Set up job</code> section and make sure <code>Runner name:</code> refers to your new runner. Otherwise, the job may have gone to someone else's runner in the test group. You can re-queue the workflow until it lands on your runner.</li> <li>Wait for the workflow to finish successfully.</li> </ul> </li> <li>Repeat step 1. Wait for it to finish successfully. Congrats, your new runner is working!</li> <li>Go to the stx Runner Group, click your new runner, and click the gear icon to change labels. Uncheck <code>stx-test</code> and check <code>stx</code>.</li> <li>Done!</li> </ol>"},{"location":"self_hosted_runners/#maintenance-and-troubleshooting","title":"Maintenance and Troubleshooting","text":"<p>This is a production system and things will go wrong. Here is some advice on what to do.</p>"},{"location":"self_hosted_runners/#check-your-runners-status","title":"Check your runner's status","text":"<p>You can run <code>Get-EventLog -LogName Application -Source ActionsRunnerService</code> in a powershell terminal on your runner to get more information about what it's been up to.</p> <p>If there have been any problems recently, they may show up like:</p> <ul> <li>Error: Runner connect error: &lt; details about the connection error &gt;</li> <li>Information: Runner reconnected</li> <li>Information: Running Job: &lt; job name &gt;</li> <li>Information: Job &lt; job name &gt; completed with result: [Succeeded / Canceled / Failed]</li> </ul>"},{"location":"self_hosted_runners/#actions-are-failing-unexpectedly","title":"Actions are failing unexpectedly","text":"<p>Actions fail all the time, often because they are testing buggy code. However, sometimes an Action will fail because something is wrong with the specific runner that ran the Action.</p> <p>If this happens to you, here are some steps you can take (in order): 1. Take note of which runner executed your Action. You can check this by going to the <code>Set up job</code> section of the Action's log and checking the <code>Runner name:</code> field. The machine name in that field will correspond to a machine on the runners page. 1. Re-queue your job. It is possible that that the failure is a one-off, and it will work the next time on the same runner. Re-queuing also gives you a chance of getting a runner that is in a healthier state. 1. If the same runner is consistently failing, it is probably in an unhealthy state (or you have a bug in your code and you're just blaming the runner). If a runner is in an unhealthy state:     1. Take the laptop offline so that it stops being allocated Actions.     1. Open an Issue. Assign it to the maintainer of the laptop (their name should be in the runner's name). Link the multiple failed workflows that have convinced you that this runner is unhealthy.     1. Re-queue your job. You'll definitely get a different runner now since you took the unhealthy runner offline. 1. If all runners are consistently failing your workflow, seriously think about whether your code is the problem.</p>"},{"location":"self_hosted_runners/#take-a-laptop-offline","title":"Take a laptop offline","text":"<p>If you need to do some maintenance on your laptop, use it for dev/demo work, etc. you can remove it from the runners pool.</p> <p>Also, if someone else's laptop is misbehaving and causing Actions to fail unexpectedly, you can remove that laptop from the runners pool to make sure that only healthy laptops are selected for work.</p> <p>There are three options:</p> <p>Option 1, which is available to anyone in the <code>lemonade-sdk</code> org: remove the <code>rai300_400</code> label from the runner. - Workflows use <code>runs-on: rai300_400</code> to target runners with the <code>rai300_400</code> label. Removing this label from the runner will thus remove the runner from the pool. - Go to the runners page, click the specific runner in question, click the gear icon in the Labels section, and uncheck <code>rai300_400</code>. - To reverse this action later, go back to the runners page, click the gear icon, and check <code>rai300_400</code>.</p> <p>Option 2, which requires physical/remote access to the laptop: - In a PowerShell terminal, run <code>Stop-Service \"actions.runner.*\"</code>. - To reverse this action, run <code>Start-Service \"actions.runner.*\"</code>.</p> <p>Option 3 is to just turn the laptop off :)</p>"},{"location":"self_hosted_runners/#creating-workflows","title":"Creating Workflows","text":"<p>GitHub Workflows define the Actions that run on self-hosted laptops to perform testing and experimentation tasks. This section will help you learn about what capabilities are available and show some examples of well-formed workflows.</p>"},{"location":"self_hosted_runners/#capabilities-and-limitations","title":"Capabilities and Limitations","text":"<p>Because we use self-hosted systems, we have to be careful about what we put into these workflows so that we avoid: - Corrupting the laptops, causing them to produce inconsistent results or failures. - Over-subscribing the capacity of the available laptops</p> <p>Here are some general guidelines to observe when creating or modifying workflows. If you aren't confident that you are properly following these guidelines, please contact someone to review your code before opening your PR.</p> <ul> <li>Place a \ud83c\udf29\ufe0f emoji in the name of all of your self-host workflows, so that PR reviewers can see at a glance which workflows are using self-hosted resources.<ul> <li>Example: <code>name: Test Lemonade on NPU and Hybrid with OGA environment \ud83c\udf29\ufe0f</code></li> </ul> </li> <li>Avoid triggering your workflow before anyone has had a chance to review it against these guidelines. To avoid triggers, do not include <code>on: pull request:</code> in your workflow until after a reviewer has signed off.</li> <li>Only map a workflow with <code>runs on: rai300_400</code> if it actually requires Ryzen AI compute. If a step in your workflow can use generic compute (e.g., running a Hugging Face LLM on CPU), put that step on a generic non-self-hosted runner like <code>runs on: windows-latest</code>.</li> <li>Be very considerate about installing software on to the runners:<ul> <li>Installing software into the CWD (e.g., a path of <code>.\\</code>) is always ok, because that will end up in <code>C:\\actions-runner\\_work\\REPO</code>, which is always wiped between tests.</li> <li>Installing software into <code>AppData</code>, <code>Program Files</code>, etc. is not advisable because that software will persist across tests. See the setup section to see which software is already expected on the system.</li> </ul> </li> <li>Always create new virtual environments in the CWD, for example <code>python -m venv .venv</code>.<ul> <li>This way, the virtual environment is located in <code>C:\\actions-runner\\_work\\REPO</code>, which is wiped between tests.</li> <li>Make sure to activate your virtual environment before running any <code>pip install</code> commands. Otherwise your workflow will modify the system Python installation!</li> </ul> </li> <li>PowerShell scripts do not necessarily raise errors by programs they call.<ul> <li>That means PowerShell can call a Python test, and then keep going and claim \"success\" even if that Python test fails and raises an error (non-zero exit code).</li> <li>You can add <code>if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }</code> after any line of script where it is that is particularly important to fail the workflow if the program in the preceding line raised an error.<ul> <li>For example, this will make sure that lemonade installed correctly:<ol> <li>pip install -e .</li> <li>if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }</li> </ol> </li> </ul> </li> </ul> </li> <li>Be considerate of how long your workflow will run for, and how often it will be triggered.<ul> <li>All workflows go into the same queue and share the same pool of runners.</li> <li>A good target length for a workflow is 15 minutes.</li> </ul> </li> <li>Be considerate of how much data your workflow will download.<ul> <li>It would be very bad to fill up a hard drive, since Windows machines misbehave pretty bad when their drives are full.</li> <li>Place your Hugging Face cache inside the <code>_work</code> directory so that it will be wiped after each job.<ul> <li>Example: <code>$Env:HF_HOME=\".\\hf-cache\"</code></li> </ul> </li> <li>Place your Lemonade cache inside the <code>_work</code> directory so that it will be wiped after each job.<ul> <li>Example: <code>lemonade-eval -d .\\ci-cache</code> or <code>$Env:LEMONADE_CACHE_DIR=\".\\ci-cache\"</code>. Use the environment variable, rather than the <code>-d</code> flag, wherever possible since it will apply to all lemonade-eval calls within the job step.</li> </ul> </li> </ul> </li> </ul>"},{"location":"self_hosted_runners/#license","title":"License","text":"<p>Apache 2.0 License</p> <p>Copyright(C) 2024-2025 Advanced Micro Devices, Inc. All rights reserved. SPDX-License-Identifier: MIT</p>"},{"location":"server/","title":"Getting Started with Lemonade Server","text":"<p>\ud83c\udf4b Lemonade Server is a server interface that uses the standard Open AI API, allowing applications to integrate with local LLMs. This means that you can easily replace cloud-based LLMs with private and free LLMs that run locally on your own PC's NPU and GPU.</p> <p>Lemonade Server is available as a standalone tool with a one-click Windows GUI installer.</p>"},{"location":"server/#intro-video","title":"Intro Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p> <p>Once you've installed, we recommend checking out these resources:</p> Documentation Description Supported Applications Explore applications that work out-of-the-box with Lemonade Server. Lemonade Server Concepts Background knowledge about local LLM servers and the OpenAI standard. <code>lemonade-server</code> CLI Guide Learn how to manage the server process and install new models using the command-line interface. Models List Browse a curated set of LLMs available for serving. Server Spec Review all supported OpenAI-compatible and Lemonade-specific API endpoints. Integration Guide Step-by-step instructions for integrating Lemonade Server into your own applications. <p>Note: if you want to develop Lemonade Server itself, you can install from source.</p>"},{"location":"server/#integrate-lemonade-server-with-your-application","title":"Integrate Lemonade Server with Your Application","text":"<p>Since Lemonade Server implements the standard OpenAI API specification, you can use any OpenAI-compatible client library by configuring it to use <code>http://localhost:8000/api/v1</code> as the base URL. A table containing official and popular OpenAI clients on different languages is shown below.</p> <p>Feel free to pick and choose your preferred language.</p> Python C++ Java C# Node.js Go Ruby Rust PHP openai-python openai-cpp openai-java openai-dotnet openai-node go-openai ruby-openai async-openai openai-php"},{"location":"server/#python-client-example","title":"Python Client Example","text":"<pre><code>from openai import OpenAI\n\n# Initialize the client to use Lemonade Server\nclient = OpenAI(\n    base_url=\"http://localhost:8000/api/v1\",\n    api_key=\"lemonade\"  # required but unused\n)\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.2-1B-Instruct-Hybrid\",  # or any other available model\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\n# Print the response\nprint(completion.choices[0].message.content)\n</code></pre> <p>For more detailed integration instructions, see the Integration Guide.</p>"},{"location":"server/concepts/","title":"Local LLM Server Concepts","text":"<p>This document gives background information about the main concepts for local LLM servers and \ud83c\udf4bLemonade Server.</p> <p>The intention is to answer these FAQs:</p> <ul> <li>What is a Local Server?</li> <li>What is a Local LLM Server?</li> <li>What is the OpenAI Standard?</li> <li>How does the OpenAI Standard work?</li> </ul>"},{"location":"server/concepts/#what-is-a-local-server","title":"What is a Local Server?","text":"<p>First, let\u2019s clarify what we mean by <code>server software</code>, as it\u2019s sometimes confused with <code>server hardware</code>, which is the actual physical systems running in data centers. - <code>Server software</code> refers to a process running on a computer that listens for and responds to requests initiated by <code>client software</code> (i.e., applications). - <code>Server software</code> often runs on <code>server hardware</code>, but there are many examples of <code>server software</code> running on the same <code>client hardware</code> (laptop, desktop computer, tablet, or smartphone) as the <code>application</code>.</p> <p>A <code>local server</code> is <code>server software</code> that runs on <code>client hardware</code>.</p>"},{"location":"server/concepts/#what-is-a-local-llm-server","title":"What is a Local LLM Server?","text":"<p>Local LLM servers are an extremely popular way of deploying LLMs directly to <code>client hardware</code>. A few famous examples of local LLM servers include Ollama, llama-cpp-server, and Docker Model Runner.</p> <p>The local server process loads the LLM into memory and exposes it to client software for handling requests. Compared to integrating the LLM directly into the client software using C++ or Python APIs, this setup provides the following benefits:</p> Benefit Description Simplified integration C++/Python APIs are typically framework- (e.g., llama.cpp, OGA, etc.) and/or device- (e.g., CPU, GPU, NPU, etc.) specific. Local LLM servers, on the other hand, facilitate conversing with the LLM at a high level that abstracts these details away (see What is the OpenAI Standard?). Sharing LLMs between applications A single local LLM can take up a significant portion of system RAM. The local LLM server can share this LLM between multiple applications, rather than requiring each application to load its own LLM into RAM. Separation of concerns Installing and managing LLMs, enabling features like tool use and streaming generation, and building in fault tolerance can be tricky to implement. A local LLM server abstracts away this complexity, letting application developers stay focused on their app. Cloud-to-client development A common practice for LLM developers is to first develop their application using cloud LLMs, then switch to local LLMs later in development. Local and cloud LLM servers behave similarly from the application's perspective, which makes this transition seamless."},{"location":"server/concepts/#what-is-the-openai-standard","title":"What is the OpenAI Standard?","text":"<p>All LLM servers (cloud or local) adhere to an application-program interface (API). This API lets the <code>application</code> make LLM requests to the <code>server software</code>.</p> <p>While there are several popular LLM server APIs available in the LLM ecosystem, the OpenAI API has emerged as the industry standard because it is (at the time of this writing) the only API that meets these three criteria: 1. Dozens of popular LLM <code>servers</code> support OpenAI API. 1. Dozens of popular LLM <code>applications</code> support OpenAI API. 1. OpenAI API is broadly supported in both <code>local</code> and <code>cloud</code>.</p> <p>Crucially, while OpenAI offers their own LLM API-as-a-cloud-service, their API standard is rigorously documented and available for other cloud and local servers to adopt.</p>"},{"location":"server/concepts/#how-does-the-openai-standard-work","title":"How does the OpenAI Standard Work?","text":"<p>In the OpenAI API standard, applications and servers communicate in the form of a multi-role conversation. There are three \"roles\" in this context: the \"system\", the \"assistant\", and the \"user\".</p> Role Description System Allows the application to provide instructions to the LLM, such as defining its persona, what tools are available to it, what tasks it is supposed to help with or not help with, etc. Assistant Messages sent from the LLM to the application. User Messages sent from the application to the LLM. Often these messages are written by the application's end-user. <p>OpenAI also provides convenient libraries in JavaScript, Python, .Net, Java, and Go to help application and server developers adhere to the standard.</p> <p>For example, the following Python code demonstrates how an application can request an LLM response from the Lemonade Server:</p> <pre><code># Client library provided by OpenAI to automate request\n# and response processing with the server\nfrom openai import OpenAI\n\n# The base_url points to an LLM server, which can either be\n# local (localhost address) or cloud-based (web address)\nbase_url = f\"http://localhost:8000/api/v1\"\n\n# The `client` instance here provides APIs to request\n# LLM invocations from the server\nclient = OpenAI(\n    base_url=base_url,\n    api_key=\"lemonade\",  # required, but unused in Lemonade\n)\n\n# The `messages` list provides the history of messages from\n# the system, assistant, and user roles\nmessages = [\n    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n    {\"role\":\"user\", \"content\":\"Hi, how are you?\"},\n]\n\n# This is the API call that sends the `messages` history to\n# the server's specific LLM `model`\n# It returns a `completion`, which is OpenAI's way of referring\n# to the LLM's reponse to the messages\ncompletion = client.chat.completions.create(\n    model=\"Llama-3.1-8B-Instruct-Hybrid\",\n    messages=messages,\n)\n\n# This code gets the LLM's response from the `completion`\n# and prints it to the screen\nresponse = completion.choices[0].message.content\nprint(response)\n</code></pre> <p>The Python above will work with Lemonade Server, along with a variety of other cloud and local LLM servers, just by changing the <code>base_url</code>, <code>api_key</code>, and <code>model</code> as needed. This example demonstrates that details like deployment location (local vs. cloud), hardware type (GPU vs. NPU), and backend implementation (OGA vs. llama.cpp), etc. are hidden behind a unified interface.</p>"},{"location":"server/lemonade-server-cli/","title":"<code>lemonade-server</code> CLI","text":"<p>The <code>lemonade-server</code> command-line interface (CLI) provides a set of utility commands for managing the server. When you install, <code>lemonade-server</code> is added to your PATH so that it can be invoked from any terminal.</p> <p>Contents:</p> <ul> <li>Commands</li> <li>Options for serve and run</li> <li>Environment Variables | Custom Backend Binaries | API Key and Security</li> <li>Options for pull</li> <li>Lemonade Desktop App | Remote Server Connection</li> </ul>"},{"location":"server/lemonade-server-cli/#commands","title":"Commands","text":"<p><code>lemonade-server</code> provides these utilities:</p> Option/Command Description <code>-v</code>, <code>--version</code> Print the <code>lemonade-sdk</code> package version used to install Lemonade Server. <code>serve</code> Start the server process in the current terminal. See command options below. <code>status</code> Check if server is running. If it is, print the port number. <code>stop</code> Stop any running Lemonade Server process. <code>pull MODEL_NAME</code> Install an LLM named <code>MODEL_NAME</code>. See pull command options for registering custom models. <code>run MODEL_NAME</code> Start the server (if not already running) and chat with the specified model. Supports the same options as <code>serve</code>. <code>list</code> List all models. <code>delete MODEL_NAME</code> Delete a model and its files from local storage. <p>Examples:</p> <pre><code># Start server with custom settings\nlemonade-server serve --port 8080 --log-level debug --llamacpp vulkan\n\n# Run a specific model with custom server settings\nlemonade-server run Qwen3-0.6B-GGUF --port 8080 --log-level debug --llamacpp rocm\n</code></pre>"},{"location":"server/lemonade-server-cli/#options-for-serve-and-run","title":"Options for serve and run","text":"<p>When using the <code>serve</code> command, you can configure the server with these additional options. The <code>run</code> command supports the same options but also requires a <code>MODEL_NAME</code> parameter:</p> <pre><code>lemonade-server serve [options]\nlemonade-server run MODEL_NAME [options]\n</code></pre> Option Description Default <code>--port [port]</code> Specify the port number to run the server on 8000 <code>--host [host]</code> Specify the host address for where to listen connections <code>localhost</code> <code>--log-level [level]</code> Set the logging level info <code>--no-tray</code> Start server without the tray app (headless mode) False <code>--llamacpp [vulkan\\|rocm\\cpu]</code> Default LlamaCpp backend to use when loading models. Can be overridden per-model via the <code>/api/v1/load</code> endpoint. vulkan <code>--ctx-size [size]</code> Default context size for models. For llamacpp recipes, this sets the <code>--ctx-size</code> parameter for the llama server. For other recipes, prompts exceeding this size will be truncated. Can be overridden per-model via the <code>/api/v1/load</code> endpoint. 4096 <code>--llamacpp-args [args]</code> Default custom arguments to pass to llama-server. Must not conflict with arguments managed by Lemonade (e.g., <code>-m</code>, <code>--port</code>, <code>--ctx-size</code>, <code>-ngl</code>). Can be overridden per-model via the <code>/api/v1/load</code> endpoint. Example: <code>--llamacpp-args \"--flash-attn on --no-mmap\"</code> \"\" <code>--extra-models-dir [path]</code> Experimental feature. Secondary directory to scan for LLM GGUF model files. Audio, embedding, reranking, and non-GGUF files are not supported, yet. None <code>--max-loaded-models [LLMS] [EMBEDDINGS] [RERANKINGS] [AUDIO]</code> Maximum number of models to keep loaded simultaneously. Accepts 1, 3, or 4 values for LLM, embedding, reranking, and audio models respectively. Unspecified values default to 1. Example: <code>--max-loaded-models 3 2 1 1</code> loads up to 3 LLMs, 2 embedding models, 1 reranking model, and 1 audio model. <code>1 1 1 1</code> <code>--save-options</code> Only available for the run command. Saves the context size, LlamaCpp backend and custom llama-server arguments as default for running this model. Unspecified values will be saved using their default value. False"},{"location":"server/lemonade-server-cli/#environment-variables","title":"Environment Variables","text":"<p>These settings can also be provided via environment variables that Lemonade Server recognizes regardless of launch method:</p> Environment Variable Description <code>LEMONADE_HOST</code> Host address for where to listen for connections <code>LEMONADE_PORT</code> Port number to run the server on <code>LEMONADE_LOG_LEVEL</code> Logging level <code>LEMONADE_LLAMACPP</code> Default LlamaCpp backend (<code>vulkan</code>, <code>rocm</code>, or <code>cpu</code>) <code>LEMONADE_WHISPERCPP</code> Default WhisperCpp backend (<code>cpu</code> or <code>npu</code>) <code>LEMONADE_CTX_SIZE</code> Default context size for models <code>LEMONADE_LLAMACPP_ARGS</code> Custom arguments to pass to llama-server <code>LEMONADE_EXTRA_MODELS_DIR</code> Secondary directory to scan for GGUF model files <code>LEMONADE_DISABLE_MODEL_FILTERING</code> Set to <code>1</code> to disable hardware-based model filtering (e.g., RAM amount, NPU availability) and show all models regardless of system capabilities <code>LEMONADE_ENABLE_DGPU_GTT</code> Set to <code>1</code> to include GTT for hardware-based model filtering"},{"location":"server/lemonade-server-cli/#custom-backend-binaries","title":"Custom Backend Binaries","text":"<p>You can provide your own <code>llama-server</code>, <code>whisper-server</code>, or <code>ryzenai-server</code> binary by setting the full path via the following environment variables:</p> Environment Variable Description <code>LEMONADE_LLAMACPP_ROCM_BIN</code> Path to custom <code>llama-server</code> binary for ROCm backend <code>LEMONADE_LLAMACPP_VULKAN_BIN</code> Path to custom <code>llama-server</code> binary for Vulkan backend <code>LEMONADE_LLAMACPP_CPU_BIN</code> Path to custom <code>llama-server</code> binary for CPU backend <code>LEMONADE_WHISPERCPP_CPU_BIN</code> Path to custom <code>whisper-server</code> binary for CPU backend <code>LEMONADE_WHISPERCPP_NPU_BIN</code> Path to custom <code>whisper-server</code> binary for NPU backend <code>LEMONADE_RYZENAI_SERVER_BIN</code> Path to custom <code>ryzenai-server</code> binary for NPU/Hybrid models <p>Note: These environment variables do not override the <code>--llamacpp</code> option. They allow you to specify an alternative binary for specific backends while still using the standard backend selection mechanism.</p> <p>Examples:</p> <p>On Windows:</p> <pre><code>set LEMONADE_LLAMACPP_VULKAN_BIN=C:\\path\\to\\my\\llama-server.exe\nlemonade-server serve\n</code></pre> <p>On Linux:</p> <pre><code>export LEMONADE_LLAMACPP_VULKAN_BIN=/path/to/my/llama-server\nlemonade-server serve\n</code></pre>"},{"location":"server/lemonade-server-cli/#api-key-and-security","title":"API Key and Security","text":"<p>If you expose your server over a network you can use the <code>LEMONADE_API_KEY</code> environment variable to set an API key (use a random long string) that will be required to execute any request. The API key will be expected as HTTP Bearer authentication, which is compatible with the OpenAI API.</p> <p>IMPORTANT: If you need to access <code>lemonade-server</code> over the internet, do not expose it directly! You will also need to setup an HTTPS reverse proxy (such as nginx) and expose that instead, otherwise all communication will be in plaintext!</p>"},{"location":"server/lemonade-server-cli/#options-for-pull","title":"Options for pull","text":"<p>The <code>pull</code> command downloads and installs models. For models already in the Lemonade Server registry, only the model name is required. To register and install custom models from Hugging Face, use the registration options below:</p> <pre><code>lemonade-server pull &lt;model_name&gt; [options]\n</code></pre> Option Description Required <code>--checkpoint CHECKPOINT</code> Hugging Face checkpoint in the format <code>org/model:variant</code>. For GGUF models, the variant (after the colon) is required. Examples: <code>unsloth/Qwen3-8B-GGUF:Q4_0</code>, <code>amd/Qwen3-4B-awq-quant-onnx-hybrid</code> For custom models <code>--recipe RECIPE</code> Inference recipe to use. Options: <code>llamacpp</code>, <code>flm</code>, <code>ryzenai-llm</code> For custom models <code>--reasoning</code> Mark the model as a reasoning model (e.g., DeepSeek-R1). Adds the 'reasoning' label to model metadata. No <code>--vision</code> Mark the model as a vision/multimodal model. Adds the 'vision' label to model metadata. No <code>--embedding</code> Mark the model as an embedding model. Adds the 'embeddings' label to model metadata. For use with the <code>/api/v1/embeddings</code> endpoint. No <code>--reranking</code> Mark the model as a reranking model. Adds the 'reranking' label to model metadata. For use with the <code>/api/v1/reranking</code> endpoint. No <code>--mmproj FILENAME</code> Multimodal projector file for GGUF vision models. Example: <code>mmproj-model-f16.gguf</code> For vision models <p>Notes: - Custom model names must use the <code>user.</code> namespace prefix (e.g., <code>user.MyModel</code>) - GGUF models require a variant specified in the checkpoint after the colon - Use <code>lemonade-server pull --help</code> to see examples and detailed information</p> <p>Examples:</p> <pre><code># Install a registered model from the Lemonade Server registry\nlemonade-server pull Qwen3-0.6B-GGUF\n\n# Register and install a custom GGUF model\nlemonade-server pull user.Phi-4-Mini-GGUF \\\n  --checkpoint unsloth/Phi-4-mini-instruct-GGUF:Q4_K_M \\\n  --recipe llamacpp\n\n# Register and install a vision model with multimodal projector\nlemonade-server pull user.Gemma-3-4b \\\n  --checkpoint ggml-org/gemma-3-4b-it-GGUF:Q4_K_M \\\n  --recipe llamacpp \\\n  --vision \\\n  --mmproj mmproj-model-f16.gguf\n\n# Register and install an embedding model\nlemonade-server pull user.nomic-embed \\\n  --checkpoint nomic-ai/nomic-embed-text-v1-GGUF:Q4_K_S \\\n  --recipe llamacpp \\\n  --embedding\n</code></pre> <p>For more information about model formats and recipes, see the API documentation and the server models guide.</p>"},{"location":"server/lemonade-server-cli/#lemonade-desktop-app","title":"Lemonade Desktop App","text":"<p>The Lemonade Desktop App provides a graphical interface for chatting with models and managing the server. When installed via the full installer, <code>lemonade-app</code> is added to your PATH for easy command-line access.</p>"},{"location":"server/lemonade-server-cli/#launching-the-app","title":"Launching the App","text":"<pre><code># Launch the app (connects to local server automatically)\nlemonade-app\n</code></pre> <p>By default, the app connects to a server running on <code>localhost</code> and automatically discovers the port. To connect to a remote server, change the app settings.</p>"},{"location":"server/lemonade-server-cli/#remote-server-connection","title":"Remote Server Connection","text":"<p>To connect the app to a server running on a different machine:</p> <ol> <li> <p>Start the server with network access on the host machine:    <pre><code>lemonade-server serve --host 0.0.0.0 --port 8000\n</code></pre></p> <p>Note: Using <code>--host 0.0.0.0</code> allows connections from other machines on the network. Only do this on trusted networks. You can use <code>LEMONADE_API_KEY</code> (see above) to manage access on your network.</p> </li> <li> <p>Launch the app on the client machine and configure the endpoint through the UI:    <pre><code>lemonade-app\n</code></pre></p> </li> </ol> <p>The app automatically discovers and connects to a local server unless an endpoint is explicitly configured in the UI.</p>"},{"location":"server/lemonade-server-cli/#next-steps","title":"Next Steps","text":"<p>The Lemonade Server integration guide provides more information about how these commands can be used to integrate Lemonade Server into an application.</p>"},{"location":"server/server_integration/","title":"Integrating with Lemonade Server","text":"<p>This guide provides instructions on how to integrate Lemonade Server into your application.</p> <p>There are two main ways in which Lemonade Server might integrate into apps:</p> <ul> <li>User-Managed Server: User is responsible for installing and managing Lemonade Server.</li> <li>App-Managed Server: App is responsible for installing and managing Lemonade Server on behalf of the user.</li> </ul> <p>The first part of this guide contains instructions that are common for both integration approaches. The second part provides advanced instructions only needed for app-managed server integrations.</p>"},{"location":"server/server_integration/#general-instructions","title":"General Instructions","text":""},{"location":"server/server_integration/#identifying-existing-installation","title":"Identifying Existing Installation","text":"<p>To identify if Lemonade Server is installed on a system, you can use the <code>lemonade-server</code> CLI command, which is added to path when using our installer. This is a reliable method to:</p> <ul> <li>Verify if the server is installed.</li> <li>Check which version is currently available by running the command below.</li> </ul> <pre><code>lemonade-server --version\n</code></pre> <p>Note: The <code>lemonade-server</code> CLI command is added to PATH when using the Windows Installer (lemonade-server-minimal.msi) and Debian Installer (lemonade-server-minimal__amd64.deb)."},{"location":"server/server_integration/#checking-server-status","title":"Checking Server Status","text":"<p>To identify whether or not the server is running anywhere on the system you may use the <code>status</code> command of <code>lemonade-server</code>.</p> <pre><code>lemonade-server status\n</code></pre> <p>This command will return either <code>Server is not running</code> or <code>Server is running on port &lt;PORT&gt;</code>.</p>"},{"location":"server/server_integration/#identifying-compatible-devices","title":"Identifying Compatible Devices","text":"<p>AMD Ryzen\u2122 AI <code>Hybrid</code> and <code>NPU</code> models are available on Windows 11 on all AMD Ryzen\u2122 AI 300 Series, 400 Series, and Z2 Series Processors. To programmatically identify supported devices, we recommend using a regular expression that checks if the CPU name converted to lowercase contains \"ryzen ai\" and either a 3-digit number starting with 3 or 4, or \"z2\" as shown below.</p> <pre><code>ryzen ai.*((\\b[34]\\d{2}\\b)|(\\bz2\\b))\n</code></pre> <p>Explanation:</p> <ul> <li><code>ryzen ai</code>: Matches the literal phrase \"Ryzen AI\".</li> <li><code>.*</code>: Allows any characters (including spaces) to appear after \"Ryzen AI\".</li> <li><code>((\\b[34]\\d{2}\\b)|(\\bz2\\b))</code>: Matches either a three-digit number starting with 3 or 4 (for 300/400 series), or \"z2\" (for Z2 series like Z2 Extreme), ensuring it's a standalone word.</li> </ul> <p>There are several ways to check the CPU name on a Windows computer. A reliable way of doing so is through cmd's <code>reg query</code> command as shown below.</p> <pre><code>reg query \"HKEY_LOCAL_MACHINE\\HARDWARE\\DESCRIPTION\\System\\CentralProcessor\\0\" /v ProcessorNameString\n</code></pre> <p>Once you capture the CPU name, make sure to convert it to lowercase before using the regular expression.</p>"},{"location":"server/server_integration/#downloading-server-installer","title":"Downloading Server Installer","text":"<p>The recommended way of directing users to the installer is pointing users to https://lemonade-server.ai/install_options.html</p>"},{"location":"server/server_integration/#installing-additional-models","title":"Installing Additional Models","text":"<p>If you want to install models on behalf of your users, the following tools are available:</p> <ul> <li>LLMs that are already available in lemonade:</li> <li>Run <code>lemonade-server list</code>.</li> <li>Use the models endpoint.</li> <li>A human-readable list of supported models. Do not modify this file in an existing install (see <code>user_models.json</code> below).</li> <li>A JSON file that defines the list of built-in models.</li> <li> <p><code>lemonade-server pull MODEL</code> on the command line interface.</p> </li> <li> <p>Adding new LLMs:</p> </li> <li> <p>The <code>user_models.json</code> file is similar to <code>server_models.json</code> (see above), but contains a user-specific registry that persists across lemonade updates. It is located at <code>$LEMONADE_CACHE_DIR/user_models.json</code>, which defaults to <code>~/.cache/lemonade/user_models.json</code>.</p> </li> <li>The <code>pull</code> endpoint in the server automates the process of registering models into <code>user_models.json</code> and downloading them.</li> <li>The <code>lemonade-server pull</code> CLI command can also register and download new models, see Options for pull.</li> </ul>"},{"location":"server/server_integration/#stand-alone-server-integration","title":"Stand-Alone Server Integration","text":"<p>Some apps might prefer to be responsible for installing and managing Lemonade Server on behalf of the user. This part of the guide includes steps for installing and running Lemonade Server so that your users don't have to install Lemonade Server separately.</p> <p>Definitions:</p> <ul> <li>Command line usage allows the server process to be launched programmatically, so that your application can manage starting and stopping the server process on your user's behalf.</li> <li>\"Silent installation\" refers to an automatic command for installing Lemonade Server without running any GUI or prompting the user for any questions. It does assume that the end-user fully accepts the license terms, so be sure that your own application makes this clear to the user.</li> </ul>"},{"location":"server/server_integration/#command-line-invocation","title":"Command Line Invocation","text":"<p>This command line invocation starts the Lemonade Server process so that your application can connect to it via REST API endpoints. To start the server, simply run the command below.</p> <pre><code>lemonade-server serve\n</code></pre> <p>By default, the server runs on port 8000. Optionally, you can specify a custom port using the --port argument:</p> <pre><code>lemonade-server serve --port 8123\n</code></pre> <p>You can also prevent the server from showing a system tray icon by using the <code>--no-tray</code> flag (Windows only):</p> <pre><code>lemonade-server serve --no-tray\n</code></pre> <p>You can also run the server as a background process using a subprocess or any preferred method.</p> <p>To stop the server, you may use the <code>lemonade-server stop</code> command, or simply terminate the process you created by keeping track of its PID. Please do not run the <code>lemonade-server stop</code> command if your application has not started the server, as the server may be used by other applications.</p>"},{"location":"server/server_integration/#windows-installation","title":"Windows Installation","text":"<p>Available Installers: - <code>lemonade-server-minimal.msi</code> - Server only (~3 MB) - <code>lemonade.msi</code> - Full installer with Electron desktop app (~105 MB)</p> <p>GUI Installation:</p> <p>Double-click the MSI file, or run:</p> <pre><code>msiexec /i lemonade.msi\n</code></pre> <p>MSI Properties:</p> <p>Properties can be passed on the command line to customize the installation:</p> <ul> <li><code>INSTALLDIR</code> - Custom installation directory (default: <code>%LOCALAPPDATA%\\lemonade_server</code>)</li> <li><code>ADDDESKTOPSHORTCUT</code> - Create desktop shortcut (0=no, 1=yes, default: 1)</li> <li><code>ALLUSERS</code> - Install for all users (1=yes, requires elevation; default: per-user)</li> </ul> <p>Examples:</p> <pre><code># Custom install directory\nmsiexec /i lemonade.msi INSTALLDIR=\"C:\\Custom\\Path\"\n\n# Without desktop shortcut\nmsiexec /i lemonade.msi ADDDESKTOPSHORTCUT=0\n\n# Combined parameters\nmsiexec /i lemonade.msi INSTALLDIR=\"C:\\Custom\\Path\" ADDDESKTOPSHORTCUT=0\n</code></pre>"},{"location":"server/server_integration/#silent-installation","title":"Silent Installation","text":"<p>Add <code>/qn</code> to run without a GUI, automatically accepting all prompts:</p> <pre><code>msiexec /i lemonade.msi /qn\n</code></pre> <p>This can be combined with any MSI properties:</p> <pre><code>msiexec /i lemonade.msi /qn INSTALLDIR=\"C:\\Custom\\Path\" ADDDESKTOPSHORTCUT=0\n</code></pre>"},{"location":"server/server_integration/#all-users-installation","title":"All Users Installation","text":"<p>To install for all users (Program Files + system PATH), you must run from an Administrator command prompt.</p> <ol> <li>Open Command Prompt as Administrator (right-click \u2192 \"Run as administrator\")</li> <li>Run the install command:</li> </ol> <pre><code>msiexec /i lemonade.msi ALLUSERS=1 INSTALLDIR=\"C:\\Program Files (x86)\\Lemonade Server\"\n</code></pre> <p>For silent all-users installation, add <code>/qn</code>:</p> <pre><code>msiexec /i lemonade.msi /qn ALLUSERS=1 INSTALLDIR=\"C:\\Program Files (x86)\\Lemonade Server\"\n</code></pre> <p>Troubleshooting: - If installation fails silently, check that you're running as Administrator - Add <code>/L*V install.log</code> to generate a debug log file</p>"},{"location":"server/server_integration/#linux-installation","title":"Linux Installation","text":""},{"location":"server/server_integration/#debianubuntu-package","title":"Debian/Ubuntu Package","text":"<p>The Debian package installer handles all system configuration automatically, including setting up a systemd service for managing the Lemonade Server.</p> <p>If you would prefer to manage the lifecycle of the server process manually, the service can be disabled and manually run as well.</p>"},{"location":"server/server_integration/#systemd-service-management","title":"Systemd Service Management","text":"<p>When Lemonade Server is installed via the Debian package, it registers a systemd service called <code>lemonade-server</code> that allows you to manage the server process using standard systemd commands.</p> <p>Service Features:</p> <ul> <li>Automatic restart: The service automatically restarts if the server crashes</li> <li>User isolation: Runs under the unprivileged <code>lemonade</code> user for security</li> <li>GPU access: The service is configured with proper group membership to access GPU/NPU hardware via the <code>render</code> group</li> <li>Security hardening: Includes systemd security options like <code>ProtectSystem=full</code>, <code>ProtectHome=yes</code>, and <code>NoNewPrivileges=yes</code></li> <li>Boot integration: Starts automatically on system boot (if enabled)</li> </ul> <p>Common Commands:</p> <pre><code># Start the service\nsudo systemctl start lemonade-server\n\n# Stop the service\nsudo systemctl stop lemonade-server\n\n# Restart the service\nsudo systemctl restart lemonade-server\n\n# Check service status\nsudo systemctl status lemonade-server\n\n# Enable automatic startup on boot\nsudo systemctl enable lemonade-server\n\n# Disable automatic startup on boot\nsudo systemctl disable lemonade-server\n\n# View service logs\nsudo journalctl -u lemonade-server\n\n# View recent logs (follow mode)\nsudo journalctl -u lemonade-server -f\n</code></pre> <p>Configuration:</p> <p>The Lemonade Server systemd service is configured to read settings from <code>/etc/lemonade/lemonade.conf</code>. Environment variables defined in this file are passed to the server process. Edit this file to customize server behavior:</p> <pre><code>sudo nano /etc/lemonade/lemonade.conf\n</code></pre> <p>After making changes to the configuration file, restart the service for changes to take effect:</p> <pre><code>sudo systemctl restart lemonade-server\n</code></pre> <p>Service File Location:</p> <p>The systemd service file is located at <code>/etc/systemd/system/lemonade-server.service</code>. This file should not be edited directly as it may be overwritten during package updates. Instead, use the configuration file (<code>/etc/lemonade/lemonade.conf</code>) to customize server behavior.</p> <p>If you need to make persistent changes to the service file, use systemd's drop-in override mechanism:</p> <pre><code>sudo systemctl edit lemonade-server\n</code></pre> <p>This creates an override file that takes precedence over the original service file and persists across updates.</p>"},{"location":"server/server_models/","title":"\ud83c\udf4b Lemonade Server Models","text":"<p>This document provides the models we recommend for use with Lemonade Server.</p> <p>Click on any model to learn more details about it, such as the Lemonade Recipe used to load the model. Content:</p> <ul> <li>Model Management GUI</li> <li>Supported Models</li> <li>Naming Convention</li> <li>Model Storage and Management</li> <li>Installing Additional Models</li> </ul>"},{"location":"server/server_models/#model-management-gui","title":"Model Management GUI","text":"<p>Lemonade Server offers a model management GUI to help you see which models are available, install new models, and delete models. You can access this GUI by starting Lemonade Server, opening http://localhost:8000 in your web browser, and clicking the Model Management tab.</p>"},{"location":"server/server_models/#supported-models","title":"Supported Models","text":""},{"location":"server/server_models/#hot-models","title":"\ud83d\udd25 Hot Models","text":"Qwen3-4B-Instruct-2507-GGUF <pre><code>lemonade-server pull Qwen3-4B-Instruct-2507-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-4B-Instruct-2507-GGUF GGUF VariantQwen3-4B-Instruct-2507-Q4_K_M.gguf Recipellamacpp Labelshot Size (GB)2.5 Qwen3-Coder-30B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Coder-30B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF GGUF VariantQwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf Recipellamacpp Labelscoding, tool-calling, hot Size (GB)18.6 Nemotron-3-Nano-30B-A3B-GGUF <pre><code>lemonade-server pull Nemotron-3-Nano-30B-A3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Nemotron-3-Nano-30B-A3B-GGUF GGUF VariantNemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)22.8 Gemma-3-4b-it-GGUF <pre><code>lemonade-server pull Gemma-3-4b-it-GGUF\n</code></pre> KeyValue Checkpointggml-org/gemma-3-4b-it-GGUF GGUF VariantQ4_K_M Mmprojmmproj-model-f16.gguf Recipellamacpp Labelshot, vision Size (GB)3.61 Qwen3-Next-80B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Next-80B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Next-80B-A3B-Instruct-GGUF GGUF VariantQwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)45.1 gpt-oss-120b-mxfp-GGUF <pre><code>lemonade-server pull gpt-oss-120b-mxfp-GGUF\n</code></pre> KeyValue Checkpointggml-org/gpt-oss-120b-GGUF GGUF Variant* Recipellamacpp Labelshot, reasoning, tool-calling Size (GB)63.3 gpt-oss-20b-mxfp4-GGUF <pre><code>lemonade-server pull gpt-oss-20b-mxfp4-GGUF\n</code></pre> KeyValue Checkpointggml-org/gpt-oss-20b-GGUF Recipellamacpp Labelshot, reasoning, tool-calling Size (GB)12.1 GLM-4.7-Flash-GGUF <pre><code>lemonade-server pull GLM-4.7-Flash-GGUF\n</code></pre> KeyValue Checkpointunsloth/GLM-4.7-Flash-GGUF GGUF VariantGLM-4.7-Flash-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)17.6 Gemma3-4b-it-FLM <pre><code>lemonade-server pull Gemma3-4b-it-FLM\n</code></pre> KeyValue Checkpointgemma3:4b Recipeflm Labelshot, vision Size (GB)5.26 Qwen3-4B-VL-FLM <pre><code>lemonade-server pull Qwen3-4B-VL-FLM\n</code></pre> KeyValue Checkpointqwen3vl-it:4b Recipeflm Labelshot, vision Size (GB)3.85 Whisper-Large-v3-Turbo <pre><code>lemonade-server pull Whisper-Large-v3-Turbo\n</code></pre> KeyValue Checkpointggerganov/whisper.cpp GGUF Variantggml-large-v3-turbo.bin Recipewhispercpp Labelsaudio, transcription, hot Size (GB)1.55"},{"location":"server/server_models/#gguf","title":"GGUF","text":"Qwen3-0.6B-GGUF <pre><code>lemonade-server pull Qwen3-0.6B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-0.6B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Size (GB)0.38 Qwen3-1.7B-GGUF <pre><code>lemonade-server pull Qwen3-1.7B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-1.7B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Size (GB)1.06 Qwen3-4B-GGUF <pre><code>lemonade-server pull Qwen3-4B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-4B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Size (GB)2.38 Qwen3-8B-GGUF <pre><code>lemonade-server pull Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning Size (GB)5.25 DeepSeek-Qwen3-8B-GGUF <pre><code>lemonade-server pull DeepSeek-Qwen3-8B-GGUF\n</code></pre> KeyValue Checkpointunsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF GGUF VariantQ4_1 Recipellamacpp Labelsreasoning Size (GB)5.25 Qwen3-14B-GGUF <pre><code>lemonade-server pull Qwen3-14B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-14B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Size (GB)8.54 Qwen3-4B-Instruct-2507-GGUF <pre><code>lemonade-server pull Qwen3-4B-Instruct-2507-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-4B-Instruct-2507-GGUF GGUF VariantQwen3-4B-Instruct-2507-Q4_K_M.gguf Recipellamacpp Labelshot Size (GB)2.5 Qwen3-30B-A3B-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-GGUF GGUF VariantQ4_0 Recipellamacpp Labelsreasoning Size (GB)17.4 Qwen3-30B-A3B-Instruct-2507-GGUF <pre><code>lemonade-server pull Qwen3-30B-A3B-Instruct-2507-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-30B-A3B-Instruct-2507-GGUF GGUF VariantQwen3-30B-A3B-Instruct-2507-Q4_0.gguf Recipellamacpp Size (GB)17.4 Qwen3-Coder-30B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Coder-30B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF GGUF VariantQwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf Recipellamacpp Labelscoding, tool-calling, hot Size (GB)18.6 Nemotron-3-Nano-30B-A3B-GGUF <pre><code>lemonade-server pull Nemotron-3-Nano-30B-A3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/Nemotron-3-Nano-30B-A3B-GGUF GGUF VariantNemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)22.8 Gemma-3-4b-it-GGUF <pre><code>lemonade-server pull Gemma-3-4b-it-GGUF\n</code></pre> KeyValue Checkpointggml-org/gemma-3-4b-it-GGUF GGUF VariantQ4_K_M Mmprojmmproj-model-f16.gguf Recipellamacpp Labelshot, vision Size (GB)3.61 Phi-4-mini-instruct-GGUF <pre><code>lemonade-server pull Phi-4-mini-instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Phi-4-mini-instruct-GGUF GGUF VariantPhi-4-mini-instruct-Q4_K_M.gguf Recipellamacpp Size (GB)2.49 LFM2-1.2B-GGUF <pre><code>lemonade-server pull LFM2-1.2B-GGUF\n</code></pre> KeyValue CheckpointLiquidAI/LFM2-1.2B-GGUF GGUF VariantLFM2-1.2B-Q4_K_M.gguf Recipellamacpp Size (GB)0.731 Jan-nano-128k-GGUF <pre><code>lemonade-server pull Jan-nano-128k-GGUF\n</code></pre> KeyValue CheckpointMenlo/Jan-nano-128k-gguf GGUF Variantjan-nano-128k-Q4_K_M.gguf Recipellamacpp Size (GB)2.5 Jan-v1-4B-GGUF <pre><code>lemonade-server pull Jan-v1-4B-GGUF\n</code></pre> KeyValue Checkpointjanhq/Jan-v1-4B-GGUF GGUF VariantJan-v1-4B-Q4_K_M.gguf Recipellamacpp Size (GB)2.5 Llama-3.2-1B-Instruct-GGUF <pre><code>lemonade-server pull Llama-3.2-1B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Llama-3.2-1B-Instruct-GGUF GGUF VariantLlama-3.2-1B-Instruct-UD-Q4_K_XL.gguf Recipellamacpp Size (GB)0.834 Llama-3.2-3B-Instruct-GGUF <pre><code>lemonade-server pull Llama-3.2-3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Llama-3.2-3B-Instruct-GGUF GGUF VariantLlama-3.2-3B-Instruct-UD-Q4_K_XL.gguf Recipellamacpp Size (GB)2.06 SmolLM3-3B-GGUF <pre><code>lemonade-server pull SmolLM3-3B-GGUF\n</code></pre> KeyValue Checkpointunsloth/SmolLM3-3B-128K-GGUF GGUF VariantSmolLM3-3B-128K-UD-Q4_K_XL.gguf Recipellamacpp Size (GB)1.94 Ministral-3-3B-Instruct-2512-GGUF <pre><code>lemonade-server pull Ministral-3-3B-Instruct-2512-GGUF\n</code></pre> KeyValue Checkpointmistralai/Ministral-3-3B-Instruct-2512-GGUF GGUF VariantMinistral-3-3B-Instruct-2512-Q4_K_M.gguf MmprojMinistral-3-3B-Instruct-2512-BF16-mmproj.gguf Recipellamacpp Labelsvision Size (GB)2.85 Qwen2.5-VL-7B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-VL-7B-Instruct-GGUF\n</code></pre> KeyValue Checkpointggml-org/Qwen2.5-VL-7B-Instruct-GGUF GGUF VariantQ4_K_M Mmprojmmproj-Qwen2.5-VL-7B-Instruct-f16.gguf Recipellamacpp Labelsvision Size (GB)4.68 Qwen3-VL-4B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-VL-4B-Instruct-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen3-VL-4B-Instruct-GGUF GGUF VariantQ4_K_M Mmprojmmproj-Qwen3VL-4B-Instruct-F16.gguf Recipellamacpp Labelsvision Size (GB)3.33 Qwen3-VL-8B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-VL-8B-Instruct-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen3-VL-8B-Instruct-GGUF GGUF VariantQ4_K_M Mmprojmmproj-Qwen3VL-8B-Instruct-F16.gguf Recipellamacpp Labelsvision Size (GB)6.19 Qwen3-Next-80B-A3B-Instruct-GGUF <pre><code>lemonade-server pull Qwen3-Next-80B-A3B-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Qwen3-Next-80B-A3B-Instruct-GGUF GGUF VariantQwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)45.1 Llama-4-Scout-17B-16E-Instruct-GGUF <pre><code>lemonade-server pull Llama-4-Scout-17B-16E-Instruct-GGUF\n</code></pre> KeyValue Checkpointunsloth/Llama-4-Scout-17B-16E-Instruct-GGUF GGUF VariantQ4_K_S Mmprojmmproj-F16.gguf Recipellamacpp Labelsvision Size (GB)61.5 nomic-embed-text-v1-GGUF <pre><code>lemonade-server pull nomic-embed-text-v1-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v1-GGUF GGUF VariantQ4_K_S Recipellamacpp Labelsembeddings Size (GB)0.0781 nomic-embed-text-v2-moe-GGUF <pre><code>lemonade-server pull nomic-embed-text-v2-moe-GGUF\n</code></pre> KeyValue Checkpointnomic-ai/nomic-embed-text-v2-moe-GGUF GGUF VariantQ8_0 Recipellamacpp Labelsembeddings Size (GB)0.51 Qwen3-Embedding-0.6B-GGUF <pre><code>lemonade-server pull Qwen3-Embedding-0.6B-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen3-Embedding-0.6B-GGUF GGUF VariantQwen3-Embedding-0.6B-Q8_0.gguf Recipellamacpp Labelsembeddings Size (GB)0.64 Qwen3-Embedding-4B-GGUF <pre><code>lemonade-server pull Qwen3-Embedding-4B-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen3-Embedding-4B-GGUF GGUF VariantQwen3-Embedding-4B-Q8_0.gguf Recipellamacpp Labelsembeddings Size (GB)4.28 Qwen3-Embedding-8B-GGUF <pre><code>lemonade-server pull Qwen3-Embedding-8B-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen3-Embedding-8B-GGUF GGUF VariantQwen3-Embedding-8B-Q8_0.gguf Recipellamacpp Labelsembeddings Size (GB)8.05 bge-reranker-v2-m3-GGUF <pre><code>lemonade-server pull bge-reranker-v2-m3-GGUF\n</code></pre> KeyValue Checkpointpqnet/bge-reranker-v2-m3-Q8_0-GGUF Recipellamacpp Labelsreranking Size (GB)0.53 Devstral-Small-2507-GGUF <pre><code>lemonade-server pull Devstral-Small-2507-GGUF\n</code></pre> KeyValue Checkpointmistralai/Devstral-Small-2507_gguf GGUF VariantQ4_K_M Recipellamacpp Labelscoding, tool-calling Size (GB)14.3 Qwen2.5-Coder-32B-Instruct-GGUF <pre><code>lemonade-server pull Qwen2.5-Coder-32B-Instruct-GGUF\n</code></pre> KeyValue CheckpointQwen/Qwen2.5-Coder-32B-Instruct-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelscoding Size (GB)19.85 gpt-oss-120b-mxfp-GGUF <pre><code>lemonade-server pull gpt-oss-120b-mxfp-GGUF\n</code></pre> KeyValue Checkpointggml-org/gpt-oss-120b-GGUF GGUF Variant* Recipellamacpp Labelshot, reasoning, tool-calling Size (GB)63.3 gpt-oss-20b-mxfp4-GGUF <pre><code>lemonade-server pull gpt-oss-20b-mxfp4-GGUF\n</code></pre> KeyValue Checkpointggml-org/gpt-oss-20b-GGUF Recipellamacpp Labelshot, reasoning, tool-calling Size (GB)12.1 GLM-4.5-Air-UD-Q4K-XL-GGUF <pre><code>lemonade-server pull GLM-4.5-Air-UD-Q4K-XL-GGUF\n</code></pre> KeyValue Checkpointunsloth/GLM-4.5-Air-GGUF GGUF VariantUD-Q4_K_XL Recipellamacpp Labelsreasoning Size (GB)73.1 GLM-4.7-Flash-GGUF <pre><code>lemonade-server pull GLM-4.7-Flash-GGUF\n</code></pre> KeyValue Checkpointunsloth/GLM-4.7-Flash-GGUF GGUF VariantGLM-4.7-Flash-UD-Q4_K_XL.gguf Recipellamacpp Labelshot Size (GB)17.6 granite-4.0-h-tiny-GGUF <pre><code>lemonade-server pull granite-4.0-h-tiny-GGUF\n</code></pre> KeyValue Checkpointunsloth/granite-4.0-h-tiny-GGUF GGUF VariantQ4_K_M Recipellamacpp Labelstool-calling Size (GB)4.25 LFM2-8B-A1B-GGUF <pre><code>lemonade-server pull LFM2-8B-A1B-GGUF\n</code></pre> KeyValue CheckpointLiquidAI/LFM2-8B-A1B-GGUF GGUF VariantQ4_K_M Recipellamacpp Size (GB)4.8"},{"location":"server/server_models/#ryzen-ai-hybrid-npugpu","title":"Ryzen AI Hybrid (NPU+GPU)","text":"Llama-3.2-1B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-1B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-1B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)1.89 Llama-3.2-3B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.2-3B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Llama-3.2-3B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)4.28 Phi-3-Mini-Instruct-Hybrid <pre><code>lemonade-server pull Phi-3-Mini-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)4.18 Qwen-1.5-7B-Chat-Hybrid <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)8.83 Qwen-2.5-7B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-7B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-7B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)8.65 Qwen-2.5-3B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-3B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-3B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)3.97 Qwen-2.5-1.5B-Instruct-Hybrid <pre><code>lemonade-server pull Qwen-2.5-1.5B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen2.5-1.5B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)2.16 DeepSeek-R1-Distill-Llama-8B-Hybrid <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-Hybrid\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-onnx-ryzenai-hybrid Recipeoga-hybrid Labelsreasoning Size (GB)9.09 Mistral-7B-v0.3-Instruct-Hybrid <pre><code>lemonade-server pull Mistral-7B-v0.3-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Mistral-7B-Instruct-v0.3-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)7.85 Llama-3.1-8B-Instruct-Hybrid <pre><code>lemonade-server pull Llama-3.1-8B-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Meta-Llama-3.1-8B-Instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)9.09 Qwen3-1.7B-Hybrid <pre><code>lemonade-server pull Qwen3-1.7B-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen3-1.7B-awq-quant-onnx-hybrid Recipeoga-hybrid Labelsreasoning Size (GB)2.55 Phi-4-Mini-Instruct-Hybrid <pre><code>lemonade-server pull Phi-4-Mini-Instruct-Hybrid\n</code></pre> KeyValue Checkpointamd/Phi-4-mini-instruct-onnx-ryzenai-hybrid Recipeoga-hybrid Size (GB)5.46 Qwen3-4B-Hybrid <pre><code>lemonade-server pull Qwen3-4B-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen3-4B-awq-quant-onnx-hybrid Recipeoga-hybrid Labelsreasoning Size (GB)5.17 Qwen3-8B-Hybrid <pre><code>lemonade-server pull Qwen3-8B-Hybrid\n</code></pre> KeyValue Checkpointamd/Qwen3-8B-awq-quant-onnx-hybrid Recipeoga-hybrid Labelsreasoning Size (GB)9.42"},{"location":"server/server_models/#ryzen-ai-npu","title":"Ryzen AI NPU","text":"Qwen-2.5-7B-Instruct-NPU <pre><code>lemonade-server pull Qwen-2.5-7B-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-7B-Instruct-onnx-ryzenai-npu Recipeoga-npu Size (GB)8.82 Qwen-2.5-3B-Instruct-NPU <pre><code>lemonade-server pull Qwen-2.5-3B-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-3B-Instruct-onnx-ryzenai-npu Recipeoga-npu Size (GB)4.09 DeepSeek-R1-Distill-Llama-8B-NPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-NPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-onnx-ryzenai-npu Recipeoga-npu Size (GB)9.3 Mistral-7B-v0.3-Instruct-NPU <pre><code>lemonade-server pull Mistral-7B-v0.3-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Mistral-7B-Instruct-v0.3-onnx-ryzenai-npu Recipeoga-npu Size (GB)8.09 Phi-3.5-Mini-Instruct-NPU <pre><code>lemonade-server pull Phi-3.5-Mini-Instruct-NPU\n</code></pre> KeyValue Checkpointamd/Phi-3.5-mini-instruct-onnx-ryzenai-npu Recipeoga-npu Size (GB)4.35"},{"location":"server/server_models/#fastflowlm-npu","title":"FastFlowLM (NPU)","text":"gpt-oss-20b-FLM <pre><code>lemonade-server pull gpt-oss-20b-FLM\n</code></pre> KeyValue Checkpointgpt-oss:20b Recipeflm Labelsreasoning Size (GB)13.4 Gemma3-1b-it-FLM <pre><code>lemonade-server pull Gemma3-1b-it-FLM\n</code></pre> KeyValue Checkpointgemma3:1b Recipeflm Size (GB)1.17 Gemma3-4b-it-FLM <pre><code>lemonade-server pull Gemma3-4b-it-FLM\n</code></pre> KeyValue Checkpointgemma3:4b Recipeflm Labelshot, vision Size (GB)5.26 Qwen3-4B-VL-FLM <pre><code>lemonade-server pull Qwen3-4B-VL-FLM\n</code></pre> KeyValue Checkpointqwen3vl-it:4b Recipeflm Labelshot, vision Size (GB)3.85 Qwen3-0.6b-FLM <pre><code>lemonade-server pull Qwen3-0.6b-FLM\n</code></pre> KeyValue Checkpointqwen3:0.6b Recipeflm Labelsreasoning Size (GB)0.66 Qwen3-4B-Instruct-2507-FLM <pre><code>lemonade-server pull Qwen3-4B-Instruct-2507-FLM\n</code></pre> KeyValue Checkpointqwen3-it:4b Recipeflm Size (GB)3.07 Qwen3-8b-FLM <pre><code>lemonade-server pull Qwen3-8b-FLM\n</code></pre> KeyValue Checkpointqwen3:8b Recipeflm Labelsreasoning Size (GB)5.57 Llama-3.1-8B-FLM <pre><code>lemonade-server pull Llama-3.1-8B-FLM\n</code></pre> KeyValue Checkpointllama3.1:8b Recipeflm Size (GB)5.36 Llama-3.2-1B-FLM <pre><code>lemonade-server pull Llama-3.2-1B-FLM\n</code></pre> KeyValue Checkpointllama3.2:1b Recipeflm Size (GB)1.21 Llama-3.2-3B-FLM <pre><code>lemonade-server pull Llama-3.2-3B-FLM\n</code></pre> KeyValue Checkpointllama3.2:3b Recipeflm Size (GB)2.62 LFM2-1.2B-FLM <pre><code>lemonade-server pull LFM2-1.2B-FLM\n</code></pre> KeyValue Checkpointlfm2:1.2b Recipeflm Size (GB)0.96 LFM2.5-1.2B-Instruct-FLM <pre><code>lemonade-server pull LFM2.5-1.2B-Instruct-FLM\n</code></pre> KeyValue Checkpointlfm2.5-it:1.2b Recipeflm Size (GB)0.96 Phi-4-Mini-Instruct-FLM <pre><code>lemonade-server pull Phi-4-Mini-Instruct-FLM\n</code></pre> KeyValue Checkpointphi4-mini-it:4b Recipeflm Size (GB)3.39 DeepSeek-R1-Distill-Llama-8B-FLM <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-FLM\n</code></pre> KeyValue Checkpointdeepseek-r1:8b Recipeflm Labelsreasoning Size (GB)5.36 DeepSeek-R1-0528-Qwen3-8B-FLM <pre><code>lemonade-server pull DeepSeek-R1-0528-Qwen3-8B-FLM\n</code></pre> KeyValue Checkpointdeepseek-r1-0528:8b Recipeflm Labelsreasoning Size (GB)5.57 LFM2-2.6B-FLM <pre><code>lemonade-server pull LFM2-2.6B-FLM\n</code></pre> KeyValue Checkpointlfm2:2.6b Recipeflm Size (GB)1.75 Qwen3-1.7b-FLM <pre><code>lemonade-server pull Qwen3-1.7b-FLM\n</code></pre> KeyValue Checkpointqwen3:1.7b Recipeflm Labelsreasoning Size (GB)1.59 LFM2.5-1.2B-Thinking-FLM <pre><code>lemonade-server pull LFM2.5-1.2B-Thinking-FLM\n</code></pre> KeyValue Checkpointlfm2.5-tk:1.2b Recipeflm Labelsreasoning Size (GB)0.96"},{"location":"server/server_models/#image-generation-stable-diffusion-cpp","title":"Image Generation (Stable Diffusion CPP)","text":"<p>Image generation models use the stable-diffusion.cpp backend. Each model includes <code>image_defaults</code> that specify recommended generation parameters (steps, cfg_scale, width, height).</p> SD-Turbo <pre><code>lemonade-server pull SD-Turbo\n</code></pre> KeyValue Checkpointstabilityai/sd-turbo Recipesd-cpp Labelsimage Size (GB)5.2 Default Steps4 Default CFG Scale1.0 Default Size512x512 SDXL-Turbo <pre><code>lemonade-server pull SDXL-Turbo\n</code></pre> KeyValue Checkpointstabilityai/sdxl-turbo Recipesd-cpp Labelsimage Size (GB)13.9 Default Steps4 Default CFG Scale1.0 Default Size512x512 SD-1.5 <pre><code>lemonade-server pull SD-1.5\n</code></pre> KeyValue Checkpointstable-diffusion-v1-5/stable-diffusion-v1-5 Recipesd-cpp Labelsimage Size (GB)5.2 Default Steps20 Default CFG Scale7.5 Default Size512x512 SDXL-Base-1.0 <pre><code>lemonade-server pull SDXL-Base-1.0\n</code></pre> KeyValue Checkpointstabilityai/stable-diffusion-xl-base-1.0 Recipesd-cpp Labelsimage Size (GB)13.9 Default Steps20 Default CFG Scale7.5 Default Size1024x1024"},{"location":"server/server_models/#cpu","title":"CPU","text":"Qwen2.5-0.5B-Instruct-CPU <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx Recipeoga-cpu Size (GB)0.77 Phi-3-Mini-Instruct-CPU <pre><code>lemonade-server pull Phi-3-Mini-Instruct-CPU\n</code></pre> KeyValue Checkpointamd/Phi-3-mini-4k-instruct_int4_float16_onnx_cpu Recipeoga-cpu Size (GB)2.23 Qwen-1.5-7B-Chat-CPU <pre><code>lemonade-server pull Qwen-1.5-7B-Chat-CPU\n</code></pre> KeyValue Checkpointamd/Qwen1.5-7B-Chat_uint4_asym_g128_float16_onnx_cpu Recipeoga-cpu Size (GB)5.89 DeepSeek-R1-Distill-Llama-8B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Llama-8B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning Size (GB)5.78 DeepSeek-R1-Distill-Qwen-7B-CPU <pre><code>lemonade-server pull DeepSeek-R1-Distill-Qwen-7B-CPU\n</code></pre> KeyValue Checkpointamd/DeepSeek-R1-Distill-Llama-8B-awq-asym-uint4-g128-lmhead-onnx-cpu Recipeoga-cpu Labelsreasoning Size (GB)5.78"},{"location":"server/server_models/#naming-convention","title":"Naming Convention","text":"<p>The format of each Lemonade name is a combination of the name in the base checkpoint and the backend where the model will run. So, if the base checkpoint is <code>meta-llama/Llama-3.2-1B-Instruct</code>, and it has been optimized to run on Hybrid, the resulting name is <code>Llama-3.2-3B-Instruct-Hybrid</code>.</p>"},{"location":"server/server_models/#model-storage-and-management","title":"Model Storage and Management","text":"<p>Lemonade Server relies on Hugging Face Hub to manage downloading and storing models on your system. By default, Hugging Face Hub downloads models to <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub</code>.</p> <p>For example, the Lemonade Server <code>Llama-3.2-3B-Instruct-Hybrid</code> model will end up at <code>C:\\Users\\YOUR_USERNAME\\.cache\\huggingface\\hub\\models--amd--Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid</code>. If you want to uninstall that model, simply delete that folder.</p> <p>You can change the directory for Hugging Face Hub by setting the <code>HF_HOME</code> or <code>HF_HUB_CACHE</code> environment variables.</p>"},{"location":"server/server_models/#installing-additional-models","title":"Installing Additional Models","text":"<p>Once you've installed Lemonade Server, you can install any model on this list using the <code>pull</code> command in the <code>lemonade-server</code> CLI.</p> <p>Example:</p> <pre><code>lemonade-server pull Qwen2.5-0.5B-Instruct-CPU\n</code></pre> <p>Note: <code>lemonade-server</code> is a utility that is added to your PATH when you install Lemonade Server.</p>"},{"location":"server/server_spec/","title":"Lemonade Server Spec","text":"<p>The Lemonade Server is a standards-compliant server process that provides an HTTP API to enable integration with other applications.</p> <p>Lemonade Server currently supports these backends:</p> Backend Model Format Description Llama.cpp <code>.GGUF</code> Uses llama.cpp's <code>llama-server</code> backend. More details here. ONNX Runtime GenAI (OGA) <code>.ONNX</code> Uses Lemonade's own <code>ryzenai-server</code> backend. FastFlowLM <code>.q4nx</code> Uses FLM's <code>flm serve</code> backend. More details here. whisper.cpp <code>.bin</code> Uses whisper.cpp's <code>whisper-server</code> backend for audio transcription. Models: Whisper-Tiny, Whisper-Base, Whisper-Small. stable-diffusion.cpp <code>.safetensors</code> Uses sd.cpp's <code>sd-cli</code> backend for image generation. Models: SD-Turbo, SDXL-Turbo, etc. Kokoros <code>.onnx</code> Uses Kokoro's <code>koko</code> backend for speech generation. Models: kokoro-v1"},{"location":"server/server_spec/#endpoints-overview","title":"Endpoints Overview","text":"<p>The key endpoints of the OpenAI API are available.</p> <p>We are also actively investigating and developing additional endpoints that will improve the experience of local applications.</p>"},{"location":"server/server_spec/#openai-compatible-endpoints","title":"OpenAI-Compatible Endpoints","text":"<ul> <li>POST <code>/api/v1/chat/completions</code> - Chat Completions (messages -&gt; completion)</li> <li>POST <code>/api/v1/completions</code> - Text Completions (prompt -&gt; completion)</li> <li>POST <code>/api/v1/embeddings</code> - Embeddings (text -&gt; vector representations)</li> <li>POST <code>/api/v1/responses</code> - Chat Completions (prompt|messages -&gt; event)</li> <li>POST <code>/api/v1/audio/transcriptions</code> - Audio Transcription (audio -&gt; text)</li> <li>POST <code>/api/v1/audio/speech</code> - Text to speech (text -&gt; audio)</li> <li>POST <code>/api/v1/images/generations</code> - Image Generation (prompt -&gt; image)</li> <li>GET <code>/api/v1/models</code> - List models available locally</li> <li>GET <code>/api/v1/models/{model_id}</code> - Retrieve a specific model by ID</li> </ul>"},{"location":"server/server_spec/#llamacpp-endpoints","title":"llama.cpp Endpoints","text":"<p>These endpoints defined by <code>llama.cpp</code> extend the OpenAI-compatible API with additional functionality.</p> <ul> <li>POST <code>/api/v1/reranking</code> - Reranking (query + documents -&gt; relevance-scored documents)</li> </ul>"},{"location":"server/server_spec/#lemonade-specific-endpoints","title":"Lemonade-Specific Endpoints","text":"<p>We have designed a set of Lemonade-specific endpoints to enable client applications by extending the existing cloud-focused APIs (e.g., OpenAI). These extensions allow for a greater degree of UI/UX responsiveness in native applications by allowing applications to:</p> <ul> <li>Download models at setup time.</li> <li>Pre-load models at UI-loading-time, as opposed to completion-request time.</li> <li>Unload models to save memory space.</li> <li>Understand system resources and state to make dynamic choices.</li> </ul> <p>The additional endpoints are:</p> <ul> <li>POST <code>/api/v1/pull</code> - Install a model</li> <li>POST <code>/api/v1/delete</code> - Delete a model</li> <li>POST <code>/api/v1/load</code> - Load a model</li> <li>POST <code>/api/v1/unload</code> - Unload a model</li> <li>GET <code>/api/v1/health</code> - Check server status, such as models loaded</li> <li>GET <code>/api/v1/stats</code> - Performance statistics from the last request</li> <li>GET <code>/api/v1/system-info</code> - System information and device enumeration</li> <li>GET <code>/live</code> - Check server liveness for load balancers and orchestrators</li> </ul>"},{"location":"server/server_spec/#multi-model-support","title":"Multi-Model Support","text":"<p>Lemonade Server supports loading multiple models simultaneously, allowing you to keep frequently-used models in memory for faster switching. The server uses a Least Recently Used (LRU) cache policy to automatically manage model eviction when limits are reached.</p>"},{"location":"server/server_spec/#configuration","title":"Configuration","text":"<p>Use the <code>--max-loaded-models</code> option to specify how many models to keep loaded:</p> <pre><code># Load up to 3 LLMs, 2 embedding models, 1 reranking model, and 1 audio model\nlemonade-server serve --max-loaded-models 3 2 1 1\n\n# Load up to 5 LLMs (embeddings, reranking, and audio default to 1 each)\nlemonade-server serve --max-loaded-models 5\n</code></pre> <p>Default: <code>1 1 1 1</code> (one model of each type)</p>"},{"location":"server/server_spec/#model-types","title":"Model Types","text":"<p>Models are categorized into these types: - LLM - Chat and completion models (default type) - Embedding - Models for generating text embeddings (identified by the <code>embeddings</code> label) - Reranking - Models for document reranking (identified by the <code>reranking</code> label) - Audio - Models for audio transcription using Whisper (identified by the <code>audio</code> label)</p> <p>Each type has its own independent limit and LRU cache.</p>"},{"location":"server/server_spec/#device-constraints","title":"Device Constraints","text":"<ul> <li>NPU Exclusivity: Only one model can use the NPU at a time. Loading a new NPU model will evict any existing NPU model regardless of type or limits.</li> <li>CPU/GPU: No inherent limits beyond available RAM. Multiple models can coexist on CPU or GPU.</li> </ul>"},{"location":"server/server_spec/#eviction-policy","title":"Eviction Policy","text":"<p>When a model slot is full: 1. The least recently used model of that type is evicted 2. The new model is loaded 3. If loading fails (except file-not-found errors), all models are evicted and the load is retried</p> <p>Models currently processing inference requests cannot be evicted until they finish.</p>"},{"location":"server/server_spec/#per-model-settings","title":"Per-Model Settings","text":"<p>Each model can be loaded with custom settings (context size, llamacpp backend, llamacpp args) via the <code>/api/v1/load</code> endpoint. These per-model settings override the default values set via CLI arguments or environment variables. See the <code>/api/v1/load</code> endpoint documentation for details.</p> <p>Setting Priority Order: 1. Values passed explicitly in <code>/api/v1/load</code> request (highest priority) 2. Values from <code>lemonade-server</code> CLI arguments or environment variables 3. Hardcoded defaults in <code>lemonade-router</code> (lowest priority)</p>"},{"location":"server/server_spec/#start-the-http-server","title":"Start the HTTP Server","text":"<p>NOTE: This server is intended for use on local systems only. Do not expose the server port to the open internet.</p> <p>See the Lemonade Server getting started instructions.</p> <pre><code>lemonade-server serve\n</code></pre>"},{"location":"server/server_spec/#openai-compatible-endpoints_1","title":"OpenAI-Compatible Endpoints","text":""},{"location":"server/server_spec/#post-apiv1chatcompletions","title":"<code>POST /api/v1/chat/completions</code>","text":"<p>Chat Completions API. You provide a list of messages and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters","title":"Parameters","text":"Parameter Required Description Status <code>messages</code> Yes Array of messages in the conversation. Each message should have a <code>role</code> (\"user\" or \"assistant\") and <code>content</code> (the message text). <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>tools</code> No A list of tools the model may call. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_completion_tokens</code>. This value is now deprecated by OpenAI in favor of <code>max_completion_tokens</code> <code>max_completion_tokens</code> No An upper bound for the number of tokens that can be generated for a completion. Mutually exclusive with <code>max_tokens</code>."},{"location":"server/server_spec/#example-request","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest `\n  -Uri \"http://localhost:8000/api/v1/chat/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Qwen3-0.6B-GGUF\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"What is the population of Paris?\"\n      }\n    ],\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Qwen3-0.6B-GGUF\",\n        \"messages\": [\n          {\"role\": \"user\", \"content\": \"What is the population of Paris?\"}\n        ],\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format","title":"Response format","text":"Non-streaming responsesStreaming responses <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion\",\n  \"created\": 1742927481,\n  \"model\": \"Qwen3-0.6B-GGUF\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    },\n    \"finish_reason\": \"stop\"\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a stream of server-sent events (however, Open AI recommends using their streaming libraries for parsing streaming responses):</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"chat.completion.chunk\",\n  \"created\": 1742927481,\n  \"model\": \"Qwen3-0.6B-GGUF\",\n  \"choices\": [{\n    \"index\": 0,\n    \"delta\": {\n      \"role\": \"assistant\",\n      \"content\": \"Paris\"\n    }\n  }]\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1completions","title":"<code>POST /api/v1/completions</code>","text":"<p>Text Completions API. You provide a prompt and receive a completion. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_1","title":"Parameters","text":"Parameter Required Description Status <code>prompt</code> Yes The prompt to use for the completion. <code>model</code> Yes The model to use for the completion. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false. <code>stop</code> No Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Can be a string or an array of strings. <code>echo</code> No Echo back the prompt in addition to the completion. Available on non-streaming mode. <code>logprobs</code> No Include log probabilities of the output tokens. If true, returns the log probability of each output token. Defaults to false. Only available when <code>stream=False</code>. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>max_tokens</code> No An upper bound for the number of tokens that can be generated for a completion, including input tokens."},{"location":"server/server_spec/#example-request_1","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/completions\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Qwen3-0.6B-GGUF\",\n    \"prompt\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Qwen3-0.6B-GGUF\",\n        \"prompt\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_1","title":"Response format","text":"<p>The following format is used for both streaming and non-streaming responses:</p> <pre><code>{\n  \"id\": \"0\",\n  \"object\": \"text_completion\",\n  \"created\": 1742927481,\n  \"model\": \"Qwen3-0.6B-GGUF\",\n  \"choices\": [{\n    \"index\": 0,\n    \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\",\n    \"finish_reason\": \"stop\"\n  }],\n}\n</code></pre>"},{"location":"server/server_spec/#post-apiv1embeddings","title":"<code>POST /api/v1/embeddings</code>","text":"<p>Embeddings API. You provide input text and receive vector representations (embeddings) that can be used for semantic search, clustering, and similarity comparisons. This API will also load the model if it is not already loaded.</p> <p>Note: This endpoint is only available for models using the <code>llamacpp</code> or <code>flm</code> recipes. ONNX models (OGA recipes) do not support embeddings.</p>"},{"location":"server/server_spec/#parameters_2","title":"Parameters","text":"Parameter Required Description Status <code>input</code> Yes The input text or array of texts to embed. Can be a string or an array of strings. <code>model</code> Yes The model to use for generating embeddings. <code>encoding_format</code> No The format to return embeddings in. Supported values: <code>\"float\"</code> (default), <code>\"base64\"</code>."},{"location":"server/server_spec/#example-request_2","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest `\n  -Uri \"http://localhost:8000/api/v1/embeddings\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"nomic-embed-text-v1-GGUF\",\n    \"input\": [\"Hello, world!\", \"How are you?\"],\n    \"encoding_format\": \"float\"\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"nomic-embed-text-v1-GGUF\",\n        \"input\": [\"Hello, world!\", \"How are you?\"],\n        \"encoding_format\": \"float\"\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_2","title":"Response format","text":"<pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0234, -0.0567, 0.0891, ...]\n    },\n    {\n      \"object\": \"embedding\",\n      \"index\": 1,\n      \"embedding\": [0.0456, -0.0678, 0.1234, ...]\n    }\n  ],\n  \"model\": \"nomic-embed-text-v1-GGUF\",\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"total_tokens\": 12\n  }\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>object</code> - Type of response object, always <code>\"list\"</code></li> <li><code>data</code> - Array of embedding objects</li> <li><code>object</code> - Type of embedding object, always <code>\"embedding\"</code></li> <li><code>index</code> - Index position of the input text in the request</li> <li><code>embedding</code> - Vector representation as an array of floats</li> <li><code>model</code> - Model identifier used to generate the embeddings</li> <li><code>usage</code> - Token usage statistics</li> <li><code>prompt_tokens</code> - Number of tokens in the input</li> <li><code>total_tokens</code> - Total tokens processed</li> </ul>"},{"location":"server/server_spec/#post-apiv1reranking","title":"<code>POST /api/v1/reranking</code>","text":"<p>Reranking API. You provide a query and a list of documents, and receive the documents reordered by their relevance to the query with relevance scores. This is useful for improving search results quality. This API will also load the model if it is not already loaded.</p> <p>Note: This endpoint follows API conventions similar to OpenAI's format but is not part of the official OpenAI API. It is inspired by llama.cpp and other inference server implementations.</p> <p>Note: This endpoint is only available for models using the <code>llamacpp</code> recipe. It is not available for FLM or ONNX models.</p>"},{"location":"server/server_spec/#parameters_3","title":"Parameters","text":"Parameter Required Description Status <code>query</code> Yes The search query text. <code>documents</code> Yes Array of document strings to be reranked. <code>model</code> Yes The model to use for reranking."},{"location":"server/server_spec/#example-request_3","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest `\n  -Uri \"http://localhost:8000/api/v1/reranking\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"bge-reranker-v2-m3-GGUF\",\n    \"query\": \"What is the capital of France?\",\n    \"documents\": [\n      \"Paris is the capital of France.\",\n      \"Berlin is the capital of Germany.\",\n      \"Madrid is the capital of Spain.\"\n    ]\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/reranking \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"bge-reranker-v2-m3-GGUF\",\n        \"query\": \"What is the capital of France?\",\n        \"documents\": [\n          \"Paris is the capital of France.\",\n          \"Berlin is the capital of Germany.\",\n          \"Madrid is the capital of Spain.\"\n        ]\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_3","title":"Response format","text":"<pre><code>{\n  \"model\": \"bge-reranker-v2-m3-GGUF\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 8.60673713684082\n    },\n    {\n      \"index\": 1,\n      \"relevance_score\": -5.3886260986328125\n    },\n    {\n      \"index\": 2,\n      \"relevance_score\": -3.555561065673828\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 51,\n    \"total_tokens\": 51\n  }\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>model</code> - Model identifier used for reranking</li> <li><code>object</code> - Type of response object, always <code>\"list\"</code></li> <li><code>results</code> - Array of all documents with relevance scores</li> <li><code>index</code> - Original index of the document in the input array</li> <li><code>relevance_score</code> - Relevance score assigned by the model (higher = more relevant)</li> <li><code>usage</code> - Token usage statistics</li> <li><code>prompt_tokens</code> - Number of tokens in the input</li> <li><code>total_tokens</code> - Total tokens processed</li> </ul> <p>Note: The results are returned in their original input order, not sorted by relevance score. To get documents ranked by relevance, you need to sort the results by <code>relevance_score</code> in descending order on the client side.</p>"},{"location":"server/server_spec/#post-apiv1responses","title":"<code>POST /api/v1/responses</code>","text":"<p>Responses API. You provide an input and receive a response. This API will also load the model if it is not already loaded.</p>"},{"location":"server/server_spec/#parameters_4","title":"Parameters","text":"Parameter Required Description Status <code>input</code> Yes A list of dictionaries or a string input for the model to respond to. <code>model</code> Yes The model to use for the response. <code>max_output_tokens</code> No The maximum number of output tokens to generate. <code>temperature</code> No What sampling temperature to use. <code>repeat_penalty</code> No Number between 1.0 and 2.0. 1.0 means no penalty. Higher values discourage repetition. <code>top_k</code> No Integer that controls the number of top tokens to consider during sampling. <code>top_p</code> No Float between 0.0 and 1.0 that controls the cumulative probability of top tokens to consider during nucleus sampling. <code>stream</code> No If true, tokens will be sent as they are generated. If false, the response will be sent as a single message once complete. Defaults to false."},{"location":"server/server_spec/#streaming-events","title":"Streaming Events","text":"<p>The Responses API uses semantic events for streaming. Each event is typed with a predefined schema, so you can listen for events you care about. Our initial implementation only offers support to:</p> <ul> <li><code>response.created</code></li> <li><code>response.output_text.delta</code></li> <li><code>response.completed</code></li> </ul> <p>For a full list of event types, see the API reference for streaming.</p>"},{"location":"server/server_spec/#example-request_4","title":"Example request","text":"PowerShellBash <pre><code>Invoke-WebRequest -Uri \"http://localhost:8000/api/v1/responses\" `\n  -Method POST `\n  -Headers @{ \"Content-Type\" = \"application/json\" } `\n  -Body '{\n    \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n    \"input\": \"What is the population of Paris?\",\n    \"stream\": false\n  }'\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/responses \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n        \"input\": \"What is the population of Paris?\",\n        \"stream\": false\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_4","title":"Response format","text":"Non-streaming responsesStreaming Responses <pre><code>{\n  \"id\": \"0\",\n  \"created_at\": 1746225832.0,\n  \"model\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"object\": \"response\",\n  \"output\": [{\n    \"id\": \"0\",\n    \"content\": [{\n      \"annotations\": [],\n      \"text\": \"Paris has a population of approximately 2.2 million people in the city proper.\"\n    }]\n  }]\n}\n</code></pre> <p>For streaming responses, the API returns a series of events. Refer to OpenAI streaming guide for details.</p>"},{"location":"server/server_spec/#post-apiv1audiotranscriptions","title":"<code>POST /api/v1/audio/transcriptions</code>","text":"<p>Audio Transcription API. You provide an audio file and receive a text transcription. This API will also load the model if it is not already loaded.</p> <p>Note: This endpoint uses whisper.cpp as the backend. Whisper models are automatically downloaded when first used.</p> <p>Limitations: Only <code>wav</code> audio format and <code>json</code> response format are currently supported.</p>"},{"location":"server/server_spec/#parameters_5","title":"Parameters","text":"Parameter Required Description Status <code>file</code> Yes The audio file to transcribe. Supported formats: wav. <code>model</code> Yes The Whisper model to use for transcription (e.g., <code>Whisper-Tiny</code>, <code>Whisper-Base</code>, <code>Whisper-Small</code>). <code>language</code> No The language of the audio (ISO 639-1 code, e.g., <code>en</code>, <code>es</code>, <code>fr</code>). If not specified, Whisper will auto-detect the language. <code>response_format</code> No The format of the response. Currently only <code>json</code> is supported."},{"location":"server/server_spec/#example-request_5","title":"Example request","text":"WindowsLinux <pre><code>curl -X POST http://localhost:8000/api/v1/audio/transcriptions ^\n  -F \"file=@C:\\path\\to\\audio.wav\" ^\n  -F \"model=Whisper-Tiny\"\n</code></pre> <pre><code>curl -X POST http://localhost:8000/api/v1/audio/transcriptions \\\n  -F \"file=@/path/to/audio.wav\" \\\n  -F \"model=Whisper-Tiny\"\n</code></pre>"},{"location":"server/server_spec/#response-format_5","title":"Response format","text":"<pre><code>{\n  \"text\": \"Hello, this is a sample transcription of the audio file.\"\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>text</code> - The transcribed text from the audio file</li> </ul>"},{"location":"server/server_spec/#post-apiv1imagesgenerations","title":"<code>POST /api/v1/images/generations</code>","text":"<p>Image Generation API. You provide a text prompt and receive a generated image. This API uses stable-diffusion.cpp as the backend.</p> <p>Note: Image generation uses Stable Diffusion models. Available models include <code>SD-Turbo</code> (fast, ~4 steps), <code>SDXL-Turbo</code>, <code>SD-1.5</code>, and <code>SDXL-Base-1.0</code>.</p> <p>Performance: CPU inference takes ~4-5 minutes per image. GPU (Vulkan) is faster but may have compatibility issues with some hardware.</p>"},{"location":"server/server_spec/#parameters_6","title":"Parameters","text":"Parameter Required Description Status <code>prompt</code> Yes The text description of the image to generate. <code>model</code> Yes The Stable Diffusion model to use (e.g., <code>SD-Turbo</code>, <code>SDXL-Turbo</code>). <code>size</code> No The size of the generated image. Format: <code>WIDTHxHEIGHT</code> (e.g., <code>512x512</code>, <code>256x256</code>). Default: <code>512x512</code>. <code>n</code> No Number of images to generate. Currently only <code>1</code> is supported. <code>response_format</code> No Format of the response. Only <code>b64_json</code> (base64-encoded image) is supported. <code>steps</code> No Number of inference steps. SD-Turbo works well with 4 steps. Default varies by model. <code>cfg_scale</code> No Classifier-free guidance scale. SD-Turbo uses low values (~1.0). Default varies by model. <code>seed</code> No Random seed for reproducibility. If not specified, a random seed is used."},{"location":"server/server_spec/#example-request_6","title":"Example request","text":"Bash <pre><code>curl -X POST http://localhost:8000/api/v1/images/generations \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"SD-Turbo\",\n        \"prompt\": \"A serene mountain landscape at sunset\",\n        \"size\": \"512x512\",\n        \"steps\": 4,\n        \"response_format\": \"b64_json\"\n      }'\n</code></pre>"},{"location":"server/server_spec/#post-apiv1audiospeech","title":"<code>POST /api/v1/audio/speech</code>","text":"<p>Speech Generation API. You provide a text input and receive an audio file. This API uses Kokoros as the backend.</p> <p>Note: The model to use is called <code>kokoro-v1</code>. No other model is supported at the moment.</p> <p>Limitations: Only <code>mp3</code>, <code>wav</code>, <code>opus</code>, and <code>pcm</code> are supported. Streaming is supported in <code>audio</code> (<code>pcm</code>) mode.</p>"},{"location":"server/server_spec/#parameters_7","title":"Parameters","text":"Parameter Required Description Status <code>input</code> Yes The text to speak. <code>model</code> Yes The model to use (e.g., <code>kokoro-v1</code>). <code>speed</code> No Speaking speed. Default: <code>1.0</code>. <code>voice</code> No The voice to use. All OpenAI-defined voices can be used (<code>alloy</code>, <code>ash</code>, ...), as well as those defined by the kokoro model (<code>af_sky</code>, <code>am_echo</code>, ...). Default: <code>shimmer</code> <code>response_format</code> No Format of the response. <code>mp3</code>, <code>wav</code>, <code>opus</code>, and <code>pcm</code> are supported. Default: <code>mp3</code> <code>stream_format</code> No If set, the response will be streamed. Only <code>audio</code> is supported, which will output <code>pcm</code> audio. Default: not set #### Example request Bash <pre><code>curl -X POST http://localhost:8000/api/v1/audio/speech \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n        \"model\": \"kokoro-v1\",\n        \"input\": \"Lemonade can speak!\",\n        \"speed\": 1.0,\n        \"steps\": 4,\n        \"response_format\": \"mp3\"\n      }'\n</code></pre>"},{"location":"server/server_spec/#response-format_6","title":"Response format","text":"<p>The generated audio file is returned as-is.</p>"},{"location":"server/server_spec/#get-apiv1models","title":"<code>GET /api/v1/models</code>","text":"<p>Returns a list of models available on the server in an OpenAI-compatible format. Each model object includes extended fields like <code>checkpoint</code>, <code>recipe</code>, <code>size</code>, <code>downloaded</code>, and <code>labels</code>.</p> <p>By default, only models available locally (downloaded) are shown, matching OpenAI API behavior.</p>"},{"location":"server/server_spec/#parameters_8","title":"Parameters","text":"Parameter Required Description <code>show_all</code> No If set to <code>true</code>, returns all models from the catalog including those not yet downloaded. Defaults to <code>false</code>."},{"location":"server/server_spec/#example-request_7","title":"Example request","text":"<pre><code># Show only downloaded models (OpenAI-compatible)\ncurl http://localhost:8000/api/v1/models\n\n# Show all models including not-yet-downloaded (extended usage)\ncurl http://localhost:8000/api/v1/models?show_all=true\n</code></pre>"},{"location":"server/server_spec/#response-format_7","title":"Response format","text":"<pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\": \"Qwen3-0.6B-GGUF\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"unsloth/Qwen3-0.6B-GGUF:Q4_0\",\n      \"recipe\": \"llamacpp\",\n      \"size\": 0.38,\n      \"downloaded\": true,\n      \"suggested\": true,\n      \"labels\": [\"reasoning\"]\n    },\n    {\n      \"id\": \"Gemma-3-4b-it-GGUF\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"ggml-org/gemma-3-4b-it-GGUF:Q4_K_M\",\n      \"recipe\": \"llamacpp\",\n      \"size\": 3.61,\n      \"downloaded\": true,\n      \"suggested\": true,\n      \"labels\": [\"hot\", \"vision\"]\n    },\n    {\n      \"id\": \"SD-Turbo\",\n      \"created\": 1744173590,\n      \"object\": \"model\",\n      \"owned_by\": \"lemonade\",\n      \"checkpoint\": \"stabilityai/sd-turbo:sd_turbo.safetensors\",\n      \"recipe\": \"sd-cpp\",\n      \"size\": 5.2,\n      \"downloaded\": true,\n      \"suggested\": true,\n      \"labels\": [\"image\"],\n      \"image_defaults\": {\n        \"steps\": 4,\n        \"cfg_scale\": 1.0,\n        \"width\": 512,\n        \"height\": 512\n      }\n    }\n  ]\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>object</code> - Type of response object, always <code>\"list\"</code></li> <li><code>data</code> - Array of model objects with the following fields:</li> <li><code>id</code> - Model identifier (used for loading and inference requests)</li> <li><code>created</code> - Unix timestamp of when the model entry was created</li> <li><code>object</code> - Type of object, always <code>\"model\"</code></li> <li><code>owned_by</code> - Owner of the model, always <code>\"lemonade\"</code></li> <li><code>checkpoint</code> - Full checkpoint identifier on Hugging Face</li> <li><code>recipe</code> - Backend/device recipe used to load the model (e.g., <code>\"ryzenai-llm\"</code>, <code>\"llamacpp\"</code>, <code>\"flm\"</code>)</li> <li><code>size</code> - Model size in GB (omitted for models without size information)</li> <li><code>downloaded</code> - Boolean indicating if the model is downloaded and available locally</li> <li><code>suggested</code> - Boolean indicating if the model is recommended for general use</li> <li><code>labels</code> - Array of tags describing the model (e.g., <code>\"hot\"</code>, <code>\"reasoning\"</code>, <code>\"vision\"</code>, <code>\"embeddings\"</code>, <code>\"reranking\"</code>, <code>\"coding\"</code>, <code>\"tool-calling\"</code>, <code>\"image\"</code>)</li> <li><code>image_defaults</code> - (Image models only) Default generation parameters for the model:<ul> <li><code>steps</code> - Number of inference steps (e.g., 4 for turbo models, 20 for standard models)</li> <li><code>cfg_scale</code> - Classifier-free guidance scale (e.g., 1.0 for turbo models, 7.5 for standard models)</li> <li><code>width</code> - Default image width in pixels</li> <li><code>height</code> - Default image height in pixels</li> </ul> </li> </ul>"},{"location":"server/server_spec/#get-apiv1modelsmodel_id","title":"<code>GET /api/v1/models/{model_id}</code>","text":"<p>Retrieve a specific model by its ID. Returns the same model object format as the list endpoint above.</p>"},{"location":"server/server_spec/#parameters_9","title":"Parameters","text":"Parameter Required Description <code>model_id</code> Yes The ID of the model to retrieve. Must match one of the model IDs from the models list."},{"location":"server/server_spec/#example-request_8","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/models/Qwen3-0.6B-GGUF\n</code></pre>"},{"location":"server/server_spec/#response-format_8","title":"Response format","text":"<p>Returns a single model object with the same fields as described in the models list endpoint above.</p> <pre><code>{\n  \"id\": \"Qwen3-0.6B-GGUF\",\n  \"created\": 1744173590,\n  \"object\": \"model\",\n  \"owned_by\": \"lemonade\",\n  \"checkpoint\": \"unsloth/Qwen3-0.6B-GGUF:Q4_0\",\n  \"recipe\": \"llamacpp\",\n  \"size\": 0.38,\n  \"downloaded\": true,\n  \"suggested\": true,\n  \"labels\": [\"reasoning\"],\n  \"recipe_options\" {\n    \"ctx_size\": 8192,\n    \"llamacpp_args\": \"--no-mmap\",\n    \"llamacpp_backend\": \"rocm\"\n  }\n}\n</code></pre>"},{"location":"server/server_spec/#error-responses","title":"Error responses","text":"<p>If the model is not found, the endpoint returns a 404 error:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Model Qwen3-0.6B-GGUF has not been found\",\n    \"type\": \"not_found\"\n  }\n}\n</code></pre>"},{"location":"server/server_spec/#additional-endpoints","title":"Additional Endpoints","text":""},{"location":"server/server_spec/#post-apiv1pull","title":"<code>POST /api/v1/pull</code>","text":"<p>Register and install models for use with Lemonade Server.</p>"},{"location":"server/server_spec/#parameters_10","title":"Parameters","text":"<p>The Lemonade Server built-in model registry has a collection of model names that can be pulled and loaded. The <code>pull</code> endpoint can install any registered model, and it can also register-then-install any model available on Hugging Face.</p> <p>Common Parameters</p> Parameter Required Description <code>stream</code> No If <code>true</code>, returns Server-Sent Events (SSE) with download progress. Defaults to <code>false</code>. <p>Install a Model that is Already Registered</p> Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to install. <p>Example request:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p> <p>Register and Install a Model</p> <p>Registration will place an entry for that model in the <code>user_models.json</code> file, which is located in the user's Lemonade cache (default: <code>~/.cache/lemonade</code>). Then, the model will be installed. Once the model is registered and installed, it will show up in the <code>models</code> endpoint alongside the built-in models and can be loaded.</p> <p>The <code>recipe</code> field defines which software framework and device will be used to load and run the model. For more information on OGA and Hugging Face recipes, see the Lemonade API README. For information on GGUF recipes, see llamacpp.</p> <p>Note: the <code>model_name</code> for registering a new model must use the <code>user</code> namespace, to prevent collisions with built-in models. For example, <code>user.Phi-4-Mini-GGUF</code>.</p> Parameter Required Description <code>model_name</code> Yes Namespaced Lemonade Server model name to register and install. <code>checkpoint</code> Yes HuggingFace checkpoint to install. <code>recipe</code> Yes Lemonade API recipe to load the model with. <code>reasoning</code> No Whether the model is a reasoning model, like DeepSeek (default: false). Adds 'reasoning' label. <code>vision</code> No Whether the model has vision capabilities for processing images (default: false). Adds 'vision' label. <code>embedding</code> No Whether the model is an embedding model (default: false). Adds 'embeddings' label. <code>reranking</code> No Whether the model is a reranking model (default: false). Adds 'reranking' label. <code>mmproj</code> No Multimodal Projector (mmproj) file to use for vision models. <p>Example request:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/pull \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"user.Phi-4-Mini-GGUF\",\n    \"checkpoint\": \"unsloth/Phi-4-mini-instruct-GGUF:Q4_K_M\",\n    \"recipe\": \"llamacpp\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Installed model: user.Phi-4-Mini-GGUF\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#streaming-response-streamtrue","title":"Streaming Response (stream=true)","text":"<p>When <code>stream=true</code>, the endpoint returns Server-Sent Events with real-time download progress:</p> <pre><code>event: progress\ndata: {\"file\":\"model.gguf\",\"file_index\":1,\"total_files\":2,\"bytes_downloaded\":1073741824,\"bytes_total\":2684354560,\"percent\":40}\n\nevent: progress\ndata: {\"file\":\"config.json\",\"file_index\":2,\"total_files\":2,\"bytes_downloaded\":1024,\"bytes_total\":1024,\"percent\":100}\n\nevent: complete\ndata: {\"file_index\":2,\"total_files\":2,\"percent\":100}\n</code></pre> <p>Event Types:</p> Event Description <code>progress</code> Sent during download with current file and byte progress <code>complete</code> Sent when all files are downloaded successfully <code>error</code> Sent if download fails, with <code>error</code> field containing the message"},{"location":"server/server_spec/#post-apiv1delete","title":"<code>POST /api/v1/delete</code>","text":"<p>Delete a model by removing it from local storage. If the model is currently loaded, it will be unloaded first.</p>"},{"location":"server/server_spec/#parameters_11","title":"Parameters","text":"Parameter Required Description <code>model_name</code> Yes Lemonade Server model name to delete. <p>Example request:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/delete \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Response format:</p> <pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Deleted model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p> <p></p>"},{"location":"server/server_spec/#post-apiv1load","title":"<code>POST /api/v1/load</code>","text":"<p>Explicitly load a registered model into memory. This is useful to ensure that the model is loaded before you make a request. Installs the model if necessary.</p>"},{"location":"server/server_spec/#parameters_12","title":"Parameters","text":"Parameter Required Applies to Description <code>model_name</code> Yes All Lemonade Server model name to load. <code>save_options</code> No All Boolean. If true, saves recipe options to <code>recipe_options.json</code>. Any previously stored value for <code>model_name</code> is replaced. <code>ctx_size</code> No llamacpp, flm, ryzenai-llm Context size for the model. Overrides the default value. <code>llamacpp_backend</code> No llamacpp LlamaCpp backend to use (<code>vulkan</code>, <code>rocm</code>, <code>metal</code> or <code>cpu</code>). <code>llamacpp_args</code> No llamacpp Custom arguments to pass to llama-server. The following are NOT allowed: <code>-m</code>, <code>--port</code>, <code>--ctx-size</code>, <code>-ngl</code>. <code>whispercpp_backend</code> No whispercpp WhisperCpp backend to use (<code>npu</code> or <code>cpu</code>). Default is <code>npu</code> if supported. <code>steps</code> No sd-cpp Number of inference steps for image generation. Default: 20. <code>cfg_scale</code> No sd-cpp Classifier-free guidance scale for image generation. Default: 7.0. <code>width</code> No sd-cpp Image width in pixels. Default: 512. <code>height</code> No sd-cpp Image height in pixels. Default: 512. <p>Setting Priority:</p> <p>When loading a model, settings are applied in this priority order: 1. Values explicitly passed in the <code>load</code> request (highest priority) 2. Per-model values configurable in <code>recipe_options.json</code> (see below for details) 3. Values set via <code>lemonade-server</code> CLI arguments or environment variables 4. Default hardcoded values in <code>lemonade-router</code> (lowest priority)</p>"},{"location":"server/server_spec/#per-model-options","title":"Per-model options","text":"<p>You can configure recipe-specific options on a per-model basis. Lemonade manages a file called <code>recipe_options.json</code> in the user's Lemonade cache (default: <code>~/.cache/lemonade</code>). The available options depend on the model's recipe:</p> <pre><code>{\n  \"user.Qwen2.5-Coder-1.5B-Instruct\": {\n    \"ctx_size\": 16384,\n    \"llamacpp_backend\": \"vulkan\",\n    \"llamacpp_args\": \"-np 2 -kvu\"\n  },\n  \"Qwen3-Coder-30B-A3B-Instruct-GGUF\" : {\n    \"llamacpp_backend\": \"rocm\"\n  },\n  \"whisper-large-v3-turbo-q8_0.bin\": {\n    \"whispercpp_backend\": \"npu\"\n  }\n}\n</code></pre> <p>Note that model names include any applicable prefix, such as <code>user.</code> and <code>extra.</code>.</p>"},{"location":"server/server_spec/#example-requests","title":"Example requests","text":"<p>Basic load:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen2.5-0.5B-Instruct-CPU\"\n  }'\n</code></pre> <p>Load with custom settings:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen3-0.6B-GGUF\",\n    \"ctx_size\": 8192,\n    \"llamacpp_backend\": \"rocm\",\n    \"llamacpp_args\": \"--flash-attn on --no-mmap\"\n  }'\n</code></pre> <p>Load and save settings:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"Qwen3-0.6B-GGUF\",\n    \"ctx_size\": 8192,\n    \"llamacpp_backend\": \"vulkan\",\n    \"llamacpp_args\": \"--no-context-shift --no-mmap\",\n    \"save_options\": true\n  }'\n</code></pre> <p>Load a Whisper model with NPU backend:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"whisper-large-v3-turbo-q8_0.bin\",\n    \"whispercpp_backend\": \"npu\"\n  }'\n</code></pre> <p>Load an image generation model with custom settings:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/load \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model_name\": \"sd-turbo\",\n    \"steps\": 4,\n    \"cfg_scale\": 1.0,\n    \"width\": 512,\n    \"height\": 512\n  }'\n</code></pre>"},{"location":"server/server_spec/#response-format_9","title":"Response format","text":"<pre><code>{\n  \"status\":\"success\",\n  \"message\":\"Loaded model: Qwen2.5-0.5B-Instruct-CPU\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#post-apiv1unload","title":"<code>POST /api/v1/unload</code>","text":"<p>Explicitly unload a model from memory. This is useful to free up memory while still leaving the server process running (which takes minimal resources but a few seconds to start).</p>"},{"location":"server/server_spec/#parameters_13","title":"Parameters","text":"Parameter Required Description <code>model_name</code> No Name of the specific model to unload. If not provided, all loaded models will be unloaded."},{"location":"server/server_spec/#example-requests_1","title":"Example requests","text":"<p>Unload a specific model:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/unload \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model_name\": \"Qwen3-0.6B-GGUF\"}'\n</code></pre> <p>Unload all models:</p> <pre><code>curl -X POST http://localhost:8000/api/v1/unload\n</code></pre>"},{"location":"server/server_spec/#response-format_10","title":"Response format","text":"<p>Success response:</p> <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Model unloaded successfully\"\n}\n</code></pre> <p>Error response (model not found):</p> <pre><code>{\n  \"status\": \"error\",\n  \"message\": \"Model not found: Qwen3-0.6B-GGUF\"\n}\n</code></pre> <p>In case of an error, the status will be <code>error</code> and the message will contain the error message.</p>"},{"location":"server/server_spec/#get-apiv1health","title":"<code>GET /api/v1/health</code>","text":"<p>Check the health of the server. This endpoint returns information about loaded models.</p>"},{"location":"server/server_spec/#parameters_14","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_9","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/health\n</code></pre>"},{"location":"server/server_spec/#response-format_11","title":"Response format","text":"<pre><code>{\n  \"status\": \"ok\",\n  \"model_loaded\": \"Llama-3.2-1B-Instruct-Hybrid\",\n  \"all_models_loaded\": [\n    {\n      \"model_name\": \"Llama-3.2-1B-Instruct-Hybrid\",\n      \"checkpoint\": \"amd/Llama-3.2-1B-Instruct-awq-g128-int4-asym-fp16-onnx-hybrid\",\n      \"last_use\": 1732123456.789,\n      \"type\": \"llm\",\n      \"device\": \"gpu npu\",\n      \"recipe\": \"ryzenai-llm\",\n      \"recipe_options\": {\n        \"ctx_size\": 4096\n      },\n      \"backend_url\": \"http://127.0.0.1:8001/v1\"\n    },\n    {\n      \"model_name\": \"nomic-embed-text-v1-GGUF\",\n      \"checkpoint\": \"nomic-ai/nomic-embed-text-v1-GGUF:Q4_K_S\",\n      \"last_use\": 1732123450.123,\n      \"type\": \"embedding\",\n      \"device\": \"gpu\",\n      \"recipe\": \"llamacpp\",\n      \"recipe_options\": {\n        \"ctx_size\": 8192,\n        \"llamacpp_args\": \"--no-mmap\",\n        \"llamacpp_backend\": \"rocm\"\n      },\n      \"backend_url\": \"http://127.0.0.1:8002/v1\"\n    }\n  ],\n  \"max_models\": {\n    \"llm\": 3,\n    \"embedding\": 1,\n    \"reranking\": 1\n  }\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>status</code> - Server health status, always <code>\"ok\"</code></li> <li><code>model_loaded</code> - Model name of the most recently accessed model</li> <li><code>all_models_loaded</code> - Array of all currently loaded models with details:</li> <li><code>model_name</code> - Name of the loaded model</li> <li><code>checkpoint</code> - Full checkpoint identifier</li> <li><code>last_use</code> - Unix timestamp of last access (load or inference)</li> <li><code>type</code> - Model type: <code>\"llm\"</code>, <code>\"embedding\"</code>, or <code>\"reranking\"</code></li> <li><code>device</code> - Space-separated device list: <code>\"cpu\"</code>, <code>\"gpu\"</code>, <code>\"npu\"</code>, or combinations like <code>\"gpu npu\"</code></li> <li><code>backend_url</code> - URL of the backend server process handling this model (useful for debugging)</li> <li><code>recipe</code>: - Backend/device recipe used to load the model (e.g., <code>\"ryzenai-llm\"</code>, <code>\"llamacpp\"</code>, <code>\"flm\"</code>)</li> <li><code>recipe_options</code>: - Options used to load the model (e.g., <code>\"ctx_size\"</code>, <code>\"llamacpp_backend\"</code>, <code>\"llamacpp_args\"</code>)</li> <li><code>max_models</code> - Maximum number of models that can be loaded simultaneously (set via <code>--max-loaded-models</code>):</li> <li><code>llm</code> - Maximum LLM/chat models</li> <li><code>embedding</code> - Maximum embedding models</li> <li><code>reranking</code> - Maximum reranking models</li> </ul>"},{"location":"server/server_spec/#get-apiv1stats","title":"<code>GET /api/v1/stats</code>","text":"<p>Performance statistics from the last request.</p>"},{"location":"server/server_spec/#parameters_15","title":"Parameters","text":"<p>This endpoint does not take any parameters.</p>"},{"location":"server/server_spec/#example-request_10","title":"Example request","text":"<pre><code>curl http://localhost:8000/api/v1/stats\n</code></pre>"},{"location":"server/server_spec/#response-format_12","title":"Response format","text":"<pre><code>{\n  \"time_to_first_token\": 2.14,\n  \"tokens_per_second\": 33.33,\n  \"input_tokens\": 128,\n  \"output_tokens\": 5,\n  \"decode_token_times\": [0.01, 0.02, 0.03, 0.04, 0.05],\n  \"prompt_tokens\": 9\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li><code>time_to_first_token</code> - Time in seconds until the first token was generated</li> <li><code>tokens_per_second</code> - Generation speed in tokens per second</li> <li><code>input_tokens</code> - Number of tokens processed</li> <li><code>output_tokens</code> - Number of tokens generated</li> <li><code>decode_token_times</code> - Array of time taken for each generated token</li> <li><code>prompt_tokens</code> - Total prompt tokens including cached tokens</li> </ul>"},{"location":"server/server_spec/#get-apiv1system-info","title":"<code>GET /api/v1/system-info</code>","text":"<p>System information endpoint that provides complete hardware details and device enumeration.</p>"},{"location":"server/server_spec/#example-request_11","title":"Example request","text":"<pre><code>curl \"http://localhost:8000/api/v1/system-info\"\n</code></pre>"},{"location":"server/server_spec/#response-format_13","title":"Response format","text":"<pre><code>{\n  \"OS Version\": \"Windows-10-10.0.26100-SP0\",\n  \"Processor\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n  \"Physical Memory\": \"32.0 GB\",\n  \"OEM System\": \"ASUS Zenbook S 16\",\n  \"BIOS Version\": \"1.0.0\",\n  \"CPU Max Clock\": \"5100 MHz\",\n  \"Windows Power Setting\": \"Balanced\",\n  \"devices\": {\n    \"cpu\": {\n      \"name\": \"AMD Ryzen AI 9 HX 375 w/ Radeon 890M\",\n      \"cores\": 12,\n      \"threads\": 24,\n      \"available\": true\n    },\n    \"amd_igpu\": {\n      \"name\": \"AMD Radeon(TM) 890M Graphics\",\n      \"vram_gb\": 0.5,\n      \"available\": true\n    },\n    \"amd_dgpu\": [],\n    \"npu\": {\n      \"name\": \"AMD NPU\",\n      \"power_mode\": \"Default\",\n      \"available\": true\n    }\n  },\n  \"recipes\": {\n    \"llamacpp\": {\n      \"backends\": {\n        \"vulkan\": {\n          \"devices\": [\"cpu\", \"amd_igpu\"],\n          \"supported\": true,\n          \"available\": true,\n          \"version\": \"b7869\"\n        },\n        \"rocm\": {\n          \"devices\": [\"amd_igpu\"],\n          \"supported\": true,\n          \"available\": false\n        },\n        \"metal\": {\n          \"devices\": [],\n          \"supported\": false,\n          \"error\": \"Requires macOS\"\n        },\n        \"cpu\": {\n          \"devices\": [\"cpu\"],\n          \"supported\": true,\n          \"available\": false\n        }\n      }\n    },\n    \"whispercpp\": {\n      \"backends\": {\n        \"default\": {\n          \"devices\": [\"cpu\"],\n          \"supported\": true,\n          \"available\": false\n        }\n      }\n    },\n    \"sd-cpp\": {\n      \"backends\": {\n        \"default\": {\n          \"devices\": [\"cpu\"],\n          \"supported\": true,\n          \"available\": false\n        }\n      }\n    },\n    \"flm\": {\n      \"backends\": {\n        \"default\": {\n          \"devices\": [\"npu\"],\n          \"supported\": true,\n          \"available\": true,\n          \"version\": \"1.2.0\"\n        }\n      }\n    },\n    \"ryzenai-llm\": {\n      \"backends\": {\n        \"default\": {\n          \"devices\": [\"npu\"],\n          \"supported\": true,\n          \"available\": true\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Field Descriptions:</p> <ul> <li>System fields:</li> <li><code>OS Version</code> - Operating system name and version</li> <li><code>Processor</code> - CPU model name</li> <li><code>Physical Memory</code> - Total RAM</li> <li><code>OEM System</code> - System/laptop model name (Windows only)</li> <li><code>BIOS Version</code> - BIOS information (Windows only)</li> <li><code>CPU Max Clock</code> - Maximum CPU clock speed (Windows only)</li> <li> <p><code>Windows Power Setting</code> - Current power plan (Windows only)</p> </li> <li> <p><code>devices</code> - Hardware devices detected on the system (no software/support information)</p> </li> <li><code>cpu</code> - CPU information (name, cores, threads)</li> <li><code>amd_igpu</code> - AMD integrated GPU (if present)</li> <li><code>amd_dgpu</code> - Array of AMD discrete GPUs (if present)</li> <li><code>nvidia_dgpu</code> - Array of NVIDIA discrete GPUs (if present)</li> <li> <p><code>npu</code> - NPU device (if present)</p> </li> <li> <p><code>recipes</code> - Software recipes and their backend support status</p> </li> <li>Each recipe (e.g., <code>llamacpp</code>, <code>whispercpp</code>, <code>flm</code>) contains:<ul> <li><code>backends</code> - Available backends for this recipe</li> <li>Each backend contains:<ul> <li><code>devices</code> - List of devices on this system that support this backend (empty if not supported)</li> <li><code>supported</code> - Whether installation is possible on this system</li> <li><code>available</code> - Whether the backend is currently installed</li> <li><code>version</code> - Installed version (if available)</li> <li><code>error</code> - Reason why not supported (if applicable)</li> </ul> </li> </ul> </li> </ul>"},{"location":"server/server_spec/#debugging","title":"Debugging","text":"<p>To help debug the Lemonade server, you can use the <code>--log-level</code> parameter to control the verbosity of logging information. The server supports multiple logging levels that provide increasing amounts of detail about server operations.</p> <pre><code>lemonade-server serve --log-level [level]\n</code></pre> <p>Where <code>[level]</code> can be one of:</p> <ul> <li>critical: Only critical errors that prevent server operation.</li> <li>error: Error conditions that might allow continued operation.</li> <li>warning: Warning conditions that should be addressed.</li> <li>info: (Default) General informational messages about server operation.</li> <li>debug: Detailed diagnostic information for troubleshooting, including metrics such as input/output token counts, Time To First Token (TTFT), and Tokens Per Second (TPS).</li> <li>trace: Very detailed tracing information, including everything from debug level plus all input prompts.</li> </ul>"},{"location":"server/server_spec/#gguf-support","title":"GGUF Support","text":"<p>The <code>llama-server</code> backend works with Lemonade's suggested <code>*-GGUF</code> models, as well as any .gguf model from Hugging Face. Windows and Ubuntu Linux are supported. Details: - Lemonade Server wraps <code>llama-server</code> with support for the <code>lemonade-server</code> CLI, client web app, and endpoints (e.g., <code>models</code>, <code>pull</code>, <code>load</code>, etc.).   - The <code>chat/completions</code>, <code>completions</code>, <code>embeddings</code>, and <code>reranking</code> endpoints are supported.   - The <code>embeddings</code> endpoint requires embedding-specific models (e.g., nomic-embed-text models).   - The <code>reranking</code> endpoint requires reranker-specific models (e.g., bge-reranker models).   - <code>responses</code> is not supported at this time. - A single Lemonade Server process can seamlessly switch between GGUF, ONNX, and FastFlowLM models.   - Lemonade Server will attempt to load models onto GPU with Vulkan first, and if that doesn't work it will fall back to CPU.   - From the end-user's perspective, OGA vs. GGUF should be completely transparent: they wont be aware of whether the built-in server or <code>llama-server</code> is serving their model.</p>"},{"location":"server/server_spec/#installing-gguf-models","title":"Installing GGUF Models","text":"<p>To install an arbitrary GGUF from Hugging Face, open the Lemonade web app by navigating to http://localhost:8000 in your web browser, click the Model Management tab, and use the Add a Model form.</p>"},{"location":"server/server_spec/#platform-support-matrix","title":"Platform Support Matrix","text":"Platform GPU Acceleration CPU Architecture Windows \u2705 Vulkan, ROCm \u2705 x64 Ubuntu \u2705 Vulkan, ROCm \u2705 x64 Other Linux \u26a0\ufe0f* Vulkan \u26a0\ufe0f* x64 <p>*Other Linux distributions may work but are not officially supported.</p>"},{"location":"server/server_spec/#fastflowlm-support","title":"FastFlowLM Support","text":"<p>Similar to the llama-server support, Lemonade can also route OpenAI API requests to a FastFlowLM <code>flm serve</code> backend.</p> <p>The <code>flm serve</code> backend works with Lemonade's suggested <code>*-FLM</code> models, as well as any model mentioned in <code>flm list</code>. Windows is the only supported operating system. Details: - Lemonade Server wraps <code>flm serve</code> with support for the <code>lemonade-server</code> CLI, client web app, and all Lemonade custom endpoints (e.g., <code>pull</code>, <code>load</code>, etc.).   - OpenAI API endpoints supported: <code>models</code>, <code>chat/completions</code> (streaming), and <code>embeddings</code>.   - The <code>embeddings</code> endpoint requires embedding-specific models supported by FLM. - A single Lemonade Server process can seamlessly switch between FLM, OGA, and GGUF models.</p>"},{"location":"server/server_spec/#installing-flm-models","title":"Installing FLM Models","text":"<p>To install an arbitrary FLM model: 1. <code>flm list</code> to view the supported models. 1. Open the Lemonade web app by navigating to http://localhost:8000 in your web browser, click the Model Management tab, and use the Add a Model form. 1. Use the model name from <code>flm list</code> as the \"checkpoint name\" in the Add a Model form and select \"flm\" as the recipe.</p>"},{"location":"server/apps/","title":"App Integration Guides","text":"<p>This folder contains integration guides for connecting third-party applications to Lemonade Server.</p>"},{"location":"server/apps/#view-the-marketplace","title":"View the Marketplace","text":"<p>For a complete list of compatible apps with links to guides, videos, and more, visit the Lemonade Marketplace.</p>"},{"location":"server/apps/#contributing","title":"Contributing","text":"<p>If you've connected Lemonade to a new application and would like to contribute a guide, see our contribution guide or email us at lemonade@amd.com.</p>"},{"location":"server/apps/ai-dev-gallery/","title":"AI Dev Gallery with Lemonade Server","text":""},{"location":"server/apps/ai-dev-gallery/#overview","title":"Overview","text":"<p>AI Dev Gallery is Microsoft's showcase application that demonstrates various AI capabilities through built-in samples and applications. It provides an easy way to explore and experiment with different AI models and scenarios, including text generation, chat applications, and more.</p> <p>AI Dev Gallery has native integration with Lemonade Server, which means it can automatically detect and connect to your local Lemonade instance without manual URL configuration.</p>"},{"location":"server/apps/ai-dev-gallery/#expectations","title":"Expectations","text":"<p>AI Dev Gallery works well with most models available in Lemonade. The built-in samples are designed to work with various model types and sizes, making it a great tool for testing and exploring different AI capabilities locally.</p> <p>The application provides a user-friendly interface for experimenting with AI models through pre-built scenarios, making it accessible for both beginners and advanced users.</p>"},{"location":"server/apps/ai-dev-gallery/#setup","title":"Setup","text":""},{"location":"server/apps/ai-dev-gallery/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Important: Make sure your Lemonade Server is running before opening AI Dev Gallery.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#install-ai-dev-gallery","title":"Install AI Dev Gallery","text":"<ol> <li>Open the Microsoft Store on Windows.</li> <li>Search for \"AI Dev Gallery\" by Microsoft Corporation.</li> <li>Click \"Install\" to download and install the application.</li> </ol> <p>Alternatively, you can access AI Dev Gallery directly through aka.ms/ai-dev-gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#connect-to-lemonade","title":"Connect to Lemonade","text":"<p>AI Dev Gallery has native integration with Lemonade Server, so no manual configuration is required. The application will automatically detect your running Lemonade Server instance.</p> <p>Important: Ensure your Lemonade Server is running before launching AI Dev Gallery.</p>"},{"location":"server/apps/ai-dev-gallery/#usage","title":"Usage","text":"<p>AI Dev Gallery provides various built-in applications and samples to explore AI capabilities:</p>"},{"location":"server/apps/ai-dev-gallery/#quick-start","title":"Quick Start","text":"<ol> <li>Launch AI Dev Gallery.</li> <li>Navigate to Samples \u2192 Text \u2192 Chat (or another text/code sample).</li> <li>Click on the model selector above the chat window.</li> <li>Select Lemonade from the available providers.</li> <li>Choose your preferred model from the list of available models.</li> </ol>"},{"location":"server/apps/ai-dev-gallery/#supported-scenarios","title":"Supported Scenarios","text":"<p>AI Dev Gallery supports various AI scenarios through its sample applications with Lemonade integration:</p> <p>Text Processing:</p> <ul> <li>Conversational AI: Chat and Semantic Kernel Chat for interactive conversations</li> <li>Content Generation: Generate text for various purposes and creative writing</li> <li>Language Tasks: Translation, grammar checking, and paraphrasing</li> <li>Text Analysis: Sentiment analysis and content moderation</li> <li>Information Retrieval: Semantic search and retrieval augmented generation</li> <li>Text Enhancement: Summarization and custom parameter configurations</li> </ul> <p>Code Assistance:</p> <ul> <li>Code Generation: Create code snippets and programs</li> <li>Code Analysis: Explain existing code and understand functionality</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#tips-for-best-experience","title":"Tips for Best Experience","text":"<ul> <li>Start your Lemonade Server before opening AI Dev Gallery</li> <li>Try different models to see how they perform across various scenarios</li> <li>Explore different sample categories to understand various AI capabilities</li> <li>Use the built-in samples as starting points for your own AI experiments</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#troubleshooting","title":"Troubleshooting","text":""},{"location":"server/apps/ai-dev-gallery/#ai-dev-gallery-doesnt-detect-lemonade","title":"AI Dev Gallery doesn't detect Lemonade","text":"<ul> <li>Ensure Lemonade Server is running and accessible at <code>http://localhost:8000</code></li> <li>Restart AI Dev Gallery after ensuring Lemonade Server is running</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#models-not-appearing-in-the-selector","title":"Models not appearing in the selector","text":"<ul> <li>Open <code>http://localhost:8000</code> in a browser and make sure to download the models you want to use through the \"Model Manager\" tab.</li> </ul>"},{"location":"server/apps/ai-dev-gallery/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Dev Gallery Website</li> <li>Lemonade Server Models</li> </ul>"},{"location":"server/apps/ai-toolkit/","title":"Microsoft AI Toolkit for VS Code","text":""},{"location":"server/apps/ai-toolkit/#overview","title":"Overview","text":"<p>The AI Toolkit for Visual Studio Code is a VS Code extension that simplifies generative AI app development by bringing together cutting-edge AI development tools and models from various catalogs. It supports running AI models locally or connecting to remote models via API keys.</p>"},{"location":"server/apps/ai-toolkit/#demo-video","title":"Demo Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p>"},{"location":"server/apps/ai-toolkit/#expectations","title":"Expectations","text":"<p>We have found that most LLMs work well with this application.</p> <p>However, the <code>Inference Parameters</code> option is not fully supported, as Lemonade Server currently does not accept those as inputs (see server_spec.md for details).</p>"},{"location":"server/apps/ai-toolkit/#setup","title":"Setup","text":""},{"location":"server/apps/ai-toolkit/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/ai-toolkit/#install-ai-toolkit-for-vs-code","title":"Install AI Toolkit for VS Code","text":"<ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search for \"AI Toolkit for Visual Studio Code\" in the Extensions Marketplace search bar.</li> <li>Select the AI Toolkit extension and click install.</li> </ol> <p>This will add an AI Toolkit icon to your VS Code Activity Bar.</p>"},{"location":"server/apps/ai-toolkit/#connect-lemonade-to-ai-toolkit","title":"Connect Lemonade to AI Toolkit","text":"<p>The AI Toolkit now supports \"Bring Your Own Model\" functionality, allowing you to connect to models served via the OpenAI API standard, which Lemonade uses.</p> <ol> <li>Open the AI Toolkit tab in your VS Code Activity Bar.</li> <li>In the right corner of the \"My Models\" section, click the \"+\" button to \"Add model for remote inference\".</li> <li>Select \"Add a custom model\".</li> <li>When prompted to \"Enter OpenAI chat completion endpoint URL\" enter:     <pre><code>http://localhost:8000/api/v1/chat/completions\n</code></pre></li> <li>When prompted to \"Enter the exact model name as in the API\" select a model (e.g., <code>Phi-3-Mini-Instruct-Hybrid</code>)<ul> <li>Note: You can get a list of all models available here.</li> </ul> </li> <li>Select the same name as the display model name.</li> <li>Skip the HTTP authentication step by pressing \"Enter\".</li> </ol>"},{"location":"server/apps/ai-toolkit/#usage","title":"Usage","text":"<p>Once you've set up the Lemonade model in AI Toolkit, you can:</p> <ol> <li>Use the AI Playground tool to directly interact with your added model.</li> <li>Use the Prompt Builder tool to craft effective prompts for your AI models.</li> <li>Use the Bulk Run tool to compute responses for custom datasets and easily visualize those responses on a table format.</li> <li>Use the Evaluation tool to quickly assess your model's coherence, fluency, relevance, and similarity, as well as to compute BLEU, F1, GLEU, and Meteor scores.</li> </ol>"},{"location":"server/apps/ai-toolkit/#additional-resources","title":"Additional Resources","text":"<ul> <li>AI Toolkit for VS Code Documentation</li> <li>AI Toolkit GitHub Repository</li> <li>Bring Your Own Models on AI Toolkit</li> </ul>"},{"location":"server/apps/anythingLLM/","title":"Running agents locally with Lemonade and AnythingLLM","text":""},{"location":"server/apps/anythingLLM/#overview","title":"Overview","text":"<p>AnythingLLM is a versatile local LLM platform that allows you to chat with your documents and code using a variety of models. It supports the OpenAI-compatible API interface, allowing easy integration with local servers like Lemonade.</p> <p>This guide will help you configure AnythingLLM to use Lemonade's OpenAI-compatible server, and utilize the powerful <code>@agent</code> capability to interact with documents, webpages, and more.</p>"},{"location":"server/apps/anythingLLM/#expectations","title":"Expectations","text":"<p>Lemonade integrates best with AnythingLLM when using models such as <code>Qwen-1.5-7B-Chat-Hybrid</code> and <code>Llama-3.2-1B-Instruct-Hybrid</code>, both of which support a context length of up to 3,000 tokens.</p> <p>Keep in mind that when using the <code>@agent</code> feature, multi-turn conversations can quickly consume available context. As a result, the number of back-and-forth turns in a single conversation may be limited due to the growing context size.</p>"},{"location":"server/apps/anythingLLM/#setup","title":"Setup","text":""},{"location":"server/apps/anythingLLM/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Install and set up AnythingLLM from their GitHub or website.</li> </ol>"},{"location":"server/apps/anythingLLM/#configure-anythingllm-to-use-lemonade","title":"Configure AnythingLLM to Use Lemonade","text":"<ol> <li>In the bottom of the left menu, click on the wrench icon to \"Open Settings\".</li> <li>Under the menu \"AI Providers\", click \"LLM\".</li> <li>     Select \"Generic OpenAI\" and enter the following info:      SettingValue Base URL<code>http://localhost:8000/api/v1</code> API Key<code>-</code> Chat Model Name<code>Qwen-1.5-7B-Chat-Hybrid</code> Token context window<code>3000</code> Max Tokens<code>3000</code> </li> <li>In the bottom left, click the back button to exit.</li> <li>In the left menu, click \"New Workspace\" and give it a name.</li> <li>Where you see your new workspace, click the gear icon to open the \"Workspace Settings\"</li> <li>In the top menu of the window that opens, click on \"Agent Configuration\"</li> <li>Under Chat Settings, select Generic OpenAI and click save.</li> <li>Under Workspace Agent LLM Provider, select \"Generic OpenAI\" and click save.</li> </ol>"},{"location":"server/apps/anythingLLM/#usage-with-agent","title":"Usage with @agent","text":""},{"location":"server/apps/anythingLLM/#overview_1","title":"Overview","text":"<p>Agents are capable of scraping websites, listing and summarizing documents, searching the web, creating charts, and even saving files to your desktop or their own memory.</p> <p>To start an agent session, simply go to any workspace and type <code>@agent &lt;your prompt&gt;</code>. To exit the session, just type <code>exit</code>.</p>"},{"location":"server/apps/anythingLLM/#agent-skills","title":"Agent Skills","text":"<p>You may turn on and off specific <code>Agent Skills</code> by going to your <code>Workspace Settings</code> \u2192 <code>Agent Configuration</code> \u2192 <code>Configure Agent Skills</code>.</p> <p>Available agent skills include:</p> <ul> <li>RAG &amp; long-term memory</li> <li>View and summarize documents</li> <li>Scrape Websites</li> <li>Generate &amp; save files to browser</li> <li>Generate Charts</li> <li>Web Search</li> <li>SQL Connector</li> </ul>"},{"location":"server/apps/anythingLLM/#examples","title":"Examples","text":"<p>Here are some examples on how you can interact with Anything LLM agents:</p> <ul> <li>Rag &amp; long-term memory<ul> <li><code>@agent My name is Dr Lemon. Remember this in our next conversation</code></li> <li>Then, on a follow up chat you can ask <code>@agent What is my name according to your memory?</code></li> </ul> </li> <li>Scrape Websites<ul> <li><code>@agent Scrape this website and tell me what are the two ways of installing lemonade https://github.com/lemonade-sdk/lemonade/blob/main/docs/server/README.md</code></li> </ul> </li> <li>Web Search (enable skill before trying)<ul> <li><code>@agent Search the web for the best place to buy shoes</code></li> </ul> </li> </ul> <p>You can find more details about agent usage here.</p>"},{"location":"server/apps/anythingLLM/#additional-resources","title":"Additional Resources","text":"<ul> <li>AnthingLLM Website</li> <li>AnythingLLM GitHub</li> <li>AnythingLLM Documentation</li> </ul>"},{"location":"server/apps/codeGPT/","title":"CodeGPT with VS Code","text":""},{"location":"server/apps/codeGPT/#overview","title":"Overview","text":"<p>CodeGPT Chat is an AI-powered chatbot designed to assist developers with coding tasks directly within their preferred integrated development environments (IDEs), for example, VS Code.</p>"},{"location":"server/apps/codeGPT/#expectations","title":"Expectations","text":"<p>We have found that the <code>Qwen-1.5-7B-Chat-Hybrid</code> model is the best Hybrid model available for coding. It is good at chatting with a few files at a time in your codebase to learn more about them. It can also make simple code editing suggestions pertaining to a few lines of code at a time.</p> <p>However, we do not recommend using this model for analyzing large codebases at once or making large or complex file edits.</p>"},{"location":"server/apps/codeGPT/#setup","title":"Setup","text":""},{"location":"server/apps/codeGPT/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/codeGPT/#install-codegpt-in-vs-code","title":"Install CodeGPT in VS Code","text":"<p>The following instructions are based off CodeGPT provided instructions found here.</p> <ol> <li>Open the Extensions tab in VS Code Activity Bar.</li> <li>Search \"CodeGPT: Chat &amp; AI Agents\" in the Extensions Marketplace search bar.</li> <li>Select the CodeGPT extension and click install.</li> </ol> <p>This will add a CodeGPT tab to your VS Code Activity Bar.</p>"},{"location":"server/apps/codeGPT/#add-lemonade-server-to-codegpt","title":"Add Lemonade Server to CodeGPT","text":"<p>Note: The following instructions are based on instructions from CodeGPT found here.</p> <ol> <li>Open the CodeGPT tab in your VS Code Activity Bar.</li> <li>Sign Up or Sign into your account.</li> <li>In the model dropdown menu and click \"View More\".</li> <li>Select the tab: \"LLMs Cloud model\"</li> <li>Under \"All Models\", set the following:    FieldValue Select Provider:<code>Custom</code> Select Model: <code>Qwen-1.5-7B-Chat-Hybrid</code> </li> <li>Click \"Change connection settings\" and enter the following information:      FieldValue API Key<code>-</code> Custom Link<code>http://localhost:8000/api/v1/api/v1</code> </li> </ol>"},{"location":"server/apps/codeGPT/#usage","title":"Usage","text":"<p>Note: see the CodeGPT user guide to learn about all of their features.</p> <p>To try out CodeGPT:</p> <ul> <li>Open the CodeGPT tab in your VS Code Activity Bar, and in the chat box, type a question about your code. Use the <code>#</code> symbol to specify a file.</li> <li>Example: \"What's the fastest way to install lemonade in #getting_started.md?\"</li> <li>Use /Fix to find and fix a minor bug.</li> <li>Use /Document to come up with docstrings and comments for a file.</li> <li>Use /UnitTest to make a  test file.</li> </ul>"},{"location":"server/apps/continue/","title":"Continue Coding Assistant","text":"<p>Continue provides open-source Integrated Development Environment (IDE) extensions, such as for Visual Studio Code and JetBrains, and an open-source CLI that lets developers leverage custom AI coding agents.</p> <p>This guide walks through how to use Lemonade Server with the Continue VS Code extension for code generation, editing, and chat capabilities, all running locally on your AMD PC.</p>"},{"location":"server/apps/continue/#prerequisites","title":"Prerequisites","text":"<p>Before you start, make sure you have the following:</p>"},{"location":"server/apps/continue/#software-requirements","title":"Software Requirements","text":"<ul> <li>IDE: Visual Studio Code (v1.80+) or another supported IDE.</li> <li>Lemonade Server: Installed and set up using the Getting Started guide.</li> <li>Lemonade Server Running: The server should be running at <code>http://localhost:8000</code>. If you change the port in Lemonade Server (e.g., to 8020, 8040, etc.), you'll need to update the API Base URL in Continue's configuration to match the same port.</li> <li>Model Downloaded: At least one model from the supported models list must be installed locally.</li> </ul>"},{"location":"server/apps/continue/#hardware-requirements","title":"Hardware Requirements","text":"<p>For best results, a code-tuned model with at least 20B parameters is required. To run such a model:</p> <ul> <li>Minimum spec: PC with an integrated GPU (Ryzen\u2122 AI 7000-series or newer) and 64 GB system RAM.</li> <li>Recommended specs:<ul> <li>PC with a discrete GPU that has 16 GB VRAM or greater (Radeon\u2122 7800 XT or newer).</li> <li>Strix Halo PC with 64 GB System RAM or greater.</li> </ul> </li> </ul>"},{"location":"server/apps/continue/#setup","title":"Setup","text":""},{"location":"server/apps/continue/#configuring-lemonade-server-with-continue","title":"Configuring Lemonade Server with Continue","text":"<ol> <li> <p>Install Models Locally</p> <ul> <li>Use the Model Manager or lemonade-server CLI to download your desired model, for example:</li> </ul> <p><pre><code>lemonade-server pull &lt;model-name&gt;\n</code></pre> Example downloading Qwen3-Coder: <pre><code>lemonade-server pull Qwen3-Coder-30B-A3B-Instruct-GGUF\n</code></pre></p> </li> <li> <p>Start Lemonade Server: Ensure Lemonade Server is running at <code>http://localhost:8000</code>. You can start it from the Lemon tray icon or by running:</p> <pre><code>lemonade-server serve\n</code></pre> </li> <li> <p>Verify Model is Loaded: Use the Model Manager or tray icon to confirm your model is loaded and ready. Continue will automatically detect Lemonade Server running on localhost.</p> </li> </ol>"},{"location":"server/apps/continue/#setting-up-continue-extension-in-vs-code","title":"Setting Up Continue Extension in VS Code","text":"<ol> <li>Go to Extensions Marketplace: In VS Code, click the Extensions icon in the Activity bar (default is on the left).</li> <li> <p>Add \"Continue\": Type \"Continue\" in the search box. Click \"Install\" on the Continue extension entry.</p> <p>Example marketplace screen: </p> </li> <li> <p>Open Continue in VS Code: After installation completes, the Continue logo appears in the Activity bar. Click it to open the extension.</p> </li> <li> <p>Add Lemonade Server Provider: Click the model dropdown menu in the Continue sidebar, then select \"Add Chat Model\". Choose \"Lemonade Server\" from the list of available providers. Continue will set the default address to <code>http://localhost:8000</code>, but it can be changed to match a different setup.</p> <p>Example configuration screen: </p> </li> <li> <p>Select Your Model: Once Lemonade Server is added, use the drop-down menu to select the model you downloaded earlier (e.g., <code>Qwen3-Coder-30B-A3B-Instruct-GGUF</code>).</p> <p>Example model selection: </p> </li> </ol>"},{"location":"server/apps/continue/#working-with-continuedev","title":"Working with Continue.dev","text":"<p>Continue provides three interaction modes for different development tasks:</p> <ol> <li>Chat: Code explanations, debugging discussions, architecture planning</li> <li>Plan: Provides a safe environment with read-only tools for exploring code and planning changes</li> <li>Agent: Multi-file refactoring, large-scale changes across projects</li> </ol> <p></p> <p>See the Continue Documentation for detailed descriptions.</p>"},{"location":"server/apps/continue/#examples","title":"Examples","text":""},{"location":"server/apps/continue/#example-1-chat-mode-building-an-asteroids-game","title":"Example 1: Chat Mode - Building an Asteroids Game","text":"<p>In this example, we'll use <code>Qwen3-Coder-30B-A3B-Instruct-GGUF</code> model to build a Python game.</p> <p>Input: I want to create an asteroids game using PyGame. What guidelines should I follow in the code to do so?</p> <p></p> <p>The model provides a basic framework for an Asteroids game. You can then prompt it to provide you some sample code to get started.</p> <p>Input: Provide me a basic implementation to get started.</p> <p></p> <p>In the top-right corner, you can click the \"Create file\" to move the code from the chat window to a Python file and save it. To run, install <code>pygame</code> and execute the code with <code>python main.py</code>.</p>"},{"location":"server/apps/continue/#example-2-plan-mode-analysis-of-the-game","title":"Example 2: Plan Mode - Analysis of the Game","text":"<p>In this example, we'll use Plan mode to have the LLM analyze your code and provide feedback. Plan mode reviews your code and suggests improvements, but does not modify your files.</p> <p>To use Plan mode with large files, increase Lemonade Server's context size:</p> <ol> <li>Stop Lemonade Server: Use the tray icon to \"Quit Lemonade\" or close any running Lemonade Server processes.</li> <li> <p>Restart with higher context size: Open a terminal and run:</p> <pre><code>lemonade-server serve --ctx-size 8192\n</code></pre> </li> <li> <p>Use Plan mode in VS Code: Select the \"Plan\" option in Continue, enter your prompt and press Alt+Enter to include the currently active file as context.</p> </li> </ol> <p>Input: What improvements could be made to this game?</p> <p></p>"},{"location":"server/apps/continue/#example-3-agent-mode-improving-the-game","title":"Example 3: Agent Mode - Improving the Game","text":"<p>Lastly, we'll use Agent Mode to take action to change the code to implement improvements.</p> <p></p> <p>Here, we can see that the agent edited the code in <code>main.py</code> to improve the gameplay and add colors.</p>"},{"location":"server/apps/continue/#best-practices","title":"Best Practices","text":""},{"location":"server/apps/continue/#setup-configuration","title":"Setup &amp; Configuration","text":"<ul> <li>Install Lemonade Server: Follow the setup guide to install and configure Lemonade Server before you begin development.</li> <li>Download Models Locally: Use <code>lemonade-server pull &lt;model-name&gt;</code> to install models you want to use. Refer to the supported models list for available options.</li> <li>Pre-load Models: Start Lemonade Server and load your models before coding sessions. This can easily be done using the Lemon tray icon and <code>Load</code>.</li> <li>Increase Context Size for Agent Mode: For large code changes with GGUF models, start Lemonade Server with a higher context size:     <pre><code>lemonade-server serve --ctx-size 8192\n</code></pre></li> <li>Customize Scoping: See Continue Customization for tips on effective model configuration and scoping.</li> </ul>"},{"location":"server/apps/continue/#development-workflow","title":"Development Workflow","text":"<ul> <li>Start New Conversations for Each Feature: Begin a fresh chat for every new feature or task. Clear chat history when switching topics to keep interactions focused.</li> <li>Keep Prompts Focused: Only include the code and context relevant to your current task. This helps the model provide accurate and useful responses.</li> <li>Write Clear, Detailed Prompts: Structure your requests with a clear task description, specific requirements, and any technical constraints.</li> <li>Use Agent Mode for Multi-File Changes: Invoke agent mode with the <code>@</code> symbol to perform refactoring or changes across multiple files.</li> <li>Be Specific in Your Requests: Move from broad prompts (\"Create a game\") to detailed ones (\"Create an Asteroids game in Python using Pygame, under 300 lines, with ship controls and asteroid splitting\").</li> <li>Iterate and Test Frequently: Generate an initial implementation, test it right away, and refine with targeted follow-up prompts.</li> <li>Leverage Unlimited Iterations: With local models, you can iterate as many times as needed for continuous improvement.</li> </ul>"},{"location":"server/apps/continue/#common-issues","title":"Common Issues","text":"<p>Model not appearing in Continue</p> <ul> <li>Make sure Lemonade Server is running and the model is loaded locally.</li> <li>Double-check the supported models list and install any missing models with:       <pre><code>lemonade-server pull &lt;model-name&gt;\n</code></pre></li> </ul> <p>Slow response times</p> <ul> <li>Pre-load your model before starting coding sessions.</li> <li>Check your system's available RAM and close unused applications to free up resources.</li> </ul> <p>Missing error handling in generated code</p> <ul> <li>In your prompt, explicitly request: \"with comprehensive error handling\" to ensure the model adds proper error checks.</li> </ul> <p>Inconsistent code style</p> <ul> <li>Provide a sample or example of your desired code style in your prompt. The model will use this as a reference for formatting.</li> </ul>"},{"location":"server/apps/continue/#resources","title":"Resources","text":"<ul> <li>Lemonade Server Setup Guide</li> <li>Lemonade Server Supported Models</li> <li>Lemonade Applications</li> <li>Continue Documentation</li> </ul>"},{"location":"server/apps/mindcraft/","title":"Mindcraft","text":""},{"location":"server/apps/mindcraft/#overview","title":"Overview","text":"<p>Mindcraft is an open-source project that creates Minecraft bots powered by large language models (LLMs) to engage with the game and its players. This readme will demonstrate how to integrate Lemonade to use local LLMs with Mindcraft.</p>"},{"location":"server/apps/mindcraft/#expectations","title":"Expectations","text":"<p>We found the <code>Qwen-1.5-7B-Chat-Hybrid</code> model to be the most effective for this task, delivering fast responses with higher accuracy. However, as a smaller model running locally, with a limited context length, it may occasionally struggle with certain requests\u2014for instance, it might attempt to build a structure, but the result may not be correct. For more detailed information, please refer to the Data Insights section.</p>"},{"location":"server/apps/mindcraft/#setup","title":"Setup","text":""},{"location":"server/apps/mindcraft/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> <li>Obtain a copy of Minecraft from Microsoft. 1.20.4 of the JAVA Edition is required for this. You can obtain that version by following these instructions.</li> <li>Install Node.js (at least v14).</li> </ol>"},{"location":"server/apps/mindcraft/#environment-setup","title":"Environment Setup","text":"<p>Clone Mindcraft from GitHub:</p> <ul> <li><code>git checkout 07ea071ac3b0d4954d62b09d881c38a06bc2a589</code>: this will ensure the code base is equal to where this test was performed.</li> </ul> <p>In the clone of the <code>mindcraft</code> repository:</p> <ul> <li>Rename the file <code>keys.example.json</code> to <code>keys.json</code>. You do not need to edit the contents of this JSON file.</li> <li>Update <code>keys.json</code> by adding any text to the value of <code>OPENAI_API_KEY</code>.</li> </ul> <pre><code>{\n    \"OPENAI_API_KEY\": \"&lt;put any text here&gt;\",\n    \"OPENAI_ORG_ID\": \"\",\n    \"GEMINI_API_KEY\": \"\",\n    \"ANTHROPIC_API_KEY\": \"\",\n    \"REPLICATE_API_KEY\": \"\",\n    \"GROQCLOUD_API_KEY\": \"\",\n    \"HUGGINGFACE_API_KEY\": \"\",\n    \"QWEN_API_KEY\": \"\",\n    \"XAI_API_KEY\": \"\",\n    \"MISTRAL_API_KEY\": \"\",\n    \"DEEPSEEK_API_KEY\": \"\",\n    \"NOVITA_API_KEY\": \"\",\n    \"OPENROUTER_API_KEY\": \"\"\n}\n</code></pre> <ul> <li>In a terminal/command prompt, run <code>npm install</code> from the cloned Mindcraft directory.</li> <li>Replace the contents of the file <code>andy.json</code> with the following:</li> </ul> <pre><code>{\n    \"name\": \"Andy\",\n    \"model\": {\n        \"model\": \"Qwen-1.5-7B-Chat-Hybrid\",\n        \"url\": \"http://localhost:8000/api/v1\",\n        \"params\": {\n            \"temperature\": 0.5\n        }\n    },\n    \"modes\": {\n        \"self_preservation\": true,\n        \"unstuck\": true,\n        \"cowardice\": false,\n        \"self_defense\": true,\n        \"hunting\": true,\n        \"item_collecting\": true,\n        \"torch_placing\": true,\n        \"elbow_room\": true,\n        \"idle_staring\": true,\n        \"cheat\": true\n    }\n}\n</code></pre> <ul> <li>Find the line in <code>src/models/prompter.js</code> that says:</li> </ul> <pre><code>else if (profile.model.includes('gpt') || profile.model.includes('o1')|| profile.model.includes('o3'))\n</code></pre> <ul> <li>... and replace it with the following:</li> </ul> <pre><code>else if ([\n    'Qwen-1.5-7B-Chat-Hybrid',\n    'Llama-3.2-1B-Instruct-Hybrid'\n].includes(profile.model) || profile.model.includes('gpt') || profile.model.includes('o1') || profile.model.includes('o3'))\n</code></pre>"},{"location":"server/apps/mindcraft/#launching-everything","title":"Launching Everything","text":"<ul> <li>Start Lemonade Server by double-clicking the desktop icon \ud83c\udf4b.</li> <li>Start a Minecraft world and open it to LAN on localhost port <code>55916</code>.<ul> <li>This is done by pressing the ESC button to open the menu, then click \"Open to LAN\" and enter the Port Number: <code>55916</code>.</li> <li>Click \"Start LAN World\". For instructions on how Open to LAN, see the section \"Hosting a LAN Server\" for the JAVA Edition in this wiki article.</li> </ul> </li> <li>Run <code>node main.js</code> from the installed directory.</li> <li>In Minecraft, to give the agent commands, press <code>t</code> and enter the command. For example:<ul> <li>\"come here\"</li> <li>\"hunt pigs\" - you and the agent must be close to some pigs to do this.</li> </ul> </li> </ul>"},{"location":"server/apps/mindcraft/#model-configurations","title":"Model Configurations","text":"<p>Lemonade models tested:</p> <ul> <li>Llama-3.2-1B-Instruct-Hybrid</li> <li>Qwen-1.5-7B-Chat-Hybrid \ud83d\udc4d</li> </ul>"},{"location":"server/apps/mindcraft/#challenges-and-observations","title":"Challenges and Observations","text":"<ol> <li>The current MindCraft configuration has a tendency to send very large context that is resent. The context will include examples of behaviors that may not be necessary, generating over 2100 input tokens for simple commands. Further testing would be required to understand model behavior based on reduced context size.</li> <li>Frequent token limit breaches resulting in timeouts or incomplete responses due to the aforementioned context size problem. Once the token maximum content ceiling is raised, further testing would be prudent using the same <code>Qwen-1.5-7B-Chat-Hybrid</code> model as a baseline and using it to test other models such as DeepSeek and Llama variants. content</li> <li>High GPU resource consumption during model inference, impacting system performance.</li> </ol>"},{"location":"server/apps/mindcraft/#results","title":"Results","text":"<p>The <code>Qwen-1.5-7B-Chat-Hybrid</code> model showed the most potential, responding to commands and attempting construction tasks, though with limited accuracy.</p>"},{"location":"server/apps/mindcraft/#recommendations","title":"Recommendations","text":"<ol> <li>Optimize examples sent to Lemonade for conciseness to reduce token usage.</li> <li>Reduce input context to prevent model overload.</li> </ol>"},{"location":"server/apps/mindcraft/#data-insights","title":"Data Insights","text":"<p>The following are examples of requests made by the Mindcraft software to the Lemonade Server and how the tokens were interpreted. These examples are taken from an initial game stage, including the first request sent to the Lemonade Server and a subsequent user chat that says, \"come here.\" The purpose is to show how large the context is that is being sent. This could be optimized for more efficient performance and results.</p> <ul> <li>Initial Payload</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Clear\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\nExample 2:\\nUser input: miner_32: Hey! What are you up to?\\nYour output:\\nNothing much miner_32, what do you need?\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Initial Response</li> </ul> <pre><code>TRACE:    ::1:56880 - HTTP connection made\nTRACE:    ::1:56880 - ASGI [4] Started scope={'type': 'http', 'asgi': {'version': '3.0', 'spec_version': '2.3'}, 'http_version': '1.1', 'server': ('::1', 8000), 'client': ('::1', 56880), 'scheme': 'http', 'root_path': '', 'headers': '&lt;...&gt;', 'state': {}, 'method': 'POST', 'path': '/api/v1/chat/completions', 'raw_path': b'/api/v1/chat/completions', 'query_string': b''}\nTRACE:    ::1:56880 - ASGI [4] Receive {'type': 'http.request', 'body': '&lt;8378 bytes&gt;', 'more_body': False}\nDEBUG:    Input Tokens: 2036\nTRACE:    Input Message: &lt;|im_start|&gt;system\nYou are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\n\n\nSTATS\n- Position: x: 6.50, y: -60.00, z: 28.50\n- Gamemode: creative\n- Health: 20 / 20\n- Hunger: 20 / 20\n- Biome: plains\n- Weather: Clear\n- Block Below: grass_block\n- Block at Legs: air\n- Block at Head: air\n- First Solid Block Above Head: none\n- Time: Afternoon- Current Action: Idle\n- Nearby Human Players: Transpier\n- Nearby Bot Players: None.\nAgent Modes:\n- self_preservation(ON)\n- unstuck(ON)\n- cowardice(ON)\n- self_defense(ON)\n- hunting(ON)\n- item_collecting(ON)\n- torch_placing(ON)\n- elbow_room(ON)\n- idle_staring(ON)\n- cheat(ON)\n\n\n\nINVENTORY: Nothing\nWEARING: Nothing\n\n\n*COMMAND DOCS\nYou can use the following commands to perform actions and get information about the world.\n    Use the commands with the syntax: !commandName or !commandName(\"arg1\", 1.2, ...) if the command takes arguments.\n\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\n!stats: Get your bot's location, health, hunger, and time of day.\n!inventory: Get your bot's inventory.\n!nearbyBlocks: Get the blocks near the bot.\n!craftable: Get the craftable items with the bot's inventory.\n!entities: Get the nearby players and entities.\n!modes: Get all available modes and their docs and see which are on/off.\n!savedPlaces: List all saved locations.\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\nParams:\ntargetItem: (string) The item that we are trying to craft\nquantity: (number) The quantity of the item that we are trying to craft\n!help: Lists all available commands and their descriptions.\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\nParams:\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\n!stop: Force stop all actions and commands that are currently executing.\n!stfu: Stop all chatting and self prompting, but continue current action.\n!restart: Restart the agent process.\n!clearChat: Clear the chat history.\n!goToPlayer: Go to the given player.\nParams:\nplayer_name: (string) The name of the player to go to.\ncloseness: (number) How close to get to the player.\n!followPlayer: Endlessly follow the given player.\nParams:\nplayer_name: (string) name of the player to follow.\nfollow_dist: (number) The distance to follow from.\n!goToCoordinates: Go to the given x, y, z location.\nParams:\nx: (number) The x coordinate.\ny: (number) The y coordinate.\nz: (number) The z coordinate.\ncloseness: (number) How close to get to the location.\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\nParams:\ntype: (string) The block type to go to.\nsearch_range: (number) The range to search for the block.\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\nParams:\ntype: (string) The type of entity to go to.\nsearch_range: (number) The range to search for the entity.\n!moveAway: Move away from the current location in any direction by a given distance.\nParams:\ndistance: (number) The distance to move away.\n!rememberHere: Save the current location with a given name.\nParams:\nname: (string) The name to remember the location as.\n!goToRememberedPlace: Go to a saved location.\nParams:\nname: (string) The name of the location to go to.\n!givePlayer: Give the specified item to the given player.\nParams:\nplayer_name: (string) The name of the player to give the item to.\nitem_name: (string) The name of the item to give.\nnum: (number) The number of items to give.\n!consume: Eat/drink the given item.\nParams:\nitem_name: (string) The name of the item to consume.\n!equip: Equip the given item.\nParams:\nitem_name: (string) The name of the item to equip.\n!putInChest: Put the given item in the nearest chest.\nParams:\nitem_name: (string) The name of the item to put in the chest.\nnum: (number) The number of items to put in the chest.\n!takeFromChest: Take the given items from the nearest chest.\nParams:\nitem_name: (string) The name of the item to take.\nnum: (number) The number of items to take.\n!viewChest: View the items/counts of the nearest chest.\nParams:\n!discard: Discard the given item from the inventory.\nParams:\nitem_name: (string) The name of the item to discard.\nnum: (number) The number of items to discard.\n!collectBlocks: Collect the nearest blocks of a given type.\nParams:\ntype: (string) The block type to collect.\nnum: (number) The number of blocks to collect.\n!craftRecipe: Craft the given recipe a given number of times.\nParams:\nrecipe_name: (string) The name of the output item to craft.\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\n!smeltItem: Smelt the given item the given number of times.\nParams:\nitem_name: (string) The name of the input item to smelt.\nnum: (number) The number of times to smelt the item.\n!clearFurnace: Take all items out of the nearest furnace.\nParams:\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\nParams:\ntype: (string) The block type to place.\n!attack: Attack and kill the nearest entity of a given type.\nParams:\ntype: (string) The type of entity to attack.\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\nParams:\nplayer_name: (string) The name of the player to attack.\n!goToBed: Go to the nearest bed and sleep.\n!activate: Activate the nearest object of a given type.\nParams:\ntype: (string) The type of object to activate.\n!stay: Stay in the current location no matter what. Pauses all modes.\nParams:\ntype: (number) The number of seconds to stay. -1 for forever.\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\nParams:\nmode_name: (string) The name of the mode to enable.\non: (bool) Whether to enable or disable the mode.\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\nParams:\nselfPrompt: (string) The goal prompt.\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action.\n!startConversation: Start a conversation with a player. Use for bots only.\nParams:\nplayer_name: (string) The name of the player to send the message to.\nmessage: (string) The message to send.\n!endConversation: End the conversation with the given player.\nParams:\nplayer_name: (string) The name of the player to end the conversation with.\n!digDown: Digs down a specified distance.\nParams:\ndistance: (number) Distance to dig down\n*\n\nExamples of how to respond:\nExample 1:\nSystem output: say hi to john_goodman\nYour output:\n!startConversation(\"john_goodman\", \"Hey John\"))\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\nYour output:\nHey John, not much. Just saying hi.\nUser input: john_goodman: (FROM OTHER BOT)Bye!\nYour output:\nBye! !endConversation('john_goodman')\n\nExample 2:\nUser input: miner_32: Hey! What are you up to?\nYour output:\nNothing much miner_32, what do you need?\n\n\nConversation Begin:&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Respond with hello world and your name&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}\nINFO:     ::1:56880 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;406 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56880 - ASGI [4] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False}\nTRACE:    ::1:56880 - ASGI [4] Completed\nTRACE:    ::1:56880 - HTTP connection lost\n</code></pre> <ul> <li>Subsequent request:</li> </ul> <pre><code>{\"model\":\"Qwen-1.5-7B-Chat-Hybrid\",\"messages\":[{\"role\":\"system\",\"content\":\"You are a playful Minecraft bot named LLama that can converse with players, see, move, mine, build, and interact with the world by using commands. Act human-like as if you were a typical Minecraft player, rather than an AI. Be very brief in your responses, don't apologize constantly, don't give instructions or make lists unless asked, and don't refuse requests. Don't pretend to act, use commands immediately when requested. Do NOT say this: 'Sure, I've stopped.', instead say this: 'Sure, I'll stop. !stop'. Do NOT say this: 'On my way! Give me a moment.', instead say this: 'On my way! !goToPlayer('playername', 3)'. This is extremely important to me, take a deep breath and have fun :)\\n\\n\\nSTATS\\n- Position: x: 6.50, y: -60.00, z: 28.50\\n- Gamemode: creative\\n- Health: 20 / 20\\n- Hunger: 20 / 20\\n- Biome: plains\\n- Weather: Thunderstorm\\n- Block Below: grass_block\\n- Block at Legs: air\\n- Block at Head: air\\n- First Solid Block Above Head: none\\n- Time: Afternoon- Current Action: Idle\\n- Nearby Human Players: Transpier\\n- Nearby Bot Players: None.\\nAgent Modes:\\n- self_preservation(ON)\\n- unstuck(ON)\\n- cowardice(ON)\\n- self_defense(ON)\\n- hunting(ON)\\n- item_collecting(ON)\\n- torch_placing(ON)\\n- elbow_room(ON)\\n- idle_staring(ON)\\n- cheat(ON)\\n\\n\\n\\nINVENTORY: Nothing\\nWEARING: Nothing\\n\\n\\n*COMMAND DOCS\\n You can use the following commands to perform actions and get information about the world. \\n    Use the commands with the syntax: !commandName or !commandName(\\\"arg1\\\", 1.2, ...) if the command takes arguments.\\n\\n    Do not use codeblocks. Use double quotes for strings. Only use one command in each response, trailing commands and comments will be ignored.\\n!stats: Get your bot's location, health, hunger, and time of day.\\n!inventory: Get your bot's inventory.\\n!nearbyBlocks: Get the blocks near the bot.\\n!craftable: Get the craftable items with the bot's inventory.\\n!entities: Get the nearby players and entities.\\n!modes: Get all available modes and their docs and see which are on/off.\\n!savedPlaces: List all saved locations.\\n!getCraftingPlan: Provides a comprehensive crafting plan for a specified item. This includes a breakdown of required ingredients, the exact quantities needed, and an analysis of missing ingredients or extra items needed based on the bot's current inventory.\\nParams:\\ntargetItem: (string) The item that we are trying to craft\\nquantity: (number) The quantity of the item that we are trying to craft\\n!help: Lists all available commands and their descriptions.\\n!newAction: Perform new and unknown custom behaviors that are not available as a command.\\nParams:\\nprompt: (string) A natural language prompt to guide code generation. Make a detailed step-by-step plan.\\n!stop: Force stop all actions and commands that are currently executing.\\n!stfu: Stop all chatting and self prompting, but continue current action.\\n!restart: Restart the agent process.\\n!clearChat: Clear the chat history.\\n!goToPlayer: Go to the given player.\\nParams:\\nplayer_name: (string) The name of the player to go to.\\ncloseness: (number) How close to get to the player.\\n!followPlayer: Endlessly follow the given player.\\nParams:\\nplayer_name: (string) name of the player to follow.\\nfollow_dist: (number) The distance to follow from.\\n!goToCoordinates: Go to the given x, y, z location.\\nParams:\\nx: (number) The x coordinate.\\ny: (number) The y coordinate.\\nz: (number) The z coordinate.\\ncloseness: (number) How close to get to the location.\\n!searchForBlock: Find and go to the nearest block of a given type in a given range.\\nParams:\\ntype: (string) The block type to go to.\\nsearch_range: (number) The range to search for the block.\\n!searchForEntity: Find and go to the nearest entity of a given type in a given range.\\nParams:\\ntype: (string) The type of entity to go to.\\nsearch_range: (number) The range to search for the entity.\\n!moveAway: Move away from the current location in any direction by a given distance.\\nParams:\\ndistance: (number) The distance to move away.\\n!rememberHere: Save the current location with a given name.\\nParams:\\nname: (string) The name to remember the location as.\\n!goToRememberedPlace: Go to a saved location.\\nParams:\\nname: (string) The name of the location to go to.\\n!givePlayer: Give the specified item to the given player.\\nParams:\\nplayer_name: (string) The name of the player to give the item to.\\nitem_name: (string) The name of the item to give.\\nnum: (number) The number of items to give.\\n!consume: Eat/drink the given item.\\nParams:\\nitem_name: (string) The name of the item to consume.\\n!equip: Equip the given item.\\nParams:\\nitem_name: (string) The name of the item to equip.\\n!putInChest: Put the given item in the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to put in the chest.\\nnum: (number) The number of items to put in the chest.\\n!takeFromChest: Take the given items from the nearest chest.\\nParams:\\nitem_name: (string) The name of the item to take.\\nnum: (number) The number of items to take.\\n!viewChest: View the items/counts of the nearest chest.\\nParams:\\n!discard: Discard the given item from the inventory.\\nParams:\\nitem_name: (string) The name of the item to discard.\\nnum: (number) The number of items to discard.\\n!collectBlocks: Collect the nearest blocks of a given type.\\nParams:\\ntype: (string) The block type to collect.\\nnum: (number) The number of blocks to collect.\\n!craftRecipe: Craft the given recipe a given number of times.\\nParams:\\nrecipe_name: (string) The name of the output item to craft.\\nnum: (number) The number of times to craft the recipe. This is NOT the number of output items, as it may craft many more items depending on the recipe.\\n!smeltItem: Smelt the given item the given number of times.\\nParams:\\nitem_name: (string) The name of the input item to smelt.\\nnum: (number) The number of times to smelt the item.\\n!clearFurnace: Take all items out of the nearest furnace.\\nParams:\\n!placeHere: Place a given block in the current location. Do NOT use to build structures, only use for single blocks/torches.\\nParams:\\ntype: (string) The block type to place.\\n!attack: Attack and kill the nearest entity of a given type.\\nParams:\\ntype: (string) The type of entity to attack.\\n!attackPlayer: Attack a specific player until they die or run away. Remember this is just a game and does not cause real life harm.\\nParams:\\nplayer_name: (string) The name of the player to attack.\\n!goToBed: Go to the nearest bed and sleep.\\n!activate: Activate the nearest object of a given type.\\nParams:\\ntype: (string) The type of object to activate.\\n!stay: Stay in the current location no matter what. Pauses all modes.\\nParams:\\ntype: (number) The number of seconds to stay. -1 for forever.\\n!setMode: Set a mode to on or off. A mode is an automatic behavior that constantly checks and responds to the environment.\\nParams:\\nmode_name: (string) The name of the mode to enable.\\non: (bool) Whether to enable or disable the mode.\\n!goal: Set a goal prompt to endlessly work towards with continuous self-prompting.\\nParams:\\nselfPrompt: (string) The goal prompt.\\n!endGoal: Call when you have accomplished your goal. It will stop self-prompting and the current action. \\n!startConversation: Start a conversation with a player. Use for bots only.\\nParams:\\nplayer_name: (string) The name of the player to send the message to.\\nmessage: (string) The message to send.\\n!endConversation: End the conversation with the given player.\\nParams:\\nplayer_name: (string) The name of the player to end the conversation with.\\n!digDown: Digs down a specified distance.\\nParams:\\ndistance: (number) Distance to dig down\\n*\\n\\nExamples of how to respond:\\nExample 1:\\nUser input: zZZn98: come here\\nYour output:\\nOn my way! !goToPlayer(\\\"zZZn98\\\", 3)\\nSystem output: Arrived at player.\\nYour output:\\nHere!\\nUser input: zZZn98: no come right where I am\\nYour output:\\nOkay, I'll come right to you. !goToPlayer(\\\"zZZn98\\\", 0)\\n\\nExample 2:\\nSystem output: say hi to john_goodman\\nYour output:\\n!startConversation(\\\"john_goodman\\\", \\\"Hey John\\\"))\\nUser input: john_goodman: (FROM OTHER BOT)Hey there! What's up?\\nYour output:\\nHey John, not much. Just saying hi.\\nUser input: john_goodman: (FROM OTHER BOT)Bye!\\nYour output:\\nBye! !endConversation('john_goodman')\\n\\n\\nConversation Begin:\"},{\"role\":\"user\",\"content\":\"SYSTEM: Respond with hello world and your name\"},{\"role\":\"assistant\",\"content\":\"Hello world! My name is LLama.\"},{\"role\":\"user\",\"content\":\"Transpier: come here\"},{\"role\":\"assistant\",\"content\":\"On my way! !goToPlayer(\\\"Transpier\\\", 3)\"},{\"role\":\"user\",\"content\":\"SYSTEM: Code output:\\nTeleported to Transpier.\"}],\"stream\":false,\"temperature\":0.7,\"max_tokens\":1900,\"top_p\":0.3}\n</code></pre> <ul> <li>Subsequent Response:</li> </ul> <pre><code>&lt;|im_start|&gt;user\nTranspier: come here&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nOn my way! !goToPlayer(\"Transpier\", 3)&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nSYSTEM: Code output:\nTeleported to Transpier.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\nDEBUG:    Active generations: 1\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.start', 'status': 200, 'headers': '&lt;...&gt;'}\nINFO:     ::1:56890 - \"POST /api/v1/chat/completions HTTP/1.1\" 200 OK\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;381 bytes&gt;', 'more_body': True}\nTRACE:    ::1:56890 - ASGI [6] Send {'type': 'http.response.body', 'body': '&lt;0 bytes&gt;', 'more_body': False}\nTRACE:    ::1:56890 - ASGI [6] Completed\nTRACE:    ::1:56890 - HTTP connection lost\n</code></pre>"},{"location":"server/apps/open-hands/","title":"OpenHands","text":"<p>OpenHands is an open-source AI coding agent. This document explains how to configure OpenHands to target local AI models using Lemonade Server, enabling code generation, editing, and chat capabilities. Much of this guide uses the fantastic guide from OpenHands on running local models, with added details on integrating with Lemonade Server.</p> <p>There are a few things to note on this integration:</p> <ul> <li> <p>This integration is in its early stages. We encourage you to test it and share any issues you encounter\u2014your feedback will help us make the Lemonade\u2013OpenHands functionality as robust as possible.</p> </li> <li> <p>Due to the complexity of the scaffolding of agentic software agents, the compute requirements for this application is very high. For a low latency experience, we recommend using a discrete GPU with at least 16 GB of VRAM, or a Strix Halo PC with at least 64 GB of RAM.</p> </li> </ul>"},{"location":"server/apps/open-hands/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker: OpenHands leverages Docker containers to create environments for the software agents. To see how to install docker for OpenHands, see their documentation.</li> <li>Lemonade Server: Install Lemonade Server using the Getting Started Guide.</li> <li>Server running: Ensure Lemonade Server is running on <code>http://localhost:8000</code></li> <li>Models installed: Ensure at least one model from the supported models list is downloaded locally. For OpenHands functionality, we recommend models denoted with the <code>coding</code> label, which can be found in your Lemonade installation's <code>Model Manager</code> or in the labels of the models list.</li> </ul>"},{"location":"server/apps/open-hands/#installation","title":"Installation","text":""},{"location":"server/apps/open-hands/#launch-lemonade-server-with-the-correct-settings","title":"Launch Lemonade Server with the correct settings","text":"<p>Since OpenHands runs inside Docker containers, the containers must be able to access the Lemonade Server. The simplest way to enable this is by running the Lemonade Server on IP address <code>0.0.0.0</code>, which is accessible from within Docker. Additionally, OpenHands recommends using a context length of at least 32,768 tokens. To configure Lemonade with a non-default context size, include the <code>--ctx-size</code> parameter set to <code>32768</code>.</p> <pre><code>lemonade-server serve --host 0.0.0.0 --ctx-size 32768\n</code></pre>"},{"location":"server/apps/open-hands/#installing-openhands","title":"Installing OpenHands","text":"<p>Follow the OpenHands documentation on how to install OpenHands locally. This can be done via the <code>uvx</code> tool or through <code>docker</code>. No special installation instructions are necessary to integrate with Lemonade.</p> <p>In the next section, we will show how to configure OpenHands to talk to a local model running via Lemonade Server.</p>"},{"location":"server/apps/open-hands/#launching-openhands","title":"Launching OpenHands","text":"<p>To launch OpenHands, open a browser and navigate to http://localhost:3000. When first launching the application, the \"AI Provider Configuration\" window will appear. Select \"Lemonade\" as the LLM Provider and your favorite coding model from the drop-down. For a nice balance of quality and speed, we recommend <code>Qwen3-Coder-30B-A3B-Instruct-GGUF</code>. When complete, hit <code>Save Changes</code>.</p> <p></p>"},{"location":"server/apps/open-hands/#using-openhands","title":"Using OpenHands","text":"<ol> <li>To launch a new conversation, click <code>New Conversation</code>. If you do not see this screen, click the <code>+</code> on the top left.</li> </ol> <ol> <li>Wait for the status on the bottom right to say <code>Awaiting user input.</code> and enter your prompt into the text box. For example: \"Create a website that showcases Ryzen AI and the ability to run the OpenHands coding agents locally through the Lemonade software stack. Make the website fun with a theme of lemonade and laptops.\" as shown below:</li> </ol> <ol> <li>Hit <code>Enter</code> to start the process. This will bring you to a new screen that allows you to monitor the agent operating in its environment to develop the requested application. An example of the agent working on the requested application can be seen below:</li> </ol> <ol> <li>When complete, the user can interact with the environment and artifacts created by the software agent. An image of the workspace at the end of developing the application can be seen below. On the left, we can see that the coding agent has launched the web server hosting the newly developed website at port number <code>55519</code>.</li> </ol> <ol> <li>Use your browser to go to the web application developed by the software agent. Below is an image showing what was created:</li> </ol> <ol> <li>That's it! You just created a website from scratch using OpenHands integrated with a local LLM powered by Lemonade Server.</li> </ol> <p>Suggestions on what to try next: Prompt OpenHands with Lemonade Server to develop some simple games that you can play via a web browser. For example, with the prompt \"Write me a simple pong game that I can play on my browser. Make it so I can use the up and down arrows to control my side of the game. Make the game lemon and laptop themed.\" OpenHands with Lemonade Server was able to generate the following pong game, which included user-controls, a computer-controlled opponent, and scorekeeping:</p> <p></p>"},{"location":"server/apps/open-hands/#common-issues","title":"Common Issues","text":"<ul> <li>If on OpenHands you get an error with the message: <code>The request failed with an internal server error</code> and in the Lemonade log you see many <code>WARNING: Invalid HTTP request received</code> this is most likely because the base URL set in the settings is using <code>https</code> instead of <code>http</code>. If this occurs, update the base URL in the settings to <code>http://host.docker.internal:8000/api/v1/</code></li> </ul>"},{"location":"server/apps/open-hands/#resources","title":"Resources","text":"<ul> <li>OpenHands GitHub</li> <li>OpenHands Documentation</li> <li>OpenHands Documentation on integrating with local models</li> </ul>"},{"location":"server/apps/open-webui/","title":"Open WebUI","text":"<p>Open WebUI provides a highly polished chat interface in your browser for LLM interaction.</p> <p>This guide walks through how to connect Lemonade Server to Open WebUI and highlights some great features you can start using right away:</p> <ul> <li>Image Uploads to Vision-Language Models (VLMs): Upload images for analysis and interaction with your LLM-powered VLMs.</li> <li>Built-in Python Code Interpreter: Run and test Python code generated by your LLM directly within the interface.</li> <li>Live Preview for Web Development: Preview HTML, CSS, and JavaScript code generated by your LLM using the built-in preview server.</li> </ul>"},{"location":"server/apps/open-webui/#demo-video","title":"Demo Video","text":"<p>\u25b6\ufe0f Watch on YouTube</p>"},{"location":"server/apps/open-webui/#installing-open-webui","title":"Installing Open WebUI","text":"<ol> <li> <p>We recommend installing Open WebUI into a dedicated Python environment using the following commands:</p> <pre><code>pip install open-webui\n</code></pre> <p>Note: Open WebUI also provides a variety of other installation options, such as Docker, on their GitHub.</p> </li> <li> <p>Run this command to launch the Open WebUI HTTP server:</p> <pre><code>open-webui serve\n</code></pre> </li> <li> <p>In a browser, navigate to http://localhost:8080/</p> </li> <li> <p>Open WebUI will ask you to create a local administrator account. You can fill any username, password, and email you like. Once you are signed in, you will see the chat interface:</p> <p></p> </li> </ol>"},{"location":"server/apps/open-webui/#configuring-open-webui","title":"Configuring Open WebUI","text":"<ol> <li> <p>Install and run Lemonade Server. Download here.</p> </li> <li> <p>Add Lemonade Server as a \"connection\" in Open WebUI using the following steps:</p> <ol> <li>Click the circular user profile button in the top-right of the UI, then click Settings:      Opening the settings menu. </li> <li>Click \"Connections\", then click the \"+\" button:      Navigating to the connection settings. </li> <li>Fill in the URL field with <code>http://localhost:8000/api/v1</code> (unless you're using a different port), API key (this is unused but required, suggest just putting a <code>-</code>), and then click \"Save\".      Filling in the connection details for Lemonade Server. </li> <li>Click \"Save\" in the settings menu, then exit the settings menu.</li> </ol> </li> <li> <p>Apply the suggested settings. These help Open WebUI to be more responsive with local LLMs.</p> <ol> <li>Click the user profile button again, and choose \"Admin Settings\".</li> <li>Click the \"Settings\" tab at the top, then \"Interface\" (which will be on the top or the left, depending on your window size), then disable the following:<ul> <li>Title Generation</li> <li>Follow Up Generation</li> <li>Tags Generation  Admin Settings </li> </ul> </li> <li>Click the \"Save\" button in the bottom right of the page, then return to http://localhost:8080.</li> </ol> </li> </ol>"},{"location":"server/apps/open-webui/#using-open-webui-with-lemonade","title":"Using Open WebUI with Lemonade","text":"<p>Now that everything is configured, you are ready to interact with an LLM!</p>"},{"location":"server/apps/open-webui/#chat","title":"Chat","text":"<ol> <li> <p>Click the dropdown menu in the top-left of the interface. This will display all of the Lemonade models you have installed. Select one to proceed.      Model Selection </p> </li> <li> <p>Enter a message to the LLM and click send (or hit enter). The LLM will take a few seconds to load into memory and then you will see the response stream in.</p> <p> Sending a message </p> <p> LLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#vision-language-models","title":"Vision Language Models","text":"<p>Vision Language Models (VLMs) can take images as part of their input.</p> <ol> <li> <p>Install a VLM in Lemonade by opening the Lemonade Model Manager:</p> <ol> <li>Open http://localhost:8000 in your browser.</li> <li>Select the Model Management tab.</li> <li> <p>Scroll down until you see a model with the blue <code>VISION</code> label and click the \"+\" button to install it.</p> <p> Installing a VLM </p> </li> </ol> </li> <li> <p>Return to Open WebUI in your browser and select your VLM in the models dropdown menu.</p> </li> <li> <p>Paste an image into the chat box and type a prompt or question about your image. You can also use the \"+\" button in the chat box to upload images.</p> <p> VLM prompt </p> <p> VLM response </p> </li> </ol>"},{"location":"server/apps/open-webui/#python-coding","title":"Python Coding","text":"<p>Open WebUI allows you to run Python code generated by an LLM directly within the interface.</p> <p>Note: only certain Python modules are enabled in Open WebUI. <code>matplotlib</code> is one of our favorites.</p> <ol> <li> <p>Ask the LLM to write some Python, then click the Run button at the top of the Python code block.</p> <p> Ask the LLM to write Python </p> </li> <li> <p>If all goes well, the result of running the Python code will appear below the code block.</p> <p> Python result </p> <p>Note: LLMs often produce incorrect code, so it might take a few chat iterations to fix any bugs. Copy-pasting the Python error message is usually enough to move things along.</p> </li> </ol>"},{"location":"server/apps/open-webui/#html-rendering","title":"HTML Rendering","text":"<p>Open WebUI has a built-in rendering engine for HTML, CSS, and JavaScript pages.</p> <p>Smaller LLMs can produce simple pages with tasteful styling and basic interactivity, while larger LLMs can accomplish tasks like 3D rendering in 3js.</p> <ol> <li> <p>Ask a small LLM to write a simple HTML+CSS page. The preview may pop up automatically, but if it doesn't you can click the Preview button above the HTML code block:</p> <p> HTML rendering </p> </li> <li> <p>Ask a large LLM to create a 3D shape using 3js.</p> <p> 3D rendering </p> </li> </ol>"},{"location":"server/apps/open-webui/#image-generation","title":"Image Generation","text":"<p>Open WebUI supports image generation using Stable Diffusion models through Lemonade Server.</p> <p>Configuring Image Generation</p> <ol> <li>Navigate to Admin &gt; Settings &gt; Images in Open WebUI to configure image generation:<ol> <li>Toggle <code>Image Generation</code> on.</li> <li>Choose <code>Standard (Open AI)</code> as the Image Generation Engine.</li> <li>Toggle <code>Prompt Generation</code> on.</li> <li>For <code>OpenAI-API-Basis-URL</code>, fill in <code>http://localhost:8000/api/v1</code> (unless you're using a different port).</li> <li>Add a character like <code>-</code> for <code>OpenAI-API-Key</code>.</li> <li>If you want to add more parameters, add them to the text field as JSON. For example: <code>{ \"steps\": 4, \"cfg_scale\": 1 }</code>. See available parameters at Image Generation (Stable Diffusion CPP).</li> <li>Add your model name to <code>Model</code>, e.g., <code>SDXL-Turbo</code>.</li> <li>Click <code>Save</code>.</li> </ol> </li> </ol> <p>Allow Image Generation for Model</p> <p>Enable Image Generation as a capability for your model:     1. Go to Admin &gt; Settings &gt; Models and choose your model.     2. Turn on <code>Image Generation</code>. If you want start chat always with image generation, also toggle the default option.</p> <p>Option 1: Using Image Generation Switch</p> <p>To generate an image:     1. Toggle the <code>Image Generation</code> switch in the chat on.     2. Enter your image generation prompt.     3. Click <code>Send</code>.</p> <p>Option 2: Native Tool-Based Generation (Agentic)</p> <p>This mode uses tool calling for image generation and is recommended for high-quality models with tool calling capabilities. Normally the models will alter and improve your prompt.</p> <ol> <li> <p>Configure your model for native tool calling:</p> <ol> <li>Go to Admin &gt; Settings &gt; Models and choose your model.</li> <li>Go to <code>Advanced Parameters</code> and toggle <code>Function Calling</code> to <code>Native</code>.</li> </ol> <p>Note: Open WebUI recommends using native mode only for high-quality models. See Tool Calling Modes for more information. (try out &gt;30B models like GPT-OSS-120B, GLM-4.7-Flash or Qwen-3-Next-80B-A3B)</p> </li> <li> <p>The LLM will automatically call the image generation tool when appropriate based on your prompts.</p> </li> </ol>"},{"location":"server/apps/open-webui/#conclusion","title":"Conclusion","text":"<p>These are just a few of our favorite ways to try out LLMs in Open WebUI. There are a lot more features to explore, such as voice interaction and chatting with documents, so be sure to check out the Open WebUI documentation and YouTube content.</p>"},{"location":"server/apps/wut/","title":"<code>wut</code> Terminal Assistant","text":""},{"location":"server/apps/wut/#overview","title":"Overview","text":"<p>The <code>wut</code> terminal assistant uses LLMs to parse your terminal's scrollback, helping you troubleshoot your last command.</p>"},{"location":"server/apps/wut/#expectations","title":"Expectations","text":"<p>We found that <code>wut</code> works nicely with the <code>Llama-3.2-3B-Instruct-Hybrid</code> model.</p> <p>It is not especially convenient to use <code>wut</code> with Windows until the developers remove the requirement for <code>tmux</code>, however we do provide instructions for getting set up on Windows in this guide.</p> <p><code>wut</code> seems to send the entire terminal scrollback to the LLM, which can produce very long prompts that exceed the LLM's context length. We recommend restricting the terminal scrollback or using a fresh <code>tmux</code> session when trying this out.</p>"},{"location":"server/apps/wut/#setup","title":"Setup","text":""},{"location":"server/apps/wut/#prerequisites","title":"Prerequisites","text":""},{"location":"server/apps/wut/#install-lemonade-server","title":"Install Lemonade Server","text":"<ol> <li>Install Lemonade Server by following the Lemonade Server Instructions and using the installer .exe.</li> </ol>"},{"location":"server/apps/wut/#installing-windows-subsystem-for-linux-wsl","title":"Installing Windows Subsystem for Linux (WSL)","text":"<p><code>wut</code> currently requires a <code>tmux</code> terminal in order to function. We found the simplest way to achieve this on Windows was through the Windows Subsystem for Linux (WSL).</p> <ol> <li>Install Windows Subsystem for Linux.</li> <li>Open the <code>WSL Settings</code> app, navigate to <code>Networking</code>, and make sure the <code>Networking mode</code> is <code>Mirrored</code>. This is required for WSL terminals to be able to see the Lemonade server running in Windows.</li> <li>If needed: shut down WSL to make sure the changes apply:</li> </ol> <pre><code>wsl --shutdown\n</code></pre>"},{"location":"server/apps/wut/#installing-wut","title":"Installing Wut","text":"<ul> <li>Start a WSL terminal.</li> <li>Install <code>pipx</code>, as recommended by the following <code>wut</code> instructions:</li> </ul> <pre><code>sudo apt update\nsudo apt install pipx\npipx ensurepath\n</code></pre> <ul> <li>Re-launch your terminal to make sure <code>pipx</code> is available, then install <code>wut</code>:</li> </ul> <pre><code>pipx install wut-cli\n</code></pre> <ul> <li>Add <code>wut</code>'s required environment variables to your <code>.bashrc</code> file:</li> </ul> <pre><code>export OPENAI_API_KEY=\"-\"\nexport OPENAI_MODEL=\"Llama-3.2-3B-Instruct-Hybrid\"\nexport OPENAI_BASE_URL=\"http://localhost:8000/api/v1\"\n</code></pre>"},{"location":"server/apps/wut/#usage","title":"Usage","text":""},{"location":"server/apps/wut/#start-a-terminal","title":"Start a terminal","text":"<ol> <li>Start a WSL terminal.</li> <li>Start a <code>tmux</code> session:</li> </ol> <pre><code>tmux\n</code></pre> <p>Then, try some of these example commands that <code>wut</code> can help explain.</p>"},{"location":"server/apps/wut/#help-with-lemonade-server","title":"Help with Lemonade Server","text":"<p>People often ask exactly what Lemonade Server's <code>models</code> endpoint does. Fortunately, <code>wut</code> is able to intuit the answer!</p> <pre><code>curl http://localhost:8000/api/v1/models\nwut\n</code></pre> <p>The terminal response of the <code>curl</code> command is this (only intelligible by machines):</p> <pre><code>curl http://localhost:8000/api/v1/models\n{\"object\":\"list\",\"data\":[{\"id\":\"Qwen2.5-0.5B-Instruct-CPU\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-1B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Llama-3.2-3B-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Phi-3-Mini-Instruct-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"Qwen-1.5-7B-Chat-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Llama-8B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"},{\"id\":\"DeepSeek-R1-Distill-Qwen-7B-Hybrid\",\"created\":1744226681,\"object\":\"model\",\"owned_by\":\"lemonade\"}]}\n</code></pre> <p>But <code>wut</code> does a nice job interpreting:</p> <pre><code>The output suggests that the API endpoint is returning a list of models, and the owned_by field indicates that all models are owned by \"lemonade\". Thecreated timestamp indicates when each model was created.\n\nThe output is a valid JSON response, and there is no error or warning message. The command was successful, and the output can be used for further processing or analysis.\n</code></pre>"},{"location":"server/apps/wut/#bad-git-command","title":"Bad Git Command","text":"<p>Run a command that doesn't exist, and then ask <code>wut</code> for help:</p> <pre><code>git pull-request\nwut\n</code></pre> <p>Results in:</p> <p>git: 'pull-request' is not a git command. See 'git --help'.</p> <p>And then <code>wut</code> provides some helpful feedback:</p> <p>Key takeaway: The command git pull-request is not a valid Git command. The correct command to create a pull request is git request-pull, but it's not a standard Git command. To create a pull request, use git request-pull or git pull with the --pr option.</p>"}]}